[24/11/24 13:42:38, INFO, FSL_SAGE_main.py:<module>():404] Using GPU: True
[24/11/24 13:42:40, INFO, FSL_SAGE_main.py:main():107] Aggregation Factor: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
[24/11/24 13:42:40, INFO, FSL_SAGE_main.py:main():112] Random seed: 200
[24/11/24 13:42:40, INFO, FSL_SAGE_main.py:main():113] Alignment interval (l): 10
[24/11/24 13:42:40, INFO, FSL_SAGE_main.py:main():114] Batch Round (h): 5
[24/11/24 13:43:06, INFO, FSL_SAGE_main.py:main():353]  > R  0, for the weighted aggregated final model, testing loss: 2.0409e+00, testing acc: 22.26% ( 2226/10000), training loss: 2.10, training acc: 20.87%
[24/11/24 13:43:32, INFO, FSL_SAGE_main.py:main():353]  > R  1, for the weighted aggregated final model, testing loss: 1.8046e+00, testing acc: 31.67% ( 3167/10000), training loss: 1.91, training acc: 27.81%
[24/11/24 13:43:57, INFO, FSL_SAGE_main.py:main():353]  > R  2, for the weighted aggregated final model, testing loss: 1.6825e+00, testing acc: 38.07% ( 3807/10000), training loss: 1.83, training acc: 32.05%
[24/11/24 13:44:23, INFO, FSL_SAGE_main.py:main():353]  > R  3, for the weighted aggregated final model, testing loss: 1.6225e+00, testing acc: 39.33% ( 3933/10000), training loss: 1.75, training acc: 34.26%
[24/11/24 13:44:48, INFO, FSL_SAGE_main.py:main():353]  > R  4, for the weighted aggregated final model, testing loss: 1.6453e+00, testing acc: 40.45% ( 4045/10000), training loss: 1.79, training acc: 35.64%
[24/11/24 13:45:14, INFO, FSL_SAGE_main.py:main():353]  > R  5, for the weighted aggregated final model, testing loss: 1.6351e+00, testing acc: 40.45% ( 4045/10000), training loss: 1.77, training acc: 35.56%
[24/11/24 13:45:39, INFO, FSL_SAGE_main.py:main():353]  > R  6, for the weighted aggregated final model, testing loss: 1.5893e+00, testing acc: 41.11% ( 4111/10000), training loss: 1.72, training acc: 36.55%
[24/11/24 13:46:05, INFO, FSL_SAGE_main.py:main():353]  > R  7, for the weighted aggregated final model, testing loss: 1.5098e+00, testing acc: 44.18% ( 4418/10000), training loss: 1.64, training acc: 39.93%
[24/11/24 13:46:30, INFO, FSL_SAGE_main.py:main():353]  > R  8, for the weighted aggregated final model, testing loss: 1.4861e+00, testing acc: 45.53% ( 4553/10000), training loss: 1.62, training acc: 40.39%
[24/11/24 13:46:56, INFO, FSL_SAGE_main.py:main():353]  > R  9, for the weighted aggregated final model, testing loss: 1.5469e+00, testing acc: 43.23% ( 4323/10000), training loss: 1.66, training acc: 39.21%
[24/11/24 13:47:24, INFO, FSL_SAGE_main.py:main():353]  > R 10, for the weighted aggregated final model, testing loss: 1.3940e+00, testing acc: 49.77% ( 4977/10000), training loss: 1.55, training acc: 43.81%
[24/11/24 13:47:50, INFO, FSL_SAGE_main.py:main():353]  > R 11, for the weighted aggregated final model, testing loss: 1.3793e+00, testing acc: 48.95% ( 4895/10000), training loss: 1.54, training acc: 43.28%
[24/11/24 13:48:15, INFO, FSL_SAGE_main.py:main():353]  > R 12, for the weighted aggregated final model, testing loss: 1.3182e+00, testing acc: 51.72% ( 5172/10000), training loss: 1.48, training acc: 45.64%
[24/11/24 13:48:41, INFO, FSL_SAGE_main.py:main():353]  > R 13, for the weighted aggregated final model, testing loss: 1.2619e+00, testing acc: 53.76% ( 5376/10000), training loss: 1.41, training acc: 49.01%
[24/11/24 13:49:06, INFO, FSL_SAGE_main.py:main():353]  > R 14, for the weighted aggregated final model, testing loss: 1.2329e+00, testing acc: 54.84% ( 5484/10000), training loss: 1.39, training acc: 49.27%
[24/11/24 13:49:31, INFO, FSL_SAGE_main.py:main():353]  > R 15, for the weighted aggregated final model, testing loss: 1.2436e+00, testing acc: 54.42% ( 5442/10000), training loss: 1.38, training acc: 49.55%
[24/11/24 13:49:57, INFO, FSL_SAGE_main.py:main():353]  > R 16, for the weighted aggregated final model, testing loss: 1.1811e+00, testing acc: 57.58% ( 5758/10000), training loss: 1.34, training acc: 51.37%
[24/11/24 13:50:22, INFO, FSL_SAGE_main.py:main():353]  > R 17, for the weighted aggregated final model, testing loss: 1.2726e+00, testing acc: 53.41% ( 5341/10000), training loss: 1.41, training acc: 48.40%
[24/11/24 13:50:48, INFO, FSL_SAGE_main.py:main():353]  > R 18, for the weighted aggregated final model, testing loss: 1.1768e+00, testing acc: 57.43% ( 5743/10000), training loss: 1.32, training acc: 52.21%
[24/11/24 13:51:13, INFO, FSL_SAGE_main.py:main():353]  > R 19, for the weighted aggregated final model, testing loss: 1.1605e+00, testing acc: 58.53% ( 5853/10000), training loss: 1.31, training acc: 52.87%
[24/11/24 13:51:41, INFO, FSL_SAGE_main.py:main():353]  > R 20, for the weighted aggregated final model, testing loss: 1.1157e+00, testing acc: 59.74% ( 5974/10000), training loss: 1.28, training acc: 54.21%
[24/11/24 13:52:07, INFO, FSL_SAGE_main.py:main():353]  > R 21, for the weighted aggregated final model, testing loss: 1.0943e+00, testing acc: 61.33% ( 6133/10000), training loss: 1.25, training acc: 55.34%
[24/11/24 13:52:32, INFO, FSL_SAGE_main.py:main():353]  > R 22, for the weighted aggregated final model, testing loss: 1.0775e+00, testing acc: 61.93% ( 6193/10000), training loss: 1.22, training acc: 56.35%
[24/11/24 13:52:58, INFO, FSL_SAGE_main.py:main():353]  > R 23, for the weighted aggregated final model, testing loss: 1.0795e+00, testing acc: 61.97% ( 6197/10000), training loss: 1.24, training acc: 55.56%
[24/11/24 13:53:23, INFO, FSL_SAGE_main.py:main():353]  > R 24, for the weighted aggregated final model, testing loss: 1.0497e+00, testing acc: 62.82% ( 6282/10000), training loss: 1.22, training acc: 56.42%
[24/11/24 13:53:49, INFO, FSL_SAGE_main.py:main():353]  > R 25, for the weighted aggregated final model, testing loss: 1.0419e+00, testing acc: 63.62% ( 6362/10000), training loss: 1.19, training acc: 57.71%
[24/11/24 13:54:14, INFO, FSL_SAGE_main.py:main():353]  > R 26, for the weighted aggregated final model, testing loss: 1.0239e+00, testing acc: 63.79% ( 6379/10000), training loss: 1.17, training acc: 58.34%
[24/11/24 13:54:39, INFO, FSL_SAGE_main.py:main():353]  > R 27, for the weighted aggregated final model, testing loss: 1.0236e+00, testing acc: 64.01% ( 6401/10000), training loss: 1.18, training acc: 57.96%
[24/11/24 13:55:05, INFO, FSL_SAGE_main.py:main():353]  > R 28, for the weighted aggregated final model, testing loss: 1.0268e+00, testing acc: 63.68% ( 6368/10000), training loss: 1.18, training acc: 57.74%
[24/11/24 13:55:30, INFO, FSL_SAGE_main.py:main():353]  > R 29, for the weighted aggregated final model, testing loss: 9.9524e-01, testing acc: 64.81% ( 6481/10000), training loss: 1.15, training acc: 58.86%
[24/11/24 13:55:59, INFO, FSL_SAGE_main.py:main():353]  > R 30, for the weighted aggregated final model, testing loss: 1.0027e+00, testing acc: 64.40% ( 6440/10000), training loss: 1.13, training acc: 59.90%
[24/11/24 13:56:24, INFO, FSL_SAGE_main.py:main():353]  > R 31, for the weighted aggregated final model, testing loss: 9.8090e-01, testing acc: 65.97% ( 6597/10000), training loss: 1.13, training acc: 59.95%
[24/11/24 13:56:50, INFO, FSL_SAGE_main.py:main():353]  > R 32, for the weighted aggregated final model, testing loss: 9.9124e-01, testing acc: 65.39% ( 6539/10000), training loss: 1.12, training acc: 60.00%
[24/11/24 13:57:15, INFO, FSL_SAGE_main.py:main():353]  > R 33, for the weighted aggregated final model, testing loss: 9.5752e-01, testing acc: 66.31% ( 6631/10000), training loss: 1.10, training acc: 60.76%
[24/11/24 13:57:40, INFO, FSL_SAGE_main.py:main():353]  > R 34, for the weighted aggregated final model, testing loss: 9.6100e-01, testing acc: 65.95% ( 6595/10000), training loss: 1.10, training acc: 60.69%
[24/11/24 13:58:06, INFO, FSL_SAGE_main.py:main():353]  > R 35, for the weighted aggregated final model, testing loss: 9.4395e-01, testing acc: 67.40% ( 6740/10000), training loss: 1.08, training acc: 61.34%
[24/11/24 13:58:31, INFO, FSL_SAGE_main.py:main():353]  > R 36, for the weighted aggregated final model, testing loss: 9.3027e-01, testing acc: 67.58% ( 6758/10000), training loss: 1.05, training acc: 62.63%
[24/11/24 13:58:57, INFO, FSL_SAGE_main.py:main():353]  > R 37, for the weighted aggregated final model, testing loss: 9.2955e-01, testing acc: 67.09% ( 6709/10000), training loss: 1.08, training acc: 61.98%
[24/11/24 13:59:22, INFO, FSL_SAGE_main.py:main():353]  > R 38, for the weighted aggregated final model, testing loss: 9.1032e-01, testing acc: 67.75% ( 6775/10000), training loss: 1.05, training acc: 62.85%
[24/11/24 13:59:47, INFO, FSL_SAGE_main.py:main():353]  > R 39, for the weighted aggregated final model, testing loss: 9.1763e-01, testing acc: 67.54% ( 6754/10000), training loss: 1.06, training acc: 62.51%
[24/11/24 14:00:16, INFO, FSL_SAGE_main.py:main():353]  > R 40, for the weighted aggregated final model, testing loss: 9.2473e-01, testing acc: 67.82% ( 6782/10000), training loss: 1.08, training acc: 62.16%
[24/11/24 14:00:41, INFO, FSL_SAGE_main.py:main():353]  > R 41, for the weighted aggregated final model, testing loss: 8.9677e-01, testing acc: 68.26% ( 6826/10000), training loss: 1.03, training acc: 63.36%
[24/11/24 14:01:07, INFO, FSL_SAGE_main.py:main():353]  > R 42, for the weighted aggregated final model, testing loss: 8.7480e-01, testing acc: 69.40% ( 6940/10000), training loss: 1.02, training acc: 63.75%
[24/11/24 14:01:32, INFO, FSL_SAGE_main.py:main():353]  > R 43, for the weighted aggregated final model, testing loss: 8.9740e-01, testing acc: 68.57% ( 6857/10000), training loss: 1.03, training acc: 63.62%
[24/11/24 14:01:58, INFO, FSL_SAGE_main.py:main():353]  > R 44, for the weighted aggregated final model, testing loss: 8.7427e-01, testing acc: 69.33% ( 6933/10000), training loss: 1.02, training acc: 63.99%
[24/11/24 14:02:23, INFO, FSL_SAGE_main.py:main():353]  > R 45, for the weighted aggregated final model, testing loss: 8.7629e-01, testing acc: 69.38% ( 6938/10000), training loss: 1.02, training acc: 64.21%
[24/11/24 14:02:49, INFO, FSL_SAGE_main.py:main():353]  > R 46, for the weighted aggregated final model, testing loss: 8.7503e-01, testing acc: 68.86% ( 6886/10000), training loss: 1.00, training acc: 64.94%
[24/11/24 14:03:14, INFO, FSL_SAGE_main.py:main():353]  > R 47, for the weighted aggregated final model, testing loss: 8.5227e-01, testing acc: 70.25% ( 7025/10000), training loss: 0.98, training acc: 65.42%
[24/11/24 14:03:39, INFO, FSL_SAGE_main.py:main():353]  > R 48, for the weighted aggregated final model, testing loss: 8.5226e-01, testing acc: 70.38% ( 7038/10000), training loss: 0.98, training acc: 65.17%
[24/11/24 14:04:05, INFO, FSL_SAGE_main.py:main():353]  > R 49, for the weighted aggregated final model, testing loss: 8.6383e-01, testing acc: 69.53% ( 6953/10000), training loss: 0.99, training acc: 64.97%
[24/11/24 14:04:33, INFO, FSL_SAGE_main.py:main():353]  > R 50, for the weighted aggregated final model, testing loss: 8.6595e-01, testing acc: 70.01% ( 7001/10000), training loss: 0.98, training acc: 65.59%
[24/11/24 14:04:59, INFO, FSL_SAGE_main.py:main():353]  > R 51, for the weighted aggregated final model, testing loss: 8.4957e-01, testing acc: 70.43% ( 7043/10000), training loss: 0.97, training acc: 65.68%
[24/11/24 14:05:24, INFO, FSL_SAGE_main.py:main():353]  > R 52, for the weighted aggregated final model, testing loss: 8.5285e-01, testing acc: 70.41% ( 7041/10000), training loss: 0.99, training acc: 65.56%
[24/11/24 14:05:50, INFO, FSL_SAGE_main.py:main():353]  > R 53, for the weighted aggregated final model, testing loss: 8.3093e-01, testing acc: 71.09% ( 7109/10000), training loss: 0.94, training acc: 66.56%
[24/11/24 14:06:15, INFO, FSL_SAGE_main.py:main():353]  > R 54, for the weighted aggregated final model, testing loss: 8.3318e-01, testing acc: 71.07% ( 7107/10000), training loss: 0.97, training acc: 66.13%
[24/11/24 14:06:40, INFO, FSL_SAGE_main.py:main():353]  > R 55, for the weighted aggregated final model, testing loss: 8.1186e-01, testing acc: 71.56% ( 7156/10000), training loss: 0.94, training acc: 66.98%
[24/11/24 14:07:06, INFO, FSL_SAGE_main.py:main():353]  > R 56, for the weighted aggregated final model, testing loss: 8.2848e-01, testing acc: 71.41% ( 7141/10000), training loss: 0.94, training acc: 66.77%
[24/11/24 14:07:31, INFO, FSL_SAGE_main.py:main():353]  > R 57, for the weighted aggregated final model, testing loss: 8.3361e-01, testing acc: 71.34% ( 7134/10000), training loss: 0.94, training acc: 66.88%
[24/11/24 14:07:57, INFO, FSL_SAGE_main.py:main():353]  > R 58, for the weighted aggregated final model, testing loss: 8.1439e-01, testing acc: 71.68% ( 7168/10000), training loss: 0.94, training acc: 67.05%
[24/11/24 14:08:22, INFO, FSL_SAGE_main.py:main():353]  > R 59, for the weighted aggregated final model, testing loss: 8.0843e-01, testing acc: 71.69% ( 7169/10000), training loss: 0.91, training acc: 68.17%
[24/11/24 14:08:51, INFO, FSL_SAGE_main.py:main():353]  > R 60, for the weighted aggregated final model, testing loss: 8.0467e-01, testing acc: 72.15% ( 7215/10000), training loss: 0.93, training acc: 67.40%
[24/11/24 14:09:16, INFO, FSL_SAGE_main.py:main():353]  > R 61, for the weighted aggregated final model, testing loss: 7.9679e-01, testing acc: 72.59% ( 7259/10000), training loss: 0.91, training acc: 67.80%
[24/11/24 14:09:42, INFO, FSL_SAGE_main.py:main():353]  > R 62, for the weighted aggregated final model, testing loss: 7.8628e-01, testing acc: 72.72% ( 7272/10000), training loss: 0.90, training acc: 68.45%
[24/11/24 14:10:07, INFO, FSL_SAGE_main.py:main():353]  > R 63, for the weighted aggregated final model, testing loss: 7.9679e-01, testing acc: 72.37% ( 7237/10000), training loss: 0.92, training acc: 67.83%
[24/11/24 14:10:33, INFO, FSL_SAGE_main.py:main():353]  > R 64, for the weighted aggregated final model, testing loss: 7.9438e-01, testing acc: 72.83% ( 7283/10000), training loss: 0.92, training acc: 67.73%
[24/11/24 14:10:58, INFO, FSL_SAGE_main.py:main():353]  > R 65, for the weighted aggregated final model, testing loss: 7.8498e-01, testing acc: 72.85% ( 7285/10000), training loss: 0.90, training acc: 68.14%
[24/11/24 14:11:23, INFO, FSL_SAGE_main.py:main():353]  > R 66, for the weighted aggregated final model, testing loss: 7.8267e-01, testing acc: 73.01% ( 7301/10000), training loss: 0.90, training acc: 68.30%
[24/11/24 14:11:49, INFO, FSL_SAGE_main.py:main():353]  > R 67, for the weighted aggregated final model, testing loss: 7.9961e-01, testing acc: 72.66% ( 7266/10000), training loss: 0.91, training acc: 68.13%
[24/11/24 14:12:14, INFO, FSL_SAGE_main.py:main():353]  > R 68, for the weighted aggregated final model, testing loss: 7.6390e-01, testing acc: 73.60% ( 7360/10000), training loss: 0.87, training acc: 69.41%
[24/11/24 14:12:40, INFO, FSL_SAGE_main.py:main():353]  > R 69, for the weighted aggregated final model, testing loss: 7.8316e-01, testing acc: 73.34% ( 7334/10000), training loss: 0.88, training acc: 69.01%
[24/11/24 14:13:08, INFO, FSL_SAGE_main.py:main():353]  > R 70, for the weighted aggregated final model, testing loss: 7.7006e-01, testing acc: 73.19% ( 7319/10000), training loss: 0.89, training acc: 68.83%
[24/11/24 14:13:34, INFO, FSL_SAGE_main.py:main():353]  > R 71, for the weighted aggregated final model, testing loss: 7.7346e-01, testing acc: 73.60% ( 7360/10000), training loss: 0.88, training acc: 69.44%
[24/11/24 14:13:59, INFO, FSL_SAGE_main.py:main():353]  > R 72, for the weighted aggregated final model, testing loss: 7.7479e-01, testing acc: 73.24% ( 7324/10000), training loss: 0.88, training acc: 69.12%
[24/11/24 14:14:25, INFO, FSL_SAGE_main.py:main():353]  > R 73, for the weighted aggregated final model, testing loss: 7.8305e-01, testing acc: 72.94% ( 7294/10000), training loss: 0.89, training acc: 68.79%
[24/11/24 14:14:50, INFO, FSL_SAGE_main.py:main():353]  > R 74, for the weighted aggregated final model, testing loss: 7.6311e-01, testing acc: 73.73% ( 7373/10000), training loss: 0.87, training acc: 69.51%
[24/11/24 14:15:16, INFO, FSL_SAGE_main.py:main():353]  > R 75, for the weighted aggregated final model, testing loss: 7.4604e-01, testing acc: 74.41% ( 7441/10000), training loss: 0.86, training acc: 70.00%
[24/11/24 14:15:41, INFO, FSL_SAGE_main.py:main():353]  > R 76, for the weighted aggregated final model, testing loss: 7.4681e-01, testing acc: 74.74% ( 7474/10000), training loss: 0.87, training acc: 69.36%
[24/11/24 14:16:06, INFO, FSL_SAGE_main.py:main():353]  > R 77, for the weighted aggregated final model, testing loss: 7.6434e-01, testing acc: 73.44% ( 7344/10000), training loss: 0.88, training acc: 68.96%
[24/11/24 14:16:32, INFO, FSL_SAGE_main.py:main():353]  > R 78, for the weighted aggregated final model, testing loss: 7.4740e-01, testing acc: 73.58% ( 7358/10000), training loss: 0.87, training acc: 69.65%
[24/11/24 14:16:57, INFO, FSL_SAGE_main.py:main():353]  > R 79, for the weighted aggregated final model, testing loss: 7.5829e-01, testing acc: 73.24% ( 7324/10000), training loss: 0.86, training acc: 69.50%
[24/11/24 14:17:26, INFO, FSL_SAGE_main.py:main():353]  > R 80, for the weighted aggregated final model, testing loss: 7.5010e-01, testing acc: 74.11% ( 7411/10000), training loss: 0.86, training acc: 69.98%
[24/11/24 14:17:51, INFO, FSL_SAGE_main.py:main():353]  > R 81, for the weighted aggregated final model, testing loss: 7.4342e-01, testing acc: 74.18% ( 7418/10000), training loss: 0.85, training acc: 69.99%
[24/11/24 14:18:16, INFO, FSL_SAGE_main.py:main():353]  > R 82, for the weighted aggregated final model, testing loss: 7.2770e-01, testing acc: 74.99% ( 7499/10000), training loss: 0.86, training acc: 69.91%
[24/11/24 14:18:42, INFO, FSL_SAGE_main.py:main():353]  > R 83, for the weighted aggregated final model, testing loss: 7.3892e-01, testing acc: 74.85% ( 7485/10000), training loss: 0.85, training acc: 70.55%
[24/11/24 14:19:07, INFO, FSL_SAGE_main.py:main():353]  > R 84, for the weighted aggregated final model, testing loss: 7.1887e-01, testing acc: 75.08% ( 7508/10000), training loss: 0.83, training acc: 70.90%
[24/11/24 14:19:32, INFO, FSL_SAGE_main.py:main():353]  > R 85, for the weighted aggregated final model, testing loss: 7.2778e-01, testing acc: 74.91% ( 7491/10000), training loss: 0.84, training acc: 70.31%
[24/11/24 14:19:58, INFO, FSL_SAGE_main.py:main():353]  > R 86, for the weighted aggregated final model, testing loss: 7.2272e-01, testing acc: 75.24% ( 7524/10000), training loss: 0.84, training acc: 70.57%
[24/11/24 14:20:23, INFO, FSL_SAGE_main.py:main():353]  > R 87, for the weighted aggregated final model, testing loss: 7.3094e-01, testing acc: 74.40% ( 7440/10000), training loss: 0.84, training acc: 70.20%
[24/11/24 14:20:48, INFO, FSL_SAGE_main.py:main():353]  > R 88, for the weighted aggregated final model, testing loss: 7.2154e-01, testing acc: 74.91% ( 7491/10000), training loss: 0.84, training acc: 70.66%
[24/11/24 14:21:13, INFO, FSL_SAGE_main.py:main():353]  > R 89, for the weighted aggregated final model, testing loss: 7.3060e-01, testing acc: 74.50% ( 7450/10000), training loss: 0.85, training acc: 70.29%
[24/11/24 14:21:42, INFO, FSL_SAGE_main.py:main():353]  > R 90, for the weighted aggregated final model, testing loss: 7.1736e-01, testing acc: 75.44% ( 7544/10000), training loss: 0.82, training acc: 71.21%
[24/11/24 14:22:07, INFO, FSL_SAGE_main.py:main():353]  > R 91, for the weighted aggregated final model, testing loss: 7.2679e-01, testing acc: 75.06% ( 7506/10000), training loss: 0.83, training acc: 71.03%
[24/11/24 14:22:32, INFO, FSL_SAGE_main.py:main():353]  > R 92, for the weighted aggregated final model, testing loss: 7.3701e-01, testing acc: 75.06% ( 7506/10000), training loss: 0.83, training acc: 70.79%
[24/11/24 14:22:58, INFO, FSL_SAGE_main.py:main():353]  > R 93, for the weighted aggregated final model, testing loss: 7.1496e-01, testing acc: 75.45% ( 7545/10000), training loss: 0.82, training acc: 71.18%
[24/11/24 14:23:23, INFO, FSL_SAGE_main.py:main():353]  > R 94, for the weighted aggregated final model, testing loss: 7.1288e-01, testing acc: 75.48% ( 7548/10000), training loss: 0.81, training acc: 71.59%
[24/11/24 14:23:48, INFO, FSL_SAGE_main.py:main():353]  > R 95, for the weighted aggregated final model, testing loss: 7.2098e-01, testing acc: 75.27% ( 7527/10000), training loss: 0.81, training acc: 71.38%
[24/11/24 14:24:13, INFO, FSL_SAGE_main.py:main():353]  > R 96, for the weighted aggregated final model, testing loss: 7.0907e-01, testing acc: 75.79% ( 7579/10000), training loss: 0.80, training acc: 71.92%
[24/11/24 14:24:39, INFO, FSL_SAGE_main.py:main():353]  > R 97, for the weighted aggregated final model, testing loss: 7.2907e-01, testing acc: 74.93% ( 7493/10000), training loss: 0.82, training acc: 71.40%
[24/11/24 14:25:04, INFO, FSL_SAGE_main.py:main():353]  > R 98, for the weighted aggregated final model, testing loss: 7.1689e-01, testing acc: 75.63% ( 7563/10000), training loss: 0.82, training acc: 71.12%
[24/11/24 14:25:29, INFO, FSL_SAGE_main.py:main():353]  > R 99, for the weighted aggregated final model, testing loss: 6.8984e-01, testing acc: 76.28% ( 7628/10000), training loss: 0.80, training acc: 71.78%
[24/11/24 14:25:58, INFO, FSL_SAGE_main.py:main():353]  > R 100, for the weighted aggregated final model, testing loss: 7.1694e-01, testing acc: 75.20% ( 7520/10000), training loss: 0.82, training acc: 71.45%
[24/11/24 14:26:23, INFO, FSL_SAGE_main.py:main():353]  > R 101, for the weighted aggregated final model, testing loss: 7.2428e-01, testing acc: 74.90% ( 7490/10000), training loss: 0.82, training acc: 70.89%
[24/11/24 14:26:48, INFO, FSL_SAGE_main.py:main():353]  > R 102, for the weighted aggregated final model, testing loss: 7.1799e-01, testing acc: 75.44% ( 7544/10000), training loss: 0.82, training acc: 71.41%
[24/11/24 14:27:14, INFO, FSL_SAGE_main.py:main():353]  > R 103, for the weighted aggregated final model, testing loss: 7.3197e-01, testing acc: 74.75% ( 7475/10000), training loss: 0.84, training acc: 70.42%
[24/11/24 14:27:39, INFO, FSL_SAGE_main.py:main():353]  > R 104, for the weighted aggregated final model, testing loss: 7.0716e-01, testing acc: 75.54% ( 7554/10000), training loss: 0.82, training acc: 71.27%
[24/11/24 14:28:04, INFO, FSL_SAGE_main.py:main():353]  > R 105, for the weighted aggregated final model, testing loss: 6.9239e-01, testing acc: 75.79% ( 7579/10000), training loss: 0.79, training acc: 72.29%
[24/11/24 14:28:29, INFO, FSL_SAGE_main.py:main():353]  > R 106, for the weighted aggregated final model, testing loss: 7.1396e-01, testing acc: 75.22% ( 7522/10000), training loss: 0.82, training acc: 70.99%
[24/11/24 14:28:55, INFO, FSL_SAGE_main.py:main():353]  > R 107, for the weighted aggregated final model, testing loss: 6.8788e-01, testing acc: 76.12% ( 7612/10000), training loss: 0.78, training acc: 72.51%
[24/11/24 14:29:20, INFO, FSL_SAGE_main.py:main():353]  > R 108, for the weighted aggregated final model, testing loss: 6.9322e-01, testing acc: 75.94% ( 7594/10000), training loss: 0.79, training acc: 72.54%
[24/11/24 14:29:45, INFO, FSL_SAGE_main.py:main():353]  > R 109, for the weighted aggregated final model, testing loss: 6.9401e-01, testing acc: 76.01% ( 7601/10000), training loss: 0.79, training acc: 72.10%
[24/11/24 14:30:14, INFO, FSL_SAGE_main.py:main():353]  > R 110, for the weighted aggregated final model, testing loss: 6.9434e-01, testing acc: 75.81% ( 7581/10000), training loss: 0.78, training acc: 72.23%
[24/11/24 14:30:39, INFO, FSL_SAGE_main.py:main():353]  > R 111, for the weighted aggregated final model, testing loss: 6.8788e-01, testing acc: 76.16% ( 7616/10000), training loss: 0.78, training acc: 72.62%
[24/11/24 14:31:04, INFO, FSL_SAGE_main.py:main():353]  > R 112, for the weighted aggregated final model, testing loss: 6.9624e-01, testing acc: 75.69% ( 7569/10000), training loss: 0.80, training acc: 71.85%
[24/11/24 14:31:30, INFO, FSL_SAGE_main.py:main():353]  > R 113, for the weighted aggregated final model, testing loss: 6.8124e-01, testing acc: 76.36% ( 7636/10000), training loss: 0.78, training acc: 72.82%
[24/11/24 14:31:55, INFO, FSL_SAGE_main.py:main():353]  > R 114, for the weighted aggregated final model, testing loss: 6.7634e-01, testing acc: 76.69% ( 7669/10000), training loss: 0.78, training acc: 72.73%
[24/11/24 14:32:20, INFO, FSL_SAGE_main.py:main():353]  > R 115, for the weighted aggregated final model, testing loss: 6.8063e-01, testing acc: 76.46% ( 7646/10000), training loss: 0.79, training acc: 72.42%
[24/11/24 14:32:46, INFO, FSL_SAGE_main.py:main():353]  > R 116, for the weighted aggregated final model, testing loss: 6.9131e-01, testing acc: 75.76% ( 7576/10000), training loss: 0.77, training acc: 72.88%
[24/11/24 14:33:11, INFO, FSL_SAGE_main.py:main():353]  > R 117, for the weighted aggregated final model, testing loss: 6.8005e-01, testing acc: 76.32% ( 7632/10000), training loss: 0.76, training acc: 73.13%
[24/11/24 14:33:36, INFO, FSL_SAGE_main.py:main():353]  > R 118, for the weighted aggregated final model, testing loss: 6.7869e-01, testing acc: 76.16% ( 7616/10000), training loss: 0.76, training acc: 73.49%
[24/11/24 14:34:01, INFO, FSL_SAGE_main.py:main():353]  > R 119, for the weighted aggregated final model, testing loss: 6.7429e-01, testing acc: 76.57% ( 7657/10000), training loss: 0.75, training acc: 73.45%
[24/11/24 14:34:30, INFO, FSL_SAGE_main.py:main():353]  > R 120, for the weighted aggregated final model, testing loss: 6.8791e-01, testing acc: 75.88% ( 7588/10000), training loss: 0.77, training acc: 72.79%
[24/11/24 14:34:55, INFO, FSL_SAGE_main.py:main():353]  > R 121, for the weighted aggregated final model, testing loss: 6.9887e-01, testing acc: 75.79% ( 7579/10000), training loss: 0.80, training acc: 71.81%
[24/11/24 14:35:20, INFO, FSL_SAGE_main.py:main():353]  > R 122, for the weighted aggregated final model, testing loss: 6.7573e-01, testing acc: 76.75% ( 7675/10000), training loss: 0.77, training acc: 73.32%
[24/11/24 14:35:46, INFO, FSL_SAGE_main.py:main():353]  > R 123, for the weighted aggregated final model, testing loss: 6.8521e-01, testing acc: 76.25% ( 7625/10000), training loss: 0.77, training acc: 72.93%
[24/11/24 14:36:11, INFO, FSL_SAGE_main.py:main():353]  > R 124, for the weighted aggregated final model, testing loss: 6.7190e-01, testing acc: 77.43% ( 7743/10000), training loss: 0.75, training acc: 73.90%
[24/11/24 14:36:36, INFO, FSL_SAGE_main.py:main():353]  > R 125, for the weighted aggregated final model, testing loss: 6.6695e-01, testing acc: 76.84% ( 7684/10000), training loss: 0.76, training acc: 73.40%
[24/11/24 14:37:01, INFO, FSL_SAGE_main.py:main():353]  > R 126, for the weighted aggregated final model, testing loss: 6.6940e-01, testing acc: 76.96% ( 7696/10000), training loss: 0.75, training acc: 73.54%
[24/11/24 14:37:27, INFO, FSL_SAGE_main.py:main():353]  > R 127, for the weighted aggregated final model, testing loss: 6.7275e-01, testing acc: 76.70% ( 7670/10000), training loss: 0.76, training acc: 73.38%
[24/11/24 14:37:52, INFO, FSL_SAGE_main.py:main():353]  > R 128, for the weighted aggregated final model, testing loss: 6.6528e-01, testing acc: 76.96% ( 7696/10000), training loss: 0.76, training acc: 73.46%
[24/11/24 14:38:17, INFO, FSL_SAGE_main.py:main():353]  > R 129, for the weighted aggregated final model, testing loss: 6.8100e-01, testing acc: 76.56% ( 7656/10000), training loss: 0.76, training acc: 73.20%
[24/11/24 14:38:46, INFO, FSL_SAGE_main.py:main():353]  > R 130, for the weighted aggregated final model, testing loss: 6.8289e-01, testing acc: 76.59% ( 7659/10000), training loss: 0.76, training acc: 73.59%
[24/11/24 14:39:11, INFO, FSL_SAGE_main.py:main():353]  > R 131, for the weighted aggregated final model, testing loss: 6.6367e-01, testing acc: 76.96% ( 7696/10000), training loss: 0.75, training acc: 73.88%
[24/11/24 14:39:36, INFO, FSL_SAGE_main.py:main():353]  > R 132, for the weighted aggregated final model, testing loss: 6.6823e-01, testing acc: 77.08% ( 7708/10000), training loss: 0.75, training acc: 73.74%
[24/11/24 14:40:02, INFO, FSL_SAGE_main.py:main():353]  > R 133, for the weighted aggregated final model, testing loss: 6.9302e-01, testing acc: 75.49% ( 7549/10000), training loss: 0.79, training acc: 72.24%
[24/11/24 14:40:27, INFO, FSL_SAGE_main.py:main():353]  > R 134, for the weighted aggregated final model, testing loss: 6.6934e-01, testing acc: 77.21% ( 7721/10000), training loss: 0.75, training acc: 73.69%
[24/11/24 14:40:52, INFO, FSL_SAGE_main.py:main():353]  > R 135, for the weighted aggregated final model, testing loss: 6.6922e-01, testing acc: 77.36% ( 7736/10000), training loss: 0.75, training acc: 74.09%
[24/11/24 14:41:18, INFO, FSL_SAGE_main.py:main():353]  > R 136, for the weighted aggregated final model, testing loss: 6.7505e-01, testing acc: 76.72% ( 7672/10000), training loss: 0.76, training acc: 73.30%
[24/11/24 14:41:43, INFO, FSL_SAGE_main.py:main():353]  > R 137, for the weighted aggregated final model, testing loss: 6.7081e-01, testing acc: 77.07% ( 7707/10000), training loss: 0.75, training acc: 73.81%
[24/11/24 14:42:08, INFO, FSL_SAGE_main.py:main():353]  > R 138, for the weighted aggregated final model, testing loss: 6.5442e-01, testing acc: 77.44% ( 7744/10000), training loss: 0.73, training acc: 74.49%
[24/11/24 14:42:33, INFO, FSL_SAGE_main.py:main():353]  > R 139, for the weighted aggregated final model, testing loss: 6.5879e-01, testing acc: 77.00% ( 7700/10000), training loss: 0.74, training acc: 73.96%
[24/11/24 14:43:02, INFO, FSL_SAGE_main.py:main():353]  > R 140, for the weighted aggregated final model, testing loss: 6.6420e-01, testing acc: 77.31% ( 7731/10000), training loss: 0.75, training acc: 73.93%
[24/11/24 14:43:27, INFO, FSL_SAGE_main.py:main():353]  > R 141, for the weighted aggregated final model, testing loss: 6.7677e-01, testing acc: 76.87% ( 7687/10000), training loss: 0.76, training acc: 73.37%
[24/11/24 14:43:52, INFO, FSL_SAGE_main.py:main():353]  > R 142, for the weighted aggregated final model, testing loss: 6.6108e-01, testing acc: 77.30% ( 7730/10000), training loss: 0.75, training acc: 73.65%
[24/11/24 14:44:18, INFO, FSL_SAGE_main.py:main():353]  > R 143, for the weighted aggregated final model, testing loss: 6.6353e-01, testing acc: 77.29% ( 7729/10000), training loss: 0.75, training acc: 73.92%
[24/11/24 14:44:43, INFO, FSL_SAGE_main.py:main():353]  > R 144, for the weighted aggregated final model, testing loss: 6.5322e-01, testing acc: 78.07% ( 7807/10000), training loss: 0.73, training acc: 74.34%
[24/11/24 14:45:08, INFO, FSL_SAGE_main.py:main():353]  > R 145, for the weighted aggregated final model, testing loss: 6.5503e-01, testing acc: 77.64% ( 7764/10000), training loss: 0.74, training acc: 74.30%
[24/11/24 14:45:33, INFO, FSL_SAGE_main.py:main():353]  > R 146, for the weighted aggregated final model, testing loss: 6.5610e-01, testing acc: 77.49% ( 7749/10000), training loss: 0.73, training acc: 74.40%
[24/11/24 14:45:59, INFO, FSL_SAGE_main.py:main():353]  > R 147, for the weighted aggregated final model, testing loss: 6.5521e-01, testing acc: 77.48% ( 7748/10000), training loss: 0.73, training acc: 74.47%
[24/11/24 14:46:24, INFO, FSL_SAGE_main.py:main():353]  > R 148, for the weighted aggregated final model, testing loss: 6.6185e-01, testing acc: 77.30% ( 7730/10000), training loss: 0.74, training acc: 74.07%
[24/11/24 14:46:49, INFO, FSL_SAGE_main.py:main():353]  > R 149, for the weighted aggregated final model, testing loss: 6.5458e-01, testing acc: 77.55% ( 7755/10000), training loss: 0.74, training acc: 73.85%
[24/11/24 14:47:17, INFO, FSL_SAGE_main.py:main():353]  > R 150, for the weighted aggregated final model, testing loss: 6.7619e-01, testing acc: 76.92% ( 7692/10000), training loss: 0.74, training acc: 74.18%
[24/11/24 14:47:43, INFO, FSL_SAGE_main.py:main():353]  > R 151, for the weighted aggregated final model, testing loss: 6.5377e-01, testing acc: 76.99% ( 7699/10000), training loss: 0.73, training acc: 74.66%
[24/11/24 14:48:08, INFO, FSL_SAGE_main.py:main():353]  > R 152, for the weighted aggregated final model, testing loss: 6.6394e-01, testing acc: 77.09% ( 7709/10000), training loss: 0.73, training acc: 74.54%
[24/11/24 14:48:33, INFO, FSL_SAGE_main.py:main():353]  > R 153, for the weighted aggregated final model, testing loss: 6.5965e-01, testing acc: 77.61% ( 7761/10000), training loss: 0.72, training acc: 74.74%
[24/11/24 14:48:59, INFO, FSL_SAGE_main.py:main():353]  > R 154, for the weighted aggregated final model, testing loss: 6.5606e-01, testing acc: 77.34% ( 7734/10000), training loss: 0.73, training acc: 74.62%
[24/11/24 14:49:24, INFO, FSL_SAGE_main.py:main():353]  > R 155, for the weighted aggregated final model, testing loss: 6.8561e-01, testing acc: 76.70% ( 7670/10000), training loss: 0.74, training acc: 73.78%
[24/11/24 14:49:50, INFO, FSL_SAGE_main.py:main():353]  > R 156, for the weighted aggregated final model, testing loss: 6.4500e-01, testing acc: 77.57% ( 7757/10000), training loss: 0.72, training acc: 74.98%
[24/11/24 14:50:15, INFO, FSL_SAGE_main.py:main():353]  > R 157, for the weighted aggregated final model, testing loss: 6.4488e-01, testing acc: 77.80% ( 7780/10000), training loss: 0.73, training acc: 74.68%
[24/11/24 14:50:40, INFO, FSL_SAGE_main.py:main():353]  > R 158, for the weighted aggregated final model, testing loss: 6.5814e-01, testing acc: 77.38% ( 7738/10000), training loss: 0.73, training acc: 74.36%
[24/11/24 14:51:06, INFO, FSL_SAGE_main.py:main():353]  > R 159, for the weighted aggregated final model, testing loss: 6.5872e-01, testing acc: 77.07% ( 7707/10000), training loss: 0.73, training acc: 74.47%
[24/11/24 14:51:34, INFO, FSL_SAGE_main.py:main():353]  > R 160, for the weighted aggregated final model, testing loss: 6.6767e-01, testing acc: 77.30% ( 7730/10000), training loss: 0.75, training acc: 73.68%
[24/11/24 14:51:59, INFO, FSL_SAGE_main.py:main():353]  > R 161, for the weighted aggregated final model, testing loss: 6.5341e-01, testing acc: 77.81% ( 7781/10000), training loss: 0.73, training acc: 74.77%
[24/11/24 14:52:25, INFO, FSL_SAGE_main.py:main():353]  > R 162, for the weighted aggregated final model, testing loss: 6.6081e-01, testing acc: 77.77% ( 7777/10000), training loss: 0.73, training acc: 74.48%
[24/11/24 14:52:50, INFO, FSL_SAGE_main.py:main():353]  > R 163, for the weighted aggregated final model, testing loss: 6.5085e-01, testing acc: 77.67% ( 7767/10000), training loss: 0.72, training acc: 74.64%
[24/11/24 14:53:15, INFO, FSL_SAGE_main.py:main():353]  > R 164, for the weighted aggregated final model, testing loss: 6.5128e-01, testing acc: 77.49% ( 7749/10000), training loss: 0.72, training acc: 74.80%
[24/11/24 14:53:41, INFO, FSL_SAGE_main.py:main():353]  > R 165, for the weighted aggregated final model, testing loss: 6.4223e-01, testing acc: 77.99% ( 7799/10000), training loss: 0.72, training acc: 74.91%
[24/11/24 14:54:06, INFO, FSL_SAGE_main.py:main():353]  > R 166, for the weighted aggregated final model, testing loss: 6.3868e-01, testing acc: 78.21% ( 7821/10000), training loss: 0.71, training acc: 75.05%
[24/11/24 14:54:31, INFO, FSL_SAGE_main.py:main():353]  > R 167, for the weighted aggregated final model, testing loss: 6.4055e-01, testing acc: 78.11% ( 7811/10000), training loss: 0.71, training acc: 75.17%
[24/11/24 14:54:57, INFO, FSL_SAGE_main.py:main():353]  > R 168, for the weighted aggregated final model, testing loss: 6.4396e-01, testing acc: 77.66% ( 7766/10000), training loss: 0.71, training acc: 74.86%
[24/11/24 14:55:22, INFO, FSL_SAGE_main.py:main():353]  > R 169, for the weighted aggregated final model, testing loss: 6.3962e-01, testing acc: 77.86% ( 7786/10000), training loss: 0.71, training acc: 75.11%
[24/11/24 14:55:50, INFO, FSL_SAGE_main.py:main():353]  > R 170, for the weighted aggregated final model, testing loss: 6.3223e-01, testing acc: 78.39% ( 7839/10000), training loss: 0.70, training acc: 75.39%
[24/11/24 14:56:16, INFO, FSL_SAGE_main.py:main():353]  > R 171, for the weighted aggregated final model, testing loss: 6.3561e-01, testing acc: 78.12% ( 7812/10000), training loss: 0.70, training acc: 75.39%
[24/11/24 14:56:41, INFO, FSL_SAGE_main.py:main():353]  > R 172, for the weighted aggregated final model, testing loss: 6.3792e-01, testing acc: 78.04% ( 7804/10000), training loss: 0.71, training acc: 74.86%
[24/11/24 14:57:06, INFO, FSL_SAGE_main.py:main():353]  > R 173, for the weighted aggregated final model, testing loss: 6.5003e-01, testing acc: 77.48% ( 7748/10000), training loss: 0.71, training acc: 75.06%
[24/11/24 14:57:32, INFO, FSL_SAGE_main.py:main():353]  > R 174, for the weighted aggregated final model, testing loss: 6.5085e-01, testing acc: 77.91% ( 7791/10000), training loss: 0.72, training acc: 74.56%
[24/11/24 14:57:57, INFO, FSL_SAGE_main.py:main():353]  > R 175, for the weighted aggregated final model, testing loss: 6.3272e-01, testing acc: 78.39% ( 7839/10000), training loss: 0.71, training acc: 74.77%
[24/11/24 14:58:22, INFO, FSL_SAGE_main.py:main():353]  > R 176, for the weighted aggregated final model, testing loss: 6.3128e-01, testing acc: 78.14% ( 7814/10000), training loss: 0.70, training acc: 75.57%
[24/11/24 14:58:48, INFO, FSL_SAGE_main.py:main():353]  > R 177, for the weighted aggregated final model, testing loss: 6.4972e-01, testing acc: 77.34% ( 7734/10000), training loss: 0.73, training acc: 74.31%
[24/11/24 14:59:13, INFO, FSL_SAGE_main.py:main():353]  > R 178, for the weighted aggregated final model, testing loss: 6.3588e-01, testing acc: 78.18% ( 7818/10000), training loss: 0.70, training acc: 75.25%
[24/11/24 14:59:38, INFO, FSL_SAGE_main.py:main():353]  > R 179, for the weighted aggregated final model, testing loss: 6.3764e-01, testing acc: 77.90% ( 7790/10000), training loss: 0.71, training acc: 75.40%
[24/11/24 15:00:07, INFO, FSL_SAGE_main.py:main():353]  > R 180, for the weighted aggregated final model, testing loss: 6.3361e-01, testing acc: 77.94% ( 7794/10000), training loss: 0.71, training acc: 75.42%
[24/11/24 15:00:32, INFO, FSL_SAGE_main.py:main():353]  > R 181, for the weighted aggregated final model, testing loss: 6.4253e-01, testing acc: 78.03% ( 7803/10000), training loss: 0.72, training acc: 74.93%
[24/11/24 15:00:57, INFO, FSL_SAGE_main.py:main():353]  > R 182, for the weighted aggregated final model, testing loss: 6.2682e-01, testing acc: 78.31% ( 7831/10000), training loss: 0.70, training acc: 75.69%
[24/11/24 15:01:23, INFO, FSL_SAGE_main.py:main():353]  > R 183, for the weighted aggregated final model, testing loss: 6.2254e-01, testing acc: 78.46% ( 7846/10000), training loss: 0.69, training acc: 75.62%
[24/11/24 15:01:48, INFO, FSL_SAGE_main.py:main():353]  > R 184, for the weighted aggregated final model, testing loss: 6.5677e-01, testing acc: 77.50% ( 7750/10000), training loss: 0.72, training acc: 74.94%
[24/11/24 15:02:13, INFO, FSL_SAGE_main.py:main():353]  > R 185, for the weighted aggregated final model, testing loss: 6.3774e-01, testing acc: 77.95% ( 7795/10000), training loss: 0.71, training acc: 75.18%
[24/11/24 15:02:38, INFO, FSL_SAGE_main.py:main():353]  > R 186, for the weighted aggregated final model, testing loss: 6.4970e-01, testing acc: 77.54% ( 7754/10000), training loss: 0.72, training acc: 74.87%
[24/11/24 15:03:04, INFO, FSL_SAGE_main.py:main():353]  > R 187, for the weighted aggregated final model, testing loss: 6.3318e-01, testing acc: 78.48% ( 7848/10000), training loss: 0.69, training acc: 75.69%
[24/11/24 15:03:29, INFO, FSL_SAGE_main.py:main():353]  > R 188, for the weighted aggregated final model, testing loss: 6.3301e-01, testing acc: 78.50% ( 7850/10000), training loss: 0.69, training acc: 76.11%
[24/11/24 15:03:54, INFO, FSL_SAGE_main.py:main():353]  > R 189, for the weighted aggregated final model, testing loss: 6.2311e-01, testing acc: 78.33% ( 7833/10000), training loss: 0.68, training acc: 76.03%
[24/11/24 15:04:22, INFO, FSL_SAGE_main.py:main():353]  > R 190, for the weighted aggregated final model, testing loss: 6.4338e-01, testing acc: 77.57% ( 7757/10000), training loss: 0.70, training acc: 75.45%
[24/11/24 15:04:48, INFO, FSL_SAGE_main.py:main():353]  > R 191, for the weighted aggregated final model, testing loss: 6.4083e-01, testing acc: 78.13% ( 7813/10000), training loss: 0.70, training acc: 75.50%
[24/11/24 15:05:13, INFO, FSL_SAGE_main.py:main():353]  > R 192, for the weighted aggregated final model, testing loss: 6.4143e-01, testing acc: 78.02% ( 7802/10000), training loss: 0.70, training acc: 75.52%
[24/11/24 15:05:39, INFO, FSL_SAGE_main.py:main():353]  > R 193, for the weighted aggregated final model, testing loss: 6.4259e-01, testing acc: 78.09% ( 7809/10000), training loss: 0.71, training acc: 75.37%
[24/11/24 15:06:04, INFO, FSL_SAGE_main.py:main():353]  > R 194, for the weighted aggregated final model, testing loss: 6.2502e-01, testing acc: 78.93% ( 7893/10000), training loss: 0.69, training acc: 75.90%
[24/11/24 15:06:29, INFO, FSL_SAGE_main.py:main():353]  > R 195, for the weighted aggregated final model, testing loss: 6.3182e-01, testing acc: 78.25% ( 7825/10000), training loss: 0.69, training acc: 75.76%
[24/11/24 15:06:54, INFO, FSL_SAGE_main.py:main():353]  > R 196, for the weighted aggregated final model, testing loss: 6.3431e-01, testing acc: 78.02% ( 7802/10000), training loss: 0.69, training acc: 75.94%
[24/11/24 15:07:20, INFO, FSL_SAGE_main.py:main():353]  > R 197, for the weighted aggregated final model, testing loss: 6.2248e-01, testing acc: 78.87% ( 7887/10000), training loss: 0.68, training acc: 76.24%
[24/11/24 15:07:45, INFO, FSL_SAGE_main.py:main():353]  > R 198, for the weighted aggregated final model, testing loss: 6.2230e-01, testing acc: 78.71% ( 7871/10000), training loss: 0.68, training acc: 76.26%
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():353]  > R 199, for the weighted aggregated final model, testing loss: 6.4581e-01, testing acc: 78.11% ( 7811/10000), training loss: 0.70, training acc: 75.36%
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():355] The total running time for all rounds is 5130.68 seconds
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():365] [NOTICE] Saved results to '../saves/cifar-iid-K3U3E1BR5-200-241124-134238/results.json'.
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():382] Testing accuracy: [0.2226, 0.3167, 0.3807, 0.3933, 0.4045, 0.4045, 0.4111, 0.4418, 0.4553, 0.4323, 0.4977, 0.4895, 0.5172, 0.5376, 0.5484, 0.5442, 0.5758, 0.5341, 0.5743, 0.5853, 0.5974, 0.6133, 0.6193, 0.6197, 0.6282, 0.6362, 0.6379, 0.6401, 0.6368, 0.6481, 0.644, 0.6597, 0.6539, 0.6631, 0.6595, 0.674, 0.6758, 0.6709, 0.6775, 0.6754, 0.6782, 0.6826, 0.694, 0.6857, 0.6933, 0.6938, 0.6886, 0.7025, 0.7038, 0.6953, 0.7001, 0.7043, 0.7041, 0.7109, 0.7107, 0.7156, 0.7141, 0.7134, 0.7168, 0.7169, 0.7215, 0.7259, 0.7272, 0.7237, 0.7283, 0.7285, 0.7301, 0.7266, 0.736, 0.7334, 0.7319, 0.736, 0.7324, 0.7294, 0.7373, 0.7441, 0.7474, 0.7344, 0.7358, 0.7324, 0.7411, 0.7418, 0.7499, 0.7485, 0.7508, 0.7491, 0.7524, 0.744, 0.7491, 0.745, 0.7544, 0.7506, 0.7506, 0.7545, 0.7548, 0.7527, 0.7579, 0.7493, 0.7563, 0.7628, 0.752, 0.749, 0.7544, 0.7475, 0.7554, 0.7579, 0.7522, 0.7612, 0.7594, 0.7601, 0.7581, 0.7616, 0.7569, 0.7636, 0.7669, 0.7646, 0.7576, 0.7632, 0.7616, 0.7657, 0.7588, 0.7579, 0.7675, 0.7625, 0.7743, 0.7684, 0.7696, 0.767, 0.7696, 0.7656, 0.7659, 0.7696, 0.7708, 0.7549, 0.7721, 0.7736, 0.7672, 0.7707, 0.7744, 0.77, 0.7731, 0.7687, 0.773, 0.7729, 0.7807, 0.7764, 0.7749, 0.7748, 0.773, 0.7755, 0.7692, 0.7699, 0.7709, 0.7761, 0.7734, 0.767, 0.7757, 0.778, 0.7738, 0.7707, 0.773, 0.7781, 0.7777, 0.7767, 0.7749, 0.7799, 0.7821, 0.7811, 0.7766, 0.7786, 0.7839, 0.7812, 0.7804, 0.7748, 0.7791, 0.7839, 0.7814, 0.7734, 0.7818, 0.779, 0.7794, 0.7803, 0.7831, 0.7846, 0.775, 0.7795, 0.7754, 0.7848, 0.785, 0.7833, 0.7757, 0.7813, 0.7802, 0.7809, 0.7893, 0.7825, 0.7802, 0.7887, 0.7871, 0.7811]
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():383] Testing loss: [2.0408808626706088, 1.804605674140061, 1.6825302929817876, 1.6225324823886533, 1.645298295383212, 1.635066646563856, 1.5892698885519294, 1.509764927852003, 1.4860762327532224, 1.5468513814708855, 1.3940317027176483, 1.379268130169639, 1.3182239230675032, 1.2618682263772698, 1.232937965211989, 1.2435984113548375, 1.1811069597171833, 1.2725526396232316, 1.1767788770832592, 1.1605306999592842, 1.1156584944906114, 1.0943270287936246, 1.0774691663210905, 1.0794621728643585, 1.0497066423862795, 1.0419467219823524, 1.0238980303836773, 1.0236237562155421, 1.0267557818678361, 0.9952429397196709, 1.0026540069640437, 0.9808969399597072, 0.991243852090232, 0.9575177287753625, 0.9610006333906439, 0.9439476064488858, 0.9302691575847094, 0.9295471038999437, 0.9103216913681996, 0.9176267626919324, 0.9247289435772956, 0.8967696541472326, 0.8747960728935048, 0.8974004644381849, 0.8742684601228449, 0.8762900014466877, 0.8750297057477734, 0.8522668299795706, 0.852262665199328, 0.8638304185263718, 0.8659495462345171, 0.8495701151558116, 0.8528490647484984, 0.8309332200243503, 0.833182731761208, 0.8118580625027041, 0.8284785838066777, 0.8336131866974167, 0.8143936682351028, 0.8084319603594043, 0.8046676448628872, 0.7967939127849627, 0.7862834704073169, 0.7967892243892332, 0.7943841007691396, 0.7849750496164153, 0.7826741549033153, 0.7996138703973987, 0.7638962578169907, 0.7831567729575725, 0.7700603483598444, 0.7734604221356066, 0.7747924527789973, 0.7830541428131393, 0.7631084805802454, 0.7460351439216469, 0.7468053226983999, 0.7643396160270595, 0.7474028121066999, 0.75829106720188, 0.7501046280317669, 0.7434244838696492, 0.7276956186264376, 0.7389207327667671, 0.7188743711272373, 0.7277770031102097, 0.7227198319344581, 0.730942311166208, 0.7215427401699598, 0.7305995512612259, 0.7173588166508493, 0.7267850682705264, 0.7370132902000524, 0.7149636481381669, 0.7128789172142367, 0.7209769819356218, 0.7090695130674145, 0.7290651537949526, 0.716886750882185, 0.6898391175119183, 0.7169435492044762, 0.7242767867408221, 0.7179931307895274, 0.7319693376746359, 0.7071607063842725, 0.6923946443992325, 0.713963132115859, 0.6878771461263488, 0.6932168346417101, 0.6940085027791276, 0.6943372024011009, 0.687878205051905, 0.6962416707714901, 0.6812420808061769, 0.6763385251353059, 0.6806307389766355, 0.6913062383102465, 0.6800503025326547, 0.6786905904359455, 0.6742938568320456, 0.687907467914533, 0.6988739639143401, 0.6757268004025085, 0.6852058711685712, 0.6718970989879174, 0.6669505796855009, 0.6694014694871782, 0.6727460230453105, 0.665281255788441, 0.6809950791582277, 0.6828937832313248, 0.6636685959145993, 0.6682304313665703, 0.6930189098738417, 0.6693398477155951, 0.6692182731024826, 0.6750549775890157, 0.6708138038085986, 0.6544191799586332, 0.6587899429134175, 0.6641967824742764, 0.6767720014234132, 0.6610775174973886, 0.6635276750673221, 0.6532152653494968, 0.6550326483159126, 0.6561010891123663, 0.6552108767666395, 0.6618545685387864, 0.6545753052717522, 0.6761895112598999, 0.6537739574154721, 0.6639351497722578, 0.6596516604664959, 0.6560649226737928, 0.6856123514567749, 0.645003050188475, 0.6448821927173228, 0.6581390715098079, 0.6587164541588554, 0.6676689224152625, 0.6534078528609457, 0.660809407505808, 0.6508505280259289, 0.6512818400618396, 0.6422332140463817, 0.6386788268632526, 0.6405491229099564, 0.643963418429411, 0.6396222695519652, 0.6322289524199087, 0.6356138620949998, 0.6379157078417041, 0.6500253171860417, 0.6508505091636996, 0.6327177995367895, 0.6312821088712427, 0.6497166794312151, 0.6358816521831706, 0.6376430844958825, 0.6336101323743409, 0.6425288802460779, 0.6268160196044777, 0.622544751891607, 0.6567661064335063, 0.6377398296247555, 0.6497023165980472, 0.6331788120390494, 0.6330062905444375, 0.6231060197836236, 0.6433816518210158, 0.6408275901516781, 0.6414256046844434, 0.6425930846341049, 0.6250153549864322, 0.6318158058426048, 0.6343052406099778, 0.6224829773359661, 0.6223047681247131, 0.6458059107955498]
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():384] Training accuracy: [0.20868834753390136, 0.2780711228449138, 0.32051282051282054, 0.34255370214808595, 0.3564342573702948, 0.3555742229689188, 0.3654546181847274, 0.39927597103884155, 0.40387615504620183, 0.3921356854274171, 0.438117524700988, 0.4328173126925077, 0.4563582543301732, 0.49011960478419136, 0.4926997079883195, 0.4955398215928637, 0.5136805472218888, 0.48395935837433496, 0.5220808832353294, 0.5287211488459539, 0.5421016840673627, 0.5534021360854434, 0.5635225409016361, 0.5555622224888995, 0.5642425697027881, 0.5771230849233969, 0.5834433377335093, 0.5795631825273011, 0.5774230969238769, 0.5886235449417977, 0.5990439617584703, 0.5995239809592384, 0.5999639985599424, 0.6076443057722309, 0.6068642745709828, 0.6133845353814152, 0.6262650506020241, 0.6198447937917517, 0.6285451418056722, 0.625105004200168, 0.6215648625945038, 0.6336453458138326, 0.6375455018200729, 0.6361854474178967, 0.639885595423817, 0.6421456858274331, 0.6493859754390175, 0.6542061682467298, 0.6517460698427937, 0.6496659866394656, 0.6559262370494819, 0.656826273050922, 0.655646225849034, 0.6656266250650026, 0.6612864514580583, 0.6697867914716589, 0.6677267090683627, 0.6687867514700588, 0.6705268210728429, 0.6817472698907956, 0.6740269610784432, 0.6779871194847794, 0.6844873794951798, 0.6782871314852594, 0.6772870914836594, 0.6814472578903156, 0.6830073202928117, 0.6812672506900276, 0.6940677627105084, 0.6901276051042041, 0.6882875315012601, 0.6943677747109884, 0.6911876475059002, 0.6879475179007161, 0.6951478059122365, 0.7000480019200768, 0.6935877435097404, 0.6896075843033721, 0.6965078603144126, 0.6949877995119805, 0.6997679907196288, 0.6998679947197888, 0.6991479659186367, 0.7054682187287491, 0.7090083603344134, 0.703088123524941, 0.7056682267290691, 0.7019680787231489, 0.7065682627305092, 0.702908116324653, 0.7121284851394056, 0.7103084123364934, 0.7078683147325893, 0.7117684707388295, 0.7159086363454538, 0.7138085523420937, 0.719188767550702, 0.7140285611424457, 0.7112284491379656, 0.717808712348494, 0.7144685787431497, 0.7089283571342854, 0.7140685627425097, 0.704208168326733, 0.7127485099403976, 0.7229489179567182, 0.7098883955358214, 0.7251090043601744, 0.7253690147605905, 0.7209688387535501, 0.7222888915556622, 0.7261690467618704, 0.71846873874955, 0.7281691267650706, 0.7273490939637586, 0.7242289691587663, 0.7287691507660307, 0.7312692507700308, 0.7349093963758551, 0.734489379575183, 0.7279491179647186, 0.71806872274891, 0.733229329173167, 0.7292691707668306, 0.7390295611824473, 0.734049361974479, 0.7353894155766231, 0.733809352374095, 0.734649385975439, 0.7319892795711829, 0.735929437177487, 0.7388495539821592, 0.7374294971798872, 0.7223688947557902, 0.7368694747789911, 0.7408696347853914, 0.733009320372815, 0.7381095243809752, 0.7449297971918877, 0.7396295851834074, 0.7393295731829274, 0.733669346773871, 0.7364694587783511, 0.7391895675827033, 0.7433697347893916, 0.7430097203888155, 0.7440297611904476, 0.7447497899915997, 0.7406696267850714, 0.7385095403816153, 0.7417696707868314, 0.7466498659946398, 0.7454498179927197, 0.7474098963958559, 0.7461698467938718, 0.7378095123804952, 0.749789991599664, 0.7467698707948318, 0.7435897435897436, 0.7446697867914717, 0.7367894715788632, 0.7477099083963359, 0.7447697907916316, 0.7463898555942238, 0.7480099203968159, 0.749089963598544, 0.750530021200848, 0.7517300692027681, 0.748609944397776, 0.7511100444017761, 0.7538701548061922, 0.7539301572062882, 0.748609944397776, 0.7505500220008801, 0.7456098243929757, 0.7476899075963038, 0.7557102284091364, 0.7430697227889116, 0.7525301012040482, 0.7539901596063843, 0.7541901676067043, 0.749269970798832, 0.7568702748109924, 0.7561502460098404, 0.7494499779991199, 0.7517900716028642, 0.7487499499979999, 0.7568902756110244, 0.7611104444177768, 0.7603104124164967, 0.7544901796071843, 0.7550302012080483, 0.7552102084083363, 0.7537301492059683, 0.7589903596143845, 0.7576103044121765, 0.7593503740149606, 0.7624304972198888, 0.7625905036201448, 0.7535901436057443]
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():385] Training loss: [2.09563547873315, 1.9076405769692728, 1.8321323264342835, 1.7510114890018493, 1.7853785747183493, 1.7667505683486395, 1.71974763858106, 1.6404240110144968, 1.6203612938485377, 1.6597423708165875, 1.5477069786183404, 1.5364013361263518, 1.4799668673643933, 1.4091746136735717, 1.392002093276298, 1.3848904917258342, 1.3436821420077452, 1.4113155988639852, 1.321404912392905, 1.305877957938585, 1.2800381474822533, 1.2490741120646625, 1.2230495558743562, 1.2406430033630391, 1.222363190796539, 1.1936531141210756, 1.168741803587848, 1.1831935648699754, 1.1787571171464508, 1.1517388767565175, 1.130214924120721, 1.1265254998935088, 1.1196255174301963, 1.098266489633167, 1.1022647908929044, 1.0799081084382443, 1.0549755499866476, 1.077434255269951, 1.0456884577680787, 1.0551706522172342, 1.0763805993944027, 1.0295033130330287, 1.0224300692706314, 1.0292458435657976, 1.0226737439784082, 1.0192185760151036, 0.9979122555589555, 0.9827049356380492, 0.9822376829977254, 0.9850997381841257, 0.977592393338832, 0.9743887706263982, 0.9879652463449473, 0.9443823123402875, 0.9654873327444528, 0.9420746269116875, 0.9391214515416677, 0.940961705212678, 0.9369240541191198, 0.909856728773384, 0.9311894228440205, 0.9137177371796761, 0.9033727530608043, 0.9154845876548127, 0.9184718288234779, 0.9018363047192115, 0.9016133727311482, 0.9111272186116711, 0.8732113997444851, 0.8826507976036946, 0.8894277304486767, 0.8763307857756093, 0.875354714521015, 0.8925665209008233, 0.8664920446830244, 0.8569370850958593, 0.8748277213433924, 0.8773883867809791, 0.8674253026644388, 0.8621809752539521, 0.8601865116269837, 0.8486204876851187, 0.8560144870942482, 0.8450968869163183, 0.8255920118957986, 0.8423861400165024, 0.8408529179090155, 0.84446869855012, 0.8387286620589006, 0.8455927930108766, 0.8212467034657797, 0.8339878196328044, 0.8337843912551725, 0.8172043909250022, 0.8123229502418265, 0.8140570977868621, 0.799232431343797, 0.8188093513932847, 0.8214407965123806, 0.802636090353245, 0.8203282295595902, 0.8225642780917898, 0.8240657611050982, 0.8416843595547228, 0.8168436992562758, 0.7889803105031564, 0.824135336408785, 0.7824425621493779, 0.7880911050558697, 0.7928729827015758, 0.7847081538071766, 0.784695918626761, 0.8028439207538091, 0.7822703415049245, 0.7781286055350122, 0.7853131289700516, 0.7735559128926304, 0.7597296777875672, 0.7586463025508036, 0.7549896743006379, 0.7724478478346769, 0.798384589775828, 0.7674719377328422, 0.766999569103008, 0.749762627762996, 0.7621851453495996, 0.7535487292073761, 0.7585806194455872, 0.7585241230389544, 0.7611933729727456, 0.7630341362407189, 0.7482907367116622, 0.7458669591799341, 0.7852260503756787, 0.7499343654278278, 0.7453340561183658, 0.7592629725089813, 0.7471174828272132, 0.7315338751438617, 0.7418804087559989, 0.7479300696886223, 0.7556247789289509, 0.7512098958777411, 0.7458132804956752, 0.7337112833222057, 0.7370932909823557, 0.7333150969358497, 0.7298768649574454, 0.742648035970353, 0.7404790711008562, 0.7407506037001088, 0.727937558831756, 0.7312324277620582, 0.719319289573883, 0.7257339100497976, 0.7448313849724583, 0.7183240586868073, 0.7268377800327525, 0.7311776377469226, 0.7283858983722958, 0.7455584873224943, 0.7258911438419012, 0.7307798120053367, 0.7237594829867511, 0.7196242692513017, 0.7161123978575981, 0.7120019630469742, 0.7080531428181791, 0.7092887150270641, 0.7128427782434847, 0.704279619546337, 0.6979886135678861, 0.7135333835926978, 0.7149616847663132, 0.7210297343385128, 0.7145334260760979, 0.7030229358424364, 0.7275493286038173, 0.7044613001910784, 0.7050410749499731, 0.708357015638861, 0.7183016628102795, 0.6962456170839207, 0.6904041490330344, 0.7166364852708714, 0.7056706317205163, 0.7179737954042643, 0.6931115228104531, 0.6858888003814008, 0.681402325630188, 0.7027324558669374, 0.6995263371910454, 0.7005019885589755, 0.7061936821191366, 0.6890441904996187, 0.691092311046809, 0.6856930888336124, 0.6838913882203381, 0.6804638238808581, 0.6976345313035198]
