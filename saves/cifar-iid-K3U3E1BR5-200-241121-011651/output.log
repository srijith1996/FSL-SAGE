[21/11/24 01:16:51, INFO, FSL_SAGE_main.py:<module>():408] Using GPU: True
[21/11/24 01:16:53, INFO, FSL_SAGE_main.py:main():120] Aggregation Factor: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
[21/11/24 01:16:53, INFO, FSL_SAGE_main.py:main():125] Random seed: 200
[21/11/24 01:16:53, INFO, FSL_SAGE_main.py:main():126] Alignment interval (l): 10
[21/11/24 01:16:53, INFO, FSL_SAGE_main.py:main():154] ----------------------------- WARM START USING SL -----------------------------------
[21/11/24 01:16:53, INFO, algs.py:sl_single_server():170] Configured rounds = 1
[21/11/24 01:17:08, INFO, algs.py:sl_single_server():230]  > Round 0, testing loss: 1.51, testing acc: 44.50%
[21/11/24 01:17:08, INFO, FSL_SAGE_main.py:main():160] After warm start: Test loss: 1.48, Test accuracy: 44.50
[21/11/24 01:17:08, INFO, FSL_SAGE_main.py:main():161] -------------------------------------------------------------------------------------
[21/11/24 01:17:35, INFO, FSL_SAGE_main.py:main():358]  > R  0, for the weighted aggregated final model, testing loss: 1.4785e+00, testing acc: 46.94% ( 4694/10000), training loss: 1.64, training acc: 40.43%
[21/11/24 01:18:00, INFO, FSL_SAGE_main.py:main():358]  > R  1, for the weighted aggregated final model, testing loss: 1.4432e+00, testing acc: 46.49% ( 4649/10000), training loss: 1.59, training acc: 41.30%
[21/11/24 01:18:25, INFO, FSL_SAGE_main.py:main():358]  > R  2, for the weighted aggregated final model, testing loss: 1.3281e+00, testing acc: 52.41% ( 5241/10000), training loss: 1.49, training acc: 45.61%
[21/11/24 01:18:50, INFO, FSL_SAGE_main.py:main():358]  > R  3, for the weighted aggregated final model, testing loss: 1.3025e+00, testing acc: 52.32% ( 5232/10000), training loss: 1.45, training acc: 47.21%
[21/11/24 01:19:15, INFO, FSL_SAGE_main.py:main():358]  > R  4, for the weighted aggregated final model, testing loss: 1.2913e+00, testing acc: 53.01% ( 5301/10000), training loss: 1.48, training acc: 46.55%
[21/11/24 01:19:40, INFO, FSL_SAGE_main.py:main():358]  > R  5, for the weighted aggregated final model, testing loss: 1.1902e+00, testing acc: 56.95% ( 5695/10000), training loss: 1.37, training acc: 50.43%
[21/11/24 01:20:05, INFO, FSL_SAGE_main.py:main():358]  > R  6, for the weighted aggregated final model, testing loss: 1.1686e+00, testing acc: 57.78% ( 5778/10000), training loss: 1.34, training acc: 51.67%
[21/11/24 01:20:30, INFO, FSL_SAGE_main.py:main():358]  > R  7, for the weighted aggregated final model, testing loss: 1.1352e+00, testing acc: 59.03% ( 5903/10000), training loss: 1.32, training acc: 53.16%
[21/11/24 01:20:55, INFO, FSL_SAGE_main.py:main():358]  > R  8, for the weighted aggregated final model, testing loss: 1.2081e+00, testing acc: 56.13% ( 5613/10000), training loss: 1.36, training acc: 51.01%
[21/11/24 01:21:20, INFO, FSL_SAGE_main.py:main():358]  > R  9, for the weighted aggregated final model, testing loss: 1.0955e+00, testing acc: 60.91% ( 6091/10000), training loss: 1.27, training acc: 54.85%
[21/11/24 01:21:48, INFO, FSL_SAGE_main.py:main():358]  > R 10, for the weighted aggregated final model, testing loss: 1.1016e+00, testing acc: 60.52% ( 6052/10000), training loss: 1.28, training acc: 54.54%
[21/11/24 01:22:13, INFO, FSL_SAGE_main.py:main():358]  > R 11, for the weighted aggregated final model, testing loss: 1.0803e+00, testing acc: 61.94% ( 6194/10000), training loss: 1.27, training acc: 55.37%
[21/11/24 01:22:38, INFO, FSL_SAGE_main.py:main():358]  > R 12, for the weighted aggregated final model, testing loss: 1.1132e+00, testing acc: 60.10% ( 6010/10000), training loss: 1.27, training acc: 54.75%
[21/11/24 01:23:03, INFO, FSL_SAGE_main.py:main():358]  > R 13, for the weighted aggregated final model, testing loss: 1.0449e+00, testing acc: 62.36% ( 6236/10000), training loss: 1.22, training acc: 56.54%
[21/11/24 01:23:28, INFO, FSL_SAGE_main.py:main():358]  > R 14, for the weighted aggregated final model, testing loss: 1.0580e+00, testing acc: 62.32% ( 6232/10000), training loss: 1.21, training acc: 56.66%
[21/11/24 01:23:53, INFO, FSL_SAGE_main.py:main():358]  > R 15, for the weighted aggregated final model, testing loss: 1.0420e+00, testing acc: 62.66% ( 6266/10000), training loss: 1.20, training acc: 56.53%
[21/11/24 01:24:18, INFO, FSL_SAGE_main.py:main():358]  > R 16, for the weighted aggregated final model, testing loss: 1.0240e+00, testing acc: 63.16% ( 6316/10000), training loss: 1.18, training acc: 57.92%
[21/11/24 01:24:43, INFO, FSL_SAGE_main.py:main():358]  > R 17, for the weighted aggregated final model, testing loss: 9.9190e-01, testing acc: 65.56% ( 6556/10000), training loss: 1.16, training acc: 58.67%
[21/11/24 01:25:08, INFO, FSL_SAGE_main.py:main():358]  > R 18, for the weighted aggregated final model, testing loss: 9.8358e-01, testing acc: 66.01% ( 6601/10000), training loss: 1.14, training acc: 59.49%
[21/11/24 01:25:33, INFO, FSL_SAGE_main.py:main():358]  > R 19, for the weighted aggregated final model, testing loss: 9.7057e-01, testing acc: 65.80% ( 6580/10000), training loss: 1.13, training acc: 59.88%
[21/11/24 01:26:01, INFO, FSL_SAGE_main.py:main():358]  > R 20, for the weighted aggregated final model, testing loss: 9.8447e-01, testing acc: 65.35% ( 6535/10000), training loss: 1.12, training acc: 60.18%
[21/11/24 01:26:26, INFO, FSL_SAGE_main.py:main():358]  > R 21, for the weighted aggregated final model, testing loss: 9.5916e-01, testing acc: 66.16% ( 6616/10000), training loss: 1.12, training acc: 60.38%
[21/11/24 01:26:51, INFO, FSL_SAGE_main.py:main():358]  > R 22, for the weighted aggregated final model, testing loss: 9.4434e-01, testing acc: 66.32% ( 6632/10000), training loss: 1.10, training acc: 60.91%
[21/11/24 01:27:16, INFO, FSL_SAGE_main.py:main():358]  > R 23, for the weighted aggregated final model, testing loss: 9.4531e-01, testing acc: 66.57% ( 6657/10000), training loss: 1.10, training acc: 60.88%
[21/11/24 01:27:41, INFO, FSL_SAGE_main.py:main():358]  > R 24, for the weighted aggregated final model, testing loss: 9.3614e-01, testing acc: 67.50% ( 6750/10000), training loss: 1.09, training acc: 61.47%
[21/11/24 01:28:06, INFO, FSL_SAGE_main.py:main():358]  > R 25, for the weighted aggregated final model, testing loss: 9.3368e-01, testing acc: 67.38% ( 6738/10000), training loss: 1.08, training acc: 61.45%
[21/11/24 01:28:31, INFO, FSL_SAGE_main.py:main():358]  > R 26, for the weighted aggregated final model, testing loss: 9.7540e-01, testing acc: 65.55% ( 6555/10000), training loss: 1.13, training acc: 59.76%
[21/11/24 01:28:56, INFO, FSL_SAGE_main.py:main():358]  > R 27, for the weighted aggregated final model, testing loss: 9.1560e-01, testing acc: 67.60% ( 6760/10000), training loss: 1.06, training acc: 62.13%
[21/11/24 01:29:21, INFO, FSL_SAGE_main.py:main():358]  > R 28, for the weighted aggregated final model, testing loss: 9.4023e-01, testing acc: 67.16% ( 6716/10000), training loss: 1.09, training acc: 61.12%
[21/11/24 01:29:46, INFO, FSL_SAGE_main.py:main():358]  > R 29, for the weighted aggregated final model, testing loss: 9.0341e-01, testing acc: 68.17% ( 6817/10000), training loss: 1.06, training acc: 62.59%
[21/11/24 01:30:14, INFO, FSL_SAGE_main.py:main():358]  > R 30, for the weighted aggregated final model, testing loss: 9.0314e-01, testing acc: 68.00% ( 6800/10000), training loss: 1.05, training acc: 62.69%
[21/11/24 01:30:39, INFO, FSL_SAGE_main.py:main():358]  > R 31, for the weighted aggregated final model, testing loss: 8.9373e-01, testing acc: 68.63% ( 6863/10000), training loss: 1.05, training acc: 63.11%
[21/11/24 01:31:04, INFO, FSL_SAGE_main.py:main():358]  > R 32, for the weighted aggregated final model, testing loss: 8.9894e-01, testing acc: 68.81% ( 6881/10000), training loss: 1.03, training acc: 63.26%
[21/11/24 01:31:29, INFO, FSL_SAGE_main.py:main():358]  > R 33, for the weighted aggregated final model, testing loss: 9.0131e-01, testing acc: 68.81% ( 6881/10000), training loss: 1.01, training acc: 64.34%
[21/11/24 01:31:54, INFO, FSL_SAGE_main.py:main():358]  > R 34, for the weighted aggregated final model, testing loss: 8.8687e-01, testing acc: 69.09% ( 6909/10000), training loss: 1.02, training acc: 63.69%
[21/11/24 01:32:19, INFO, FSL_SAGE_main.py:main():358]  > R 35, for the weighted aggregated final model, testing loss: 8.6713e-01, testing acc: 69.59% ( 6959/10000), training loss: 1.00, training acc: 64.75%
[21/11/24 01:32:44, INFO, FSL_SAGE_main.py:main():358]  > R 36, for the weighted aggregated final model, testing loss: 8.4748e-01, testing acc: 70.31% ( 7031/10000), training loss: 0.98, training acc: 65.63%
[21/11/24 01:33:09, INFO, FSL_SAGE_main.py:main():358]  > R 37, for the weighted aggregated final model, testing loss: 8.7914e-01, testing acc: 68.77% ( 6877/10000), training loss: 1.01, training acc: 64.06%
[21/11/24 01:33:34, INFO, FSL_SAGE_main.py:main():358]  > R 38, for the weighted aggregated final model, testing loss: 8.5580e-01, testing acc: 70.38% ( 7038/10000), training loss: 0.99, training acc: 64.82%
[21/11/24 01:33:59, INFO, FSL_SAGE_main.py:main():358]  > R 39, for the weighted aggregated final model, testing loss: 8.5686e-01, testing acc: 70.34% ( 7034/10000), training loss: 0.99, training acc: 65.09%
[21/11/24 01:34:27, INFO, FSL_SAGE_main.py:main():358]  > R 40, for the weighted aggregated final model, testing loss: 8.3986e-01, testing acc: 70.55% ( 7055/10000), training loss: 0.99, training acc: 65.28%
[21/11/24 01:34:52, INFO, FSL_SAGE_main.py:main():358]  > R 41, for the weighted aggregated final model, testing loss: 8.3416e-01, testing acc: 70.71% ( 7071/10000), training loss: 0.97, training acc: 65.33%
[21/11/24 01:35:17, INFO, FSL_SAGE_main.py:main():358]  > R 42, for the weighted aggregated final model, testing loss: 8.4577e-01, testing acc: 70.74% ( 7074/10000), training loss: 0.98, training acc: 65.38%
[21/11/24 01:35:42, INFO, FSL_SAGE_main.py:main():358]  > R 43, for the weighted aggregated final model, testing loss: 8.3711e-01, testing acc: 70.62% ( 7062/10000), training loss: 0.96, training acc: 65.96%
[21/11/24 01:36:07, INFO, FSL_SAGE_main.py:main():358]  > R 44, for the weighted aggregated final model, testing loss: 8.4480e-01, testing acc: 70.70% ( 7070/10000), training loss: 0.97, training acc: 65.72%
[21/11/24 01:36:32, INFO, FSL_SAGE_main.py:main():358]  > R 45, for the weighted aggregated final model, testing loss: 8.5349e-01, testing acc: 70.12% ( 7012/10000), training loss: 0.98, training acc: 65.14%
[21/11/24 01:36:57, INFO, FSL_SAGE_main.py:main():358]  > R 46, for the weighted aggregated final model, testing loss: 8.3072e-01, testing acc: 70.94% ( 7094/10000), training loss: 0.97, training acc: 65.77%
[21/11/24 01:37:22, INFO, FSL_SAGE_main.py:main():358]  > R 47, for the weighted aggregated final model, testing loss: 8.0602e-01, testing acc: 71.82% ( 7182/10000), training loss: 0.92, training acc: 67.72%
[21/11/24 01:37:47, INFO, FSL_SAGE_main.py:main():358]  > R 48, for the weighted aggregated final model, testing loss: 8.1418e-01, testing acc: 71.33% ( 7133/10000), training loss: 0.94, training acc: 66.68%
[21/11/24 01:38:12, INFO, FSL_SAGE_main.py:main():358]  > R 49, for the weighted aggregated final model, testing loss: 8.1719e-01, testing acc: 71.14% ( 7114/10000), training loss: 0.94, training acc: 66.65%
[21/11/24 01:38:40, INFO, FSL_SAGE_main.py:main():358]  > R 50, for the weighted aggregated final model, testing loss: 8.0955e-01, testing acc: 71.97% ( 7197/10000), training loss: 0.94, training acc: 66.36%
[21/11/24 01:39:05, INFO, FSL_SAGE_main.py:main():358]  > R 51, for the weighted aggregated final model, testing loss: 8.0674e-01, testing acc: 72.30% ( 7230/10000), training loss: 0.93, training acc: 67.31%
[21/11/24 01:39:30, INFO, FSL_SAGE_main.py:main():358]  > R 52, for the weighted aggregated final model, testing loss: 8.1720e-01, testing acc: 72.10% ( 7210/10000), training loss: 0.94, training acc: 66.89%
[21/11/24 01:39:55, INFO, FSL_SAGE_main.py:main():358]  > R 53, for the weighted aggregated final model, testing loss: 8.1981e-01, testing acc: 71.17% ( 7117/10000), training loss: 0.94, training acc: 66.55%
[21/11/24 01:40:20, INFO, FSL_SAGE_main.py:main():358]  > R 54, for the weighted aggregated final model, testing loss: 8.1327e-01, testing acc: 71.24% ( 7124/10000), training loss: 0.93, training acc: 66.69%
[21/11/24 01:40:44, INFO, FSL_SAGE_main.py:main():358]  > R 55, for the weighted aggregated final model, testing loss: 7.8251e-01, testing acc: 72.71% ( 7271/10000), training loss: 0.90, training acc: 68.08%
[21/11/24 01:41:09, INFO, FSL_SAGE_main.py:main():358]  > R 56, for the weighted aggregated final model, testing loss: 8.0052e-01, testing acc: 72.27% ( 7227/10000), training loss: 0.92, training acc: 67.94%
[21/11/24 01:41:33, INFO, FSL_SAGE_main.py:main():358]  > R 57, for the weighted aggregated final model, testing loss: 7.9431e-01, testing acc: 72.27% ( 7227/10000), training loss: 0.90, training acc: 67.93%
[21/11/24 01:41:58, INFO, FSL_SAGE_main.py:main():358]  > R 58, for the weighted aggregated final model, testing loss: 7.8847e-01, testing acc: 72.26% ( 7226/10000), training loss: 0.91, training acc: 67.98%
[21/11/24 01:42:23, INFO, FSL_SAGE_main.py:main():358]  > R 59, for the weighted aggregated final model, testing loss: 7.6411e-01, testing acc: 73.34% ( 7334/10000), training loss: 0.88, training acc: 68.68%
[21/11/24 01:42:50, INFO, FSL_SAGE_main.py:main():358]  > R 60, for the weighted aggregated final model, testing loss: 7.7317e-01, testing acc: 73.09% ( 7309/10000), training loss: 0.89, training acc: 68.78%
[21/11/24 01:43:15, INFO, FSL_SAGE_main.py:main():358]  > R 61, for the weighted aggregated final model, testing loss: 7.8829e-01, testing acc: 72.52% ( 7252/10000), training loss: 0.89, training acc: 68.71%
[21/11/24 01:43:40, INFO, FSL_SAGE_main.py:main():358]  > R 62, for the weighted aggregated final model, testing loss: 7.9461e-01, testing acc: 72.90% ( 7290/10000), training loss: 0.91, training acc: 68.03%
[21/11/24 01:44:04, INFO, FSL_SAGE_main.py:main():358]  > R 63, for the weighted aggregated final model, testing loss: 7.6516e-01, testing acc: 73.43% ( 7343/10000), training loss: 0.89, training acc: 69.04%
[21/11/24 01:44:29, INFO, FSL_SAGE_main.py:main():358]  > R 64, for the weighted aggregated final model, testing loss: 7.7814e-01, testing acc: 72.42% ( 7242/10000), training loss: 0.89, training acc: 68.60%
[21/11/24 01:44:54, INFO, FSL_SAGE_main.py:main():358]  > R 65, for the weighted aggregated final model, testing loss: 7.5894e-01, testing acc: 73.79% ( 7379/10000), training loss: 0.87, training acc: 69.21%
[21/11/24 01:45:19, INFO, FSL_SAGE_main.py:main():358]  > R 66, for the weighted aggregated final model, testing loss: 7.8887e-01, testing acc: 72.60% ( 7260/10000), training loss: 0.91, training acc: 68.06%
[21/11/24 01:45:44, INFO, FSL_SAGE_main.py:main():358]  > R 67, for the weighted aggregated final model, testing loss: 7.7682e-01, testing acc: 72.96% ( 7296/10000), training loss: 0.88, training acc: 68.80%
[21/11/24 01:46:08, INFO, FSL_SAGE_main.py:main():358]  > R 68, for the weighted aggregated final model, testing loss: 7.4308e-01, testing acc: 74.04% ( 7404/10000), training loss: 0.86, training acc: 69.93%
[21/11/24 01:46:33, INFO, FSL_SAGE_main.py:main():358]  > R 69, for the weighted aggregated final model, testing loss: 7.5374e-01, testing acc: 73.79% ( 7379/10000), training loss: 0.87, training acc: 69.22%
[21/11/24 01:47:01, INFO, FSL_SAGE_main.py:main():358]  > R 70, for the weighted aggregated final model, testing loss: 7.5011e-01, testing acc: 74.50% ( 7450/10000), training loss: 0.87, training acc: 69.63%
[21/11/24 01:47:25, INFO, FSL_SAGE_main.py:main():358]  > R 71, for the weighted aggregated final model, testing loss: 7.5768e-01, testing acc: 73.57% ( 7357/10000), training loss: 0.86, training acc: 69.96%
[21/11/24 01:47:50, INFO, FSL_SAGE_main.py:main():358]  > R 72, for the weighted aggregated final model, testing loss: 7.5963e-01, testing acc: 73.74% ( 7374/10000), training loss: 0.87, training acc: 69.55%
[21/11/24 01:48:15, INFO, FSL_SAGE_main.py:main():358]  > R 73, for the weighted aggregated final model, testing loss: 7.5640e-01, testing acc: 73.85% ( 7385/10000), training loss: 0.85, training acc: 69.85%
[21/11/24 01:48:40, INFO, FSL_SAGE_main.py:main():358]  > R 74, for the weighted aggregated final model, testing loss: 7.4973e-01, testing acc: 74.55% ( 7455/10000), training loss: 0.84, training acc: 70.51%
[21/11/24 01:49:05, INFO, FSL_SAGE_main.py:main():358]  > R 75, for the weighted aggregated final model, testing loss: 7.4264e-01, testing acc: 73.68% ( 7368/10000), training loss: 0.84, training acc: 70.34%
[21/11/24 01:49:29, INFO, FSL_SAGE_main.py:main():358]  > R 76, for the weighted aggregated final model, testing loss: 7.4149e-01, testing acc: 74.22% ( 7422/10000), training loss: 0.85, training acc: 69.78%
[21/11/24 01:49:54, INFO, FSL_SAGE_main.py:main():358]  > R 77, for the weighted aggregated final model, testing loss: 7.4123e-01, testing acc: 74.57% ( 7457/10000), training loss: 0.84, training acc: 70.26%
[21/11/24 01:50:19, INFO, FSL_SAGE_main.py:main():358]  > R 78, for the weighted aggregated final model, testing loss: 7.3432e-01, testing acc: 74.32% ( 7432/10000), training loss: 0.84, training acc: 70.55%
[21/11/24 01:50:44, INFO, FSL_SAGE_main.py:main():358]  > R 79, for the weighted aggregated final model, testing loss: 7.4587e-01, testing acc: 74.08% ( 7408/10000), training loss: 0.86, training acc: 69.82%
[21/11/24 01:51:11, INFO, FSL_SAGE_main.py:main():358]  > R 80, for the weighted aggregated final model, testing loss: 7.3668e-01, testing acc: 74.45% ( 7445/10000), training loss: 0.85, training acc: 70.06%
[21/11/24 01:51:36, INFO, FSL_SAGE_main.py:main():358]  > R 81, for the weighted aggregated final model, testing loss: 7.2966e-01, testing acc: 74.72% ( 7472/10000), training loss: 0.84, training acc: 70.51%
[21/11/24 01:52:01, INFO, FSL_SAGE_main.py:main():358]  > R 82, for the weighted aggregated final model, testing loss: 7.2216e-01, testing acc: 74.93% ( 7493/10000), training loss: 0.83, training acc: 70.94%
[21/11/24 01:52:26, INFO, FSL_SAGE_main.py:main():358]  > R 83, for the weighted aggregated final model, testing loss: 7.2199e-01, testing acc: 75.20% ( 7520/10000), training loss: 0.83, training acc: 71.20%
[21/11/24 01:52:51, INFO, FSL_SAGE_main.py:main():358]  > R 84, for the weighted aggregated final model, testing loss: 7.3321e-01, testing acc: 74.73% ( 7473/10000), training loss: 0.85, training acc: 70.10%
[21/11/24 01:53:15, INFO, FSL_SAGE_main.py:main():358]  > R 85, for the weighted aggregated final model, testing loss: 7.1972e-01, testing acc: 74.84% ( 7484/10000), training loss: 0.82, training acc: 71.17%
[21/11/24 01:53:40, INFO, FSL_SAGE_main.py:main():358]  > R 86, for the weighted aggregated final model, testing loss: 7.3032e-01, testing acc: 74.82% ( 7482/10000), training loss: 0.83, training acc: 70.83%
[21/11/24 01:54:05, INFO, FSL_SAGE_main.py:main():358]  > R 87, for the weighted aggregated final model, testing loss: 7.2427e-01, testing acc: 74.45% ( 7445/10000), training loss: 0.84, training acc: 70.18%
[21/11/24 01:54:30, INFO, FSL_SAGE_main.py:main():358]  > R 88, for the weighted aggregated final model, testing loss: 7.3023e-01, testing acc: 74.45% ( 7445/10000), training loss: 0.84, training acc: 70.15%
[21/11/24 01:54:54, INFO, FSL_SAGE_main.py:main():358]  > R 89, for the weighted aggregated final model, testing loss: 7.1471e-01, testing acc: 75.13% ( 7513/10000), training loss: 0.82, training acc: 71.18%
[21/11/24 01:55:22, INFO, FSL_SAGE_main.py:main():358]  > R 90, for the weighted aggregated final model, testing loss: 7.1531e-01, testing acc: 74.90% ( 7490/10000), training loss: 0.81, training acc: 71.34%
[21/11/24 01:55:47, INFO, FSL_SAGE_main.py:main():358]  > R 91, for the weighted aggregated final model, testing loss: 7.0912e-01, testing acc: 75.32% ( 7532/10000), training loss: 0.82, training acc: 71.24%
[21/11/24 01:56:12, INFO, FSL_SAGE_main.py:main():358]  > R 92, for the weighted aggregated final model, testing loss: 7.4325e-01, testing acc: 73.91% ( 7391/10000), training loss: 0.84, training acc: 70.13%
[21/11/24 01:56:36, INFO, FSL_SAGE_main.py:main():358]  > R 93, for the weighted aggregated final model, testing loss: 7.2808e-01, testing acc: 74.63% ( 7463/10000), training loss: 0.82, training acc: 70.93%
[21/11/24 01:57:01, INFO, FSL_SAGE_main.py:main():358]  > R 94, for the weighted aggregated final model, testing loss: 7.2213e-01, testing acc: 75.03% ( 7503/10000), training loss: 0.82, training acc: 71.11%
[21/11/24 01:57:26, INFO, FSL_SAGE_main.py:main():358]  > R 95, for the weighted aggregated final model, testing loss: 7.1576e-01, testing acc: 75.07% ( 7507/10000), training loss: 0.82, training acc: 70.90%
[21/11/24 01:57:51, INFO, FSL_SAGE_main.py:main():358]  > R 96, for the weighted aggregated final model, testing loss: 7.1097e-01, testing acc: 75.57% ( 7557/10000), training loss: 0.80, training acc: 71.88%
[21/11/24 01:58:16, INFO, FSL_SAGE_main.py:main():358]  > R 97, for the weighted aggregated final model, testing loss: 7.1389e-01, testing acc: 75.36% ( 7536/10000), training loss: 0.83, training acc: 70.79%
[21/11/24 01:58:40, INFO, FSL_SAGE_main.py:main():358]  > R 98, for the weighted aggregated final model, testing loss: 7.2624e-01, testing acc: 74.92% ( 7492/10000), training loss: 0.81, training acc: 71.21%
[21/11/24 01:59:05, INFO, FSL_SAGE_main.py:main():358]  > R 99, for the weighted aggregated final model, testing loss: 7.2116e-01, testing acc: 75.14% ( 7514/10000), training loss: 0.82, training acc: 71.15%
[21/11/24 01:59:33, INFO, FSL_SAGE_main.py:main():358]  > R 100, for the weighted aggregated final model, testing loss: 7.2235e-01, testing acc: 75.28% ( 7528/10000), training loss: 0.81, training acc: 71.22%
[21/11/24 01:59:57, INFO, FSL_SAGE_main.py:main():358]  > R 101, for the weighted aggregated final model, testing loss: 7.1090e-01, testing acc: 75.65% ( 7565/10000), training loss: 0.81, training acc: 71.81%
[21/11/24 02:00:22, INFO, FSL_SAGE_main.py:main():358]  > R 102, for the weighted aggregated final model, testing loss: 7.0449e-01, testing acc: 76.13% ( 7613/10000), training loss: 0.80, training acc: 72.21%
[21/11/24 02:00:47, INFO, FSL_SAGE_main.py:main():358]  > R 103, for the weighted aggregated final model, testing loss: 6.9365e-01, testing acc: 75.82% ( 7582/10000), training loss: 0.79, training acc: 72.24%
[21/11/24 02:01:12, INFO, FSL_SAGE_main.py:main():358]  > R 104, for the weighted aggregated final model, testing loss: 7.1651e-01, testing acc: 75.30% ( 7530/10000), training loss: 0.81, training acc: 71.40%
[21/11/24 02:01:37, INFO, FSL_SAGE_main.py:main():358]  > R 105, for the weighted aggregated final model, testing loss: 7.1150e-01, testing acc: 75.50% ( 7550/10000), training loss: 0.80, training acc: 71.81%
[21/11/24 02:02:01, INFO, FSL_SAGE_main.py:main():358]  > R 106, for the weighted aggregated final model, testing loss: 6.9438e-01, testing acc: 76.18% ( 7618/10000), training loss: 0.79, training acc: 72.06%
[21/11/24 02:02:26, INFO, FSL_SAGE_main.py:main():358]  > R 107, for the weighted aggregated final model, testing loss: 7.0179e-01, testing acc: 75.71% ( 7571/10000), training loss: 0.81, training acc: 71.60%
[21/11/24 02:02:51, INFO, FSL_SAGE_main.py:main():358]  > R 108, for the weighted aggregated final model, testing loss: 6.9476e-01, testing acc: 75.90% ( 7590/10000), training loss: 0.79, training acc: 71.85%
[21/11/24 02:03:16, INFO, FSL_SAGE_main.py:main():358]  > R 109, for the weighted aggregated final model, testing loss: 7.1200e-01, testing acc: 75.54% ( 7554/10000), training loss: 0.80, training acc: 71.83%
[21/11/24 02:03:43, INFO, FSL_SAGE_main.py:main():358]  > R 110, for the weighted aggregated final model, testing loss: 7.0883e-01, testing acc: 75.33% ( 7533/10000), training loss: 0.80, training acc: 72.00%
[21/11/24 02:04:08, INFO, FSL_SAGE_main.py:main():358]  > R 111, for the weighted aggregated final model, testing loss: 7.0261e-01, testing acc: 75.53% ( 7553/10000), training loss: 0.80, training acc: 71.70%
[21/11/24 02:04:33, INFO, FSL_SAGE_main.py:main():358]  > R 112, for the weighted aggregated final model, testing loss: 7.0289e-01, testing acc: 75.17% ( 7517/10000), training loss: 0.80, training acc: 71.90%
[21/11/24 02:04:57, INFO, FSL_SAGE_main.py:main():358]  > R 113, for the weighted aggregated final model, testing loss: 7.0237e-01, testing acc: 75.60% ( 7560/10000), training loss: 0.79, training acc: 72.12%
[21/11/24 02:05:22, INFO, FSL_SAGE_main.py:main():358]  > R 114, for the weighted aggregated final model, testing loss: 7.1617e-01, testing acc: 74.96% ( 7496/10000), training loss: 0.80, training acc: 72.11%
[21/11/24 02:05:47, INFO, FSL_SAGE_main.py:main():358]  > R 115, for the weighted aggregated final model, testing loss: 6.9719e-01, testing acc: 75.69% ( 7569/10000), training loss: 0.79, training acc: 72.34%
[21/11/24 02:06:11, INFO, FSL_SAGE_main.py:main():358]  > R 116, for the weighted aggregated final model, testing loss: 7.1897e-01, testing acc: 74.56% ( 7456/10000), training loss: 0.80, training acc: 71.58%
[21/11/24 02:06:36, INFO, FSL_SAGE_main.py:main():358]  > R 117, for the weighted aggregated final model, testing loss: 6.8027e-01, testing acc: 76.33% ( 7633/10000), training loss: 0.77, training acc: 72.87%
[21/11/24 02:07:01, INFO, FSL_SAGE_main.py:main():358]  > R 118, for the weighted aggregated final model, testing loss: 6.8276e-01, testing acc: 76.36% ( 7636/10000), training loss: 0.77, training acc: 73.06%
[21/11/24 02:07:26, INFO, FSL_SAGE_main.py:main():358]  > R 119, for the weighted aggregated final model, testing loss: 6.8534e-01, testing acc: 76.27% ( 7627/10000), training loss: 0.78, training acc: 72.71%
[21/11/24 02:07:53, INFO, FSL_SAGE_main.py:main():358]  > R 120, for the weighted aggregated final model, testing loss: 6.7682e-01, testing acc: 76.40% ( 7640/10000), training loss: 0.76, training acc: 73.06%
[21/11/24 02:08:18, INFO, FSL_SAGE_main.py:main():358]  > R 121, for the weighted aggregated final model, testing loss: 6.7208e-01, testing acc: 76.59% ( 7659/10000), training loss: 0.76, training acc: 73.43%
[21/11/24 02:08:43, INFO, FSL_SAGE_main.py:main():358]  > R 122, for the weighted aggregated final model, testing loss: 6.7344e-01, testing acc: 76.37% ( 7637/10000), training loss: 0.76, training acc: 73.40%
[21/11/24 02:09:07, INFO, FSL_SAGE_main.py:main():358]  > R 123, for the weighted aggregated final model, testing loss: 6.9428e-01, testing acc: 75.65% ( 7565/10000), training loss: 0.78, training acc: 72.43%
[21/11/24 02:09:32, INFO, FSL_SAGE_main.py:main():358]  > R 124, for the weighted aggregated final model, testing loss: 6.8246e-01, testing acc: 76.14% ( 7614/10000), training loss: 0.76, training acc: 73.28%
[21/11/24 02:09:57, INFO, FSL_SAGE_main.py:main():358]  > R 125, for the weighted aggregated final model, testing loss: 6.8480e-01, testing acc: 76.42% ( 7642/10000), training loss: 0.77, training acc: 72.70%
[21/11/24 02:10:22, INFO, FSL_SAGE_main.py:main():358]  > R 126, for the weighted aggregated final model, testing loss: 6.8215e-01, testing acc: 76.48% ( 7648/10000), training loss: 0.77, training acc: 73.15%
[21/11/24 02:10:46, INFO, FSL_SAGE_main.py:main():358]  > R 127, for the weighted aggregated final model, testing loss: 6.8781e-01, testing acc: 76.56% ( 7656/10000), training loss: 0.76, training acc: 73.07%
[21/11/24 02:11:11, INFO, FSL_SAGE_main.py:main():358]  > R 128, for the weighted aggregated final model, testing loss: 6.8009e-01, testing acc: 76.20% ( 7620/10000), training loss: 0.76, training acc: 73.68%
[21/11/24 02:11:36, INFO, FSL_SAGE_main.py:main():358]  > R 129, for the weighted aggregated final model, testing loss: 6.8787e-01, testing acc: 75.89% ( 7589/10000), training loss: 0.75, training acc: 73.40%
[21/11/24 02:12:03, INFO, FSL_SAGE_main.py:main():358]  > R 130, for the weighted aggregated final model, testing loss: 6.9166e-01, testing acc: 76.02% ( 7602/10000), training loss: 0.77, training acc: 73.23%
[21/11/24 02:12:28, INFO, FSL_SAGE_main.py:main():358]  > R 131, for the weighted aggregated final model, testing loss: 6.7468e-01, testing acc: 76.57% ( 7657/10000), training loss: 0.76, training acc: 73.24%
[21/11/24 02:12:53, INFO, FSL_SAGE_main.py:main():358]  > R 132, for the weighted aggregated final model, testing loss: 6.7489e-01, testing acc: 76.15% ( 7615/10000), training loss: 0.76, training acc: 73.37%
[21/11/24 02:13:18, INFO, FSL_SAGE_main.py:main():358]  > R 133, for the weighted aggregated final model, testing loss: 6.7445e-01, testing acc: 76.76% ( 7676/10000), training loss: 0.76, training acc: 73.55%
[21/11/24 02:13:42, INFO, FSL_SAGE_main.py:main():358]  > R 134, for the weighted aggregated final model, testing loss: 6.8184e-01, testing acc: 76.23% ( 7623/10000), training loss: 0.75, training acc: 73.54%
[21/11/24 02:14:07, INFO, FSL_SAGE_main.py:main():358]  > R 135, for the weighted aggregated final model, testing loss: 7.0077e-01, testing acc: 76.17% ( 7617/10000), training loss: 0.78, training acc: 72.68%
[21/11/24 02:14:32, INFO, FSL_SAGE_main.py:main():358]  > R 136, for the weighted aggregated final model, testing loss: 6.8258e-01, testing acc: 76.27% ( 7627/10000), training loss: 0.75, training acc: 73.42%
[21/11/24 02:14:57, INFO, FSL_SAGE_main.py:main():358]  > R 137, for the weighted aggregated final model, testing loss: 6.7924e-01, testing acc: 76.63% ( 7663/10000), training loss: 0.75, training acc: 73.69%
[21/11/24 02:15:21, INFO, FSL_SAGE_main.py:main():358]  > R 138, for the weighted aggregated final model, testing loss: 6.8555e-01, testing acc: 76.50% ( 7650/10000), training loss: 0.76, training acc: 73.09%
[21/11/24 02:15:46, INFO, FSL_SAGE_main.py:main():358]  > R 139, for the weighted aggregated final model, testing loss: 6.7022e-01, testing acc: 76.62% ( 7662/10000), training loss: 0.75, training acc: 73.63%
[21/11/24 02:16:14, INFO, FSL_SAGE_main.py:main():358]  > R 140, for the weighted aggregated final model, testing loss: 6.7989e-01, testing acc: 76.41% ( 7641/10000), training loss: 0.76, training acc: 73.32%
[21/11/24 02:16:38, INFO, FSL_SAGE_main.py:main():358]  > R 141, for the weighted aggregated final model, testing loss: 6.7757e-01, testing acc: 76.47% ( 7647/10000), training loss: 0.74, training acc: 74.08%
[21/11/24 02:17:03, INFO, FSL_SAGE_main.py:main():358]  > R 142, for the weighted aggregated final model, testing loss: 6.7469e-01, testing acc: 77.04% ( 7704/10000), training loss: 0.75, training acc: 73.75%
[21/11/24 02:17:28, INFO, FSL_SAGE_main.py:main():358]  > R 143, for the weighted aggregated final model, testing loss: 6.7710e-01, testing acc: 76.79% ( 7679/10000), training loss: 0.74, training acc: 74.10%
[21/11/24 02:17:53, INFO, FSL_SAGE_main.py:main():358]  > R 144, for the weighted aggregated final model, testing loss: 6.8100e-01, testing acc: 76.50% ( 7650/10000), training loss: 0.76, training acc: 73.52%
[21/11/24 02:18:18, INFO, FSL_SAGE_main.py:main():358]  > R 145, for the weighted aggregated final model, testing loss: 6.7449e-01, testing acc: 76.48% ( 7648/10000), training loss: 0.75, training acc: 73.80%
[21/11/24 02:18:42, INFO, FSL_SAGE_main.py:main():358]  > R 146, for the weighted aggregated final model, testing loss: 6.8423e-01, testing acc: 76.30% ( 7630/10000), training loss: 0.76, training acc: 73.08%
[21/11/24 02:19:07, INFO, FSL_SAGE_main.py:main():358]  > R 147, for the weighted aggregated final model, testing loss: 6.7299e-01, testing acc: 77.07% ( 7707/10000), training loss: 0.75, training acc: 73.67%
[21/11/24 02:19:32, INFO, FSL_SAGE_main.py:main():358]  > R 148, for the weighted aggregated final model, testing loss: 6.7764e-01, testing acc: 76.58% ( 7658/10000), training loss: 0.75, training acc: 73.71%
[21/11/24 02:19:57, INFO, FSL_SAGE_main.py:main():358]  > R 149, for the weighted aggregated final model, testing loss: 6.6782e-01, testing acc: 77.43% ( 7743/10000), training loss: 0.74, training acc: 74.14%
[21/11/24 02:20:24, INFO, FSL_SAGE_main.py:main():358]  > R 150, for the weighted aggregated final model, testing loss: 6.7506e-01, testing acc: 76.28% ( 7628/10000), training loss: 0.74, training acc: 74.02%
[21/11/24 02:20:49, INFO, FSL_SAGE_main.py:main():358]  > R 151, for the weighted aggregated final model, testing loss: 6.7065e-01, testing acc: 76.39% ( 7639/10000), training loss: 0.74, training acc: 73.71%
[21/11/24 02:21:14, INFO, FSL_SAGE_main.py:main():358]  > R 152, for the weighted aggregated final model, testing loss: 6.8589e-01, testing acc: 75.94% ( 7594/10000), training loss: 0.77, training acc: 73.05%
[21/11/24 02:21:39, INFO, FSL_SAGE_main.py:main():358]  > R 153, for the weighted aggregated final model, testing loss: 6.7011e-01, testing acc: 76.81% ( 7681/10000), training loss: 0.75, training acc: 73.77%
[21/11/24 02:22:03, INFO, FSL_SAGE_main.py:main():358]  > R 154, for the weighted aggregated final model, testing loss: 6.6866e-01, testing acc: 76.90% ( 7690/10000), training loss: 0.73, training acc: 73.97%
[21/11/24 02:22:28, INFO, FSL_SAGE_main.py:main():358]  > R 155, for the weighted aggregated final model, testing loss: 6.5721e-01, testing acc: 77.36% ( 7736/10000), training loss: 0.73, training acc: 74.76%
[21/11/24 02:22:53, INFO, FSL_SAGE_main.py:main():358]  > R 156, for the weighted aggregated final model, testing loss: 6.7425e-01, testing acc: 76.57% ( 7657/10000), training loss: 0.74, training acc: 73.79%
[21/11/24 02:23:18, INFO, FSL_SAGE_main.py:main():358]  > R 157, for the weighted aggregated final model, testing loss: 6.6921e-01, testing acc: 76.80% ( 7680/10000), training loss: 0.73, training acc: 74.36%
[21/11/24 02:23:43, INFO, FSL_SAGE_main.py:main():358]  > R 158, for the weighted aggregated final model, testing loss: 6.7595e-01, testing acc: 76.87% ( 7687/10000), training loss: 0.74, training acc: 73.97%
[21/11/24 02:24:08, INFO, FSL_SAGE_main.py:main():358]  > R 159, for the weighted aggregated final model, testing loss: 6.7565e-01, testing acc: 76.76% ( 7676/10000), training loss: 0.73, training acc: 74.31%
[21/11/24 02:24:35, INFO, FSL_SAGE_main.py:main():358]  > R 160, for the weighted aggregated final model, testing loss: 6.7810e-01, testing acc: 76.19% ( 7619/10000), training loss: 0.75, training acc: 73.63%
[21/11/24 02:25:00, INFO, FSL_SAGE_main.py:main():358]  > R 161, for the weighted aggregated final model, testing loss: 6.6798e-01, testing acc: 76.52% ( 7652/10000), training loss: 0.74, training acc: 74.10%
[21/11/24 02:25:25, INFO, FSL_SAGE_main.py:main():358]  > R 162, for the weighted aggregated final model, testing loss: 6.8963e-01, testing acc: 76.14% ( 7614/10000), training loss: 0.75, training acc: 73.70%
[21/11/24 02:25:50, INFO, FSL_SAGE_main.py:main():358]  > R 163, for the weighted aggregated final model, testing loss: 6.6611e-01, testing acc: 77.18% ( 7718/10000), training loss: 0.73, training acc: 74.44%
[21/11/24 02:26:15, INFO, FSL_SAGE_main.py:main():358]  > R 164, for the weighted aggregated final model, testing loss: 6.6556e-01, testing acc: 76.64% ( 7664/10000), training loss: 0.74, training acc: 74.18%
[21/11/24 02:26:39, INFO, FSL_SAGE_main.py:main():358]  > R 165, for the weighted aggregated final model, testing loss: 6.6867e-01, testing acc: 76.76% ( 7676/10000), training loss: 0.73, training acc: 74.44%
[21/11/24 02:27:04, INFO, FSL_SAGE_main.py:main():358]  > R 166, for the weighted aggregated final model, testing loss: 6.5763e-01, testing acc: 77.35% ( 7735/10000), training loss: 0.72, training acc: 74.47%
[21/11/24 02:27:29, INFO, FSL_SAGE_main.py:main():358]  > R 167, for the weighted aggregated final model, testing loss: 6.6633e-01, testing acc: 77.04% ( 7704/10000), training loss: 0.72, training acc: 74.65%
[21/11/24 02:27:54, INFO, FSL_SAGE_main.py:main():358]  > R 168, for the weighted aggregated final model, testing loss: 6.7439e-01, testing acc: 76.90% ( 7690/10000), training loss: 0.75, training acc: 73.78%
[21/11/24 02:28:19, INFO, FSL_SAGE_main.py:main():358]  > R 169, for the weighted aggregated final model, testing loss: 6.7333e-01, testing acc: 76.67% ( 7667/10000), training loss: 0.73, training acc: 74.39%
[21/11/24 02:28:46, INFO, FSL_SAGE_main.py:main():358]  > R 170, for the weighted aggregated final model, testing loss: 6.7440e-01, testing acc: 76.72% ( 7672/10000), training loss: 0.74, training acc: 74.21%
[21/11/24 02:29:11, INFO, FSL_SAGE_main.py:main():358]  > R 171, for the weighted aggregated final model, testing loss: 6.7028e-01, testing acc: 76.97% ( 7697/10000), training loss: 0.72, training acc: 74.58%
[21/11/24 02:29:36, INFO, FSL_SAGE_main.py:main():358]  > R 172, for the weighted aggregated final model, testing loss: 6.5720e-01, testing acc: 77.06% ( 7706/10000), training loss: 0.72, training acc: 74.79%
[21/11/24 02:30:01, INFO, FSL_SAGE_main.py:main():358]  > R 173, for the weighted aggregated final model, testing loss: 6.5570e-01, testing acc: 77.49% ( 7749/10000), training loss: 0.72, training acc: 74.29%
[21/11/24 02:30:26, INFO, FSL_SAGE_main.py:main():358]  > R 174, for the weighted aggregated final model, testing loss: 6.6355e-01, testing acc: 77.42% ( 7742/10000), training loss: 0.72, training acc: 74.66%
[21/11/24 02:30:50, INFO, FSL_SAGE_main.py:main():358]  > R 175, for the weighted aggregated final model, testing loss: 6.5825e-01, testing acc: 77.12% ( 7712/10000), training loss: 0.72, training acc: 74.76%
[21/11/24 02:31:15, INFO, FSL_SAGE_main.py:main():358]  > R 176, for the weighted aggregated final model, testing loss: 6.6387e-01, testing acc: 77.12% ( 7712/10000), training loss: 0.72, training acc: 74.75%
[21/11/24 02:31:40, INFO, FSL_SAGE_main.py:main():358]  > R 177, for the weighted aggregated final model, testing loss: 6.6608e-01, testing acc: 76.78% ( 7678/10000), training loss: 0.74, training acc: 74.04%
[21/11/24 02:32:05, INFO, FSL_SAGE_main.py:main():358]  > R 178, for the weighted aggregated final model, testing loss: 6.5619e-01, testing acc: 77.22% ( 7722/10000), training loss: 0.71, training acc: 75.03%
[21/11/24 02:32:29, INFO, FSL_SAGE_main.py:main():358]  > R 179, for the weighted aggregated final model, testing loss: 6.6776e-01, testing acc: 76.88% ( 7688/10000), training loss: 0.72, training acc: 74.66%
[21/11/24 02:32:57, INFO, FSL_SAGE_main.py:main():358]  > R 180, for the weighted aggregated final model, testing loss: 6.6310e-01, testing acc: 77.42% ( 7742/10000), training loss: 0.72, training acc: 74.94%
[21/11/24 02:33:22, INFO, FSL_SAGE_main.py:main():358]  > R 181, for the weighted aggregated final model, testing loss: 6.7108e-01, testing acc: 76.67% ( 7667/10000), training loss: 0.73, training acc: 74.40%
[21/11/24 02:33:47, INFO, FSL_SAGE_main.py:main():358]  > R 182, for the weighted aggregated final model, testing loss: 6.8479e-01, testing acc: 76.82% ( 7682/10000), training loss: 0.74, training acc: 73.95%
[21/11/24 02:34:12, INFO, FSL_SAGE_main.py:main():358]  > R 183, for the weighted aggregated final model, testing loss: 6.4390e-01, testing acc: 78.04% ( 7804/10000), training loss: 0.71, training acc: 75.18%
[21/11/24 02:34:36, INFO, FSL_SAGE_main.py:main():358]  > R 184, for the weighted aggregated final model, testing loss: 6.5394e-01, testing acc: 77.52% ( 7752/10000), training loss: 0.71, training acc: 74.80%
[21/11/24 02:35:01, INFO, FSL_SAGE_main.py:main():358]  > R 185, for the weighted aggregated final model, testing loss: 6.4420e-01, testing acc: 77.80% ( 7780/10000), training loss: 0.70, training acc: 75.15%
[21/11/24 02:35:26, INFO, FSL_SAGE_main.py:main():358]  > R 186, for the weighted aggregated final model, testing loss: 6.4578e-01, testing acc: 77.81% ( 7781/10000), training loss: 0.72, training acc: 74.61%
[21/11/24 02:35:50, INFO, FSL_SAGE_main.py:main():358]  > R 187, for the weighted aggregated final model, testing loss: 6.5525e-01, testing acc: 77.56% ( 7756/10000), training loss: 0.72, training acc: 74.75%
[21/11/24 02:36:15, INFO, FSL_SAGE_main.py:main():358]  > R 188, for the weighted aggregated final model, testing loss: 6.5771e-01, testing acc: 77.27% ( 7727/10000), training loss: 0.71, training acc: 75.19%
[21/11/24 02:36:40, INFO, FSL_SAGE_main.py:main():358]  > R 189, for the weighted aggregated final model, testing loss: 6.5370e-01, testing acc: 77.35% ( 7735/10000), training loss: 0.72, training acc: 74.92%
[21/11/24 02:37:08, INFO, FSL_SAGE_main.py:main():358]  > R 190, for the weighted aggregated final model, testing loss: 6.8627e-01, testing acc: 76.15% ( 7615/10000), training loss: 0.75, training acc: 73.97%
[21/11/24 02:37:33, INFO, FSL_SAGE_main.py:main():358]  > R 191, for the weighted aggregated final model, testing loss: 6.5002e-01, testing acc: 77.41% ( 7741/10000), training loss: 0.71, training acc: 75.09%
[21/11/24 02:37:57, INFO, FSL_SAGE_main.py:main():358]  > R 192, for the weighted aggregated final model, testing loss: 6.6267e-01, testing acc: 76.88% ( 7688/10000), training loss: 0.72, training acc: 74.60%
[21/11/24 02:38:22, INFO, FSL_SAGE_main.py:main():358]  > R 193, for the weighted aggregated final model, testing loss: 6.5267e-01, testing acc: 77.56% ( 7756/10000), training loss: 0.71, training acc: 75.04%
[21/11/24 02:38:47, INFO, FSL_SAGE_main.py:main():358]  > R 194, for the weighted aggregated final model, testing loss: 6.5118e-01, testing acc: 77.30% ( 7730/10000), training loss: 0.72, training acc: 74.71%
[21/11/24 02:39:12, INFO, FSL_SAGE_main.py:main():358]  > R 195, for the weighted aggregated final model, testing loss: 6.4367e-01, testing acc: 77.74% ( 7774/10000), training loss: 0.70, training acc: 75.51%
[21/11/24 02:39:37, INFO, FSL_SAGE_main.py:main():358]  > R 196, for the weighted aggregated final model, testing loss: 6.4880e-01, testing acc: 77.70% ( 7770/10000), training loss: 0.71, training acc: 75.25%
[21/11/24 02:40:01, INFO, FSL_SAGE_main.py:main():358]  > R 197, for the weighted aggregated final model, testing loss: 6.4624e-01, testing acc: 77.56% ( 7756/10000), training loss: 0.69, training acc: 75.59%
[21/11/24 02:40:26, INFO, FSL_SAGE_main.py:main():358]  > R 198, for the weighted aggregated final model, testing loss: 6.6845e-01, testing acc: 76.83% ( 7683/10000), training loss: 0.71, training acc: 74.96%
[21/11/24 02:40:51, INFO, FSL_SAGE_main.py:main():358]  > R 199, for the weighted aggregated final model, testing loss: 6.5472e-01, testing acc: 77.11% ( 7711/10000), training loss: 0.70, training acc: 75.34%
[21/11/24 02:40:51, INFO, FSL_SAGE_main.py:main():360] The total running time for all rounds is 5037.7 seconds
[21/11/24 02:40:51, INFO, FSL_SAGE_main.py:main():370] [NOTICE] Saved results to '../saves/cifar-iid-K3U3E1BR5-200-241121-011651/results.json'.
[21/11/24 02:40:52, INFO, FSL_SAGE_main.py:main():386] Testing accuracy: [0.4694, 0.4649, 0.5241, 0.5232, 0.5301, 0.5695, 0.5778, 0.5903, 0.5613, 0.6091, 0.6052, 0.6194, 0.601, 0.6236, 0.6232, 0.6266, 0.6316, 0.6556, 0.6601, 0.658, 0.6535, 0.6616, 0.6632, 0.6657, 0.675, 0.6738, 0.6555, 0.676, 0.6716, 0.6817, 0.68, 0.6863, 0.6881, 0.6881, 0.6909, 0.6959, 0.7031, 0.6877, 0.7038, 0.7034, 0.7055, 0.7071, 0.7074, 0.7062, 0.707, 0.7012, 0.7094, 0.7182, 0.7133, 0.7114, 0.7197, 0.723, 0.721, 0.7117, 0.7124, 0.7271, 0.7227, 0.7227, 0.7226, 0.7334, 0.7309, 0.7252, 0.729, 0.7343, 0.7242, 0.7379, 0.726, 0.7296, 0.7404, 0.7379, 0.745, 0.7357, 0.7374, 0.7385, 0.7455, 0.7368, 0.7422, 0.7457, 0.7432, 0.7408, 0.7445, 0.7472, 0.7493, 0.752, 0.7473, 0.7484, 0.7482, 0.7445, 0.7445, 0.7513, 0.749, 0.7532, 0.7391, 0.7463, 0.7503, 0.7507, 0.7557, 0.7536, 0.7492, 0.7514, 0.7528, 0.7565, 0.7613, 0.7582, 0.753, 0.755, 0.7618, 0.7571, 0.759, 0.7554, 0.7533, 0.7553, 0.7517, 0.756, 0.7496, 0.7569, 0.7456, 0.7633, 0.7636, 0.7627, 0.764, 0.7659, 0.7637, 0.7565, 0.7614, 0.7642, 0.7648, 0.7656, 0.762, 0.7589, 0.7602, 0.7657, 0.7615, 0.7676, 0.7623, 0.7617, 0.7627, 0.7663, 0.765, 0.7662, 0.7641, 0.7647, 0.7704, 0.7679, 0.765, 0.7648, 0.763, 0.7707, 0.7658, 0.7743, 0.7628, 0.7639, 0.7594, 0.7681, 0.769, 0.7736, 0.7657, 0.768, 0.7687, 0.7676, 0.7619, 0.7652, 0.7614, 0.7718, 0.7664, 0.7676, 0.7735, 0.7704, 0.769, 0.7667, 0.7672, 0.7697, 0.7706, 0.7749, 0.7742, 0.7712, 0.7712, 0.7678, 0.7722, 0.7688, 0.7742, 0.7667, 0.7682, 0.7804, 0.7752, 0.778, 0.7781, 0.7756, 0.7727, 0.7735, 0.7615, 0.7741, 0.7688, 0.7756, 0.773, 0.7774, 0.777, 0.7756, 0.7683, 0.7711]
[21/11/24 02:40:52, INFO, FSL_SAGE_main.py:main():387] Testing loss: [1.4784520079817953, 1.4431510197965405, 1.328130153161061, 1.302541720716259, 1.2912729435329195, 1.1902346550663816, 1.1685939114305037, 1.1352114586890498, 1.2080847024917603, 1.095505337926406, 1.101574390749388, 1.0802857106245016, 1.1132105988792227, 1.044874010961267, 1.057959085778345, 1.0419824402543563, 1.0239571182033684, 0.9919034725503076, 0.9835814509210707, 0.9705653590492055, 0.9844652097436446, 0.9591581662998924, 0.9443384703201584, 0.945306018183503, 0.93613568740555, 0.9336759727212447, 0.9753982658627667, 0.9156026523324508, 0.9402312553381618, 0.9034104437767705, 0.9031416645532921, 0.8937317132949829, 0.8989356129984313, 0.901307895968232, 0.8868678913840765, 0.8671274305898932, 0.8474754072442839, 0.8791379792780816, 0.855801193774501, 0.8568612374836886, 0.8398590382141403, 0.8341639939742752, 0.8457692348504369, 0.8371083306360848, 0.844799098334735, 0.853487686265873, 0.8307169147684604, 0.8060231970835335, 0.814176952537102, 0.8171860198431378, 0.809550387195394, 0.8067362995087346, 0.8172014453743077, 0.8198103399216374, 0.8132734985291203, 0.7825119668924356, 0.8005186341985872, 0.7943052724946903, 0.788470109052296, 0.7641117142725594, 0.7731725626353976, 0.7882909729510923, 0.7946076015882855, 0.7651647457593604, 0.7781396334684347, 0.7589432695243932, 0.7888731496243537, 0.7768233448644227, 0.7430771619458741, 0.7537357550633105, 0.7501122566718089, 0.7576816451700428, 0.7596268782132789, 0.7563994685305825, 0.7497268628470505, 0.7426371302785753, 0.7414881715291664, 0.7412295356581483, 0.7343191851543475, 0.7458674651158007, 0.7366805499113058, 0.7296568698520902, 0.7221596855905992, 0.7219887798345541, 0.7332102467742148, 0.7197167631946032, 0.7303242162813114, 0.7242728562294682, 0.7302329736419871, 0.7147106075588661, 0.7153066607970225, 0.7091184911094134, 0.7432528485225726, 0.7280796358856974, 0.7221322165259833, 0.7157562299619747, 0.7109654858897004, 0.7138888145549388, 0.7262390976465201, 0.7211636659465258, 0.7223521176772781, 0.7109017636202559, 0.704490883440911, 0.6936515592321565, 0.716513770290568, 0.7114967801902867, 0.6943759258034863, 0.7017862076246286, 0.6947605481630639, 0.7119952594932122, 0.7088313845894004, 0.7026128108742871, 0.7028888939302179, 0.7023678576644463, 0.7161737065526503, 0.6971895736229571, 0.7189732273168201, 0.6802718726140035, 0.6827636998665484, 0.6853425382058832, 0.6768224801443801, 0.6720806586591503, 0.6734442250638069, 0.6942833069759079, 0.6824599391297449, 0.6848004562190816, 0.6821467276615433, 0.6878125999547258, 0.6800869405269623, 0.6878717043731786, 0.6916604457022268, 0.6746829133245009, 0.6748887885220444, 0.6744548900972439, 0.6818392831313459, 0.7007722088807746, 0.6825776892372325, 0.6792442002628423, 0.6855474280405648, 0.6702201766303822, 0.6798929455159586, 0.677567607994321, 0.6746907675568061, 0.677099063803878, 0.6810032397131377, 0.6744873021976857, 0.6842315076272699, 0.6729913293560849, 0.6776363781735867, 0.667815621140637, 0.6750591714170915, 0.6706464418882057, 0.6858868312232101, 0.6701063264774371, 0.6686576287957686, 0.6572084524963475, 0.6742504306986362, 0.6692053460622136, 0.6759505468078807, 0.6756473496744905, 0.6780980882010882, 0.667975926323782, 0.6896322886400585, 0.666108839119537, 0.6655619291565086, 0.6686695240720918, 0.6576276515858083, 0.6663276666327368, 0.6743880653683143, 0.6733261641067795, 0.6743991763531407, 0.6702826317352585, 0.6572022426731979, 0.6557043175908583, 0.6635528810416595, 0.6582528880879849, 0.6638735642161551, 0.666079613604123, 0.6561881220793422, 0.6677588348901724, 0.6631043142155756, 0.6710846061947979, 0.6847869682161114, 0.6438959369176551, 0.6539399835127818, 0.6442008210888391, 0.6457833908026731, 0.6552490018591096, 0.6577105084552041, 0.6537032863007316, 0.6862707854826239, 0.6500208166581166, 0.6626690871353391, 0.6526718490485903, 0.6511829638782936, 0.6436673010451884, 0.648796180003806, 0.6462436076960986, 0.668445532457738, 0.6547212457355065]
[21/11/24 02:40:52, INFO, FSL_SAGE_main.py:main():388] Training accuracy: [0.4043161726469059, 0.41301652066082645, 0.4561182447297892, 0.4720988839553582, 0.4655186207448298, 0.5043401736069443, 0.5166606664266571, 0.5316012640505621, 0.5100804032161287, 0.548481939277571, 0.5454418176727069, 0.5536821472858915, 0.547481899275971, 0.5654026161046442, 0.5666426657066282, 0.5652626105044202, 0.5792031681267251, 0.5866634665386615, 0.5948637945517821, 0.5988439537581504, 0.6017840713628545, 0.6037641505660226, 0.609084363374535, 0.608824352974119, 0.6146845873834953, 0.6145245809832394, 0.5976239049561982, 0.6213048521940877, 0.6112044481779271, 0.6258650346013841, 0.6269250770030801, 0.6310652426097044, 0.6325653026121045, 0.6434057362294492, 0.6368854754190167, 0.6475259010360415, 0.656286251450058, 0.640625625025001, 0.6482459298371935, 0.6509260370414817, 0.6527661106444258, 0.6533461338453538, 0.6538061522460898, 0.6596263850554022, 0.6572462898515941, 0.6514460578423137, 0.6577263090523621, 0.6772470898835954, 0.6667866714668587, 0.6665466618664747, 0.6636065442617705, 0.673106924276971, 0.6688867554702188, 0.6654666186647465, 0.6668866754670186, 0.6807672306892276, 0.6794271770870834, 0.6792871714868595, 0.6798471938877555, 0.6868074722988919, 0.687847513900556, 0.68708748349934, 0.6802672106884275, 0.6903876155046201, 0.685967438697548, 0.6921276851074043, 0.6806472258890356, 0.688047521900876, 0.6993479739189568, 0.6922476899075963, 0.6962678507140285, 0.6996479859194368, 0.6954878195127805, 0.6984679387175488, 0.7050882035281412, 0.703388135525421, 0.6977879115164607, 0.7026281051242049, 0.7054682187287491, 0.6981879275171007, 0.7005680227209088, 0.7051282051282052, 0.7093683747349894, 0.7120284811392456, 0.7010480419216769, 0.7116684667386696, 0.7083283331333253, 0.701828073122925, 0.7015080603224129, 0.7117684707388295, 0.7134085363414536, 0.7123684947397896, 0.7013080523220929, 0.7092683707348294, 0.7110884435377415, 0.7090483619344774, 0.718828753150126, 0.7079483179327173, 0.7120884835393416, 0.7115484619384775, 0.7122484899395976, 0.71806872274891, 0.7221288851554062, 0.7224488979559183, 0.7139885595423817, 0.718148725949038, 0.7205688227529101, 0.7160286411456458, 0.718488739549582, 0.7183087323492939, 0.720028801152046, 0.7169886795471819, 0.71896875875035, 0.7212288491539661, 0.7211488459538381, 0.7234089363574543, 0.7158286331453259, 0.7287291491659666, 0.7306492259690388, 0.7271490859634385, 0.7305692227689108, 0.734309372374895, 0.7339893595743829, 0.7243489739589584, 0.7328093123724949, 0.7270490819632786, 0.7314892595703828, 0.7306892275691028, 0.7368494739789592, 0.7339893595743829, 0.7322892915716629, 0.7324292971718869, 0.7336893475739029, 0.735529421176847, 0.735369414776591, 0.7268090723628945, 0.734189367574703, 0.7368694747789911, 0.7308692347693908, 0.7363094523780951, 0.7332493299731989, 0.7407696307852314, 0.7375295011800472, 0.7409896395855834, 0.735229409176367, 0.7379895195807832, 0.7308092323692947, 0.7367094683787352, 0.7370894835793431, 0.7414296571862875, 0.7401896075843034, 0.7371094843793752, 0.7304892195687828, 0.7376695067802712, 0.7396695867834714, 0.7476499059962398, 0.7379095163806553, 0.7435897435897436, 0.7397095883835353, 0.7431097243889756, 0.7363094523780951, 0.7410496419856795, 0.7370294811792472, 0.7443697747909916, 0.7418496739869594, 0.7443697747909916, 0.7447297891915676, 0.7465498619944798, 0.7378295131805273, 0.7439497579903196, 0.7420896835873435, 0.7457698307932318, 0.7479499179967198, 0.7429497179887196, 0.7466498659946398, 0.7475699027961118, 0.7474698987959518, 0.7404296171846874, 0.750290011600464, 0.7466298651946078, 0.749389975599024, 0.7439897595903836, 0.7394895795831833, 0.7518100724028961, 0.7480099203968159, 0.7514700588023521, 0.7461298451938078, 0.7475099003960158, 0.7518500740029601, 0.749209968398736, 0.7396695867834714, 0.7508500340013601, 0.7460098403936157, 0.75035001400056, 0.7471298851954078, 0.7550902036081443, 0.7525301012040482, 0.7558502340093604, 0.7495899835993439, 0.7533701348053922]
[21/11/24 02:40:52, INFO, FSL_SAGE_main.py:main():389] Training loss: [1.6413438581025024, 1.5884358888970682, 1.4905560508029152, 1.450858354871813, 1.48199845240013, 1.3713385743342585, 1.341999582661927, 1.3188805743938183, 1.3622240638004914, 1.2693971873239707, 1.2758374760169109, 1.270049789783002, 1.2661878719887965, 1.2210157586418036, 1.2119723518992804, 1.2021477754789454, 1.1752553783906932, 1.1634225537455416, 1.139979672644278, 1.1288556875163362, 1.1242317944082596, 1.1211424738396214, 1.0987388188299028, 1.1011495619026457, 1.087412927138593, 1.0804536515519818, 1.129987230586035, 1.0621146667397963, 1.0923201913445353, 1.0556350289713639, 1.0512217058783573, 1.045068364713635, 1.0295527724515998, 1.014054200576462, 1.0188649780574341, 1.0038473833608264, 0.9819224458614378, 1.010474359109505, 0.9943190279201091, 0.9858334627163623, 0.9853493619814477, 0.9727722887167797, 0.9768458251431394, 0.9632073819788964, 0.9730488670084616, 0.9758879792599278, 0.973873613748235, 0.9218373210678877, 0.9363202053778651, 0.9430604575547856, 0.9446837316639247, 0.9332424252087833, 0.9449093270544484, 0.9443294142038767, 0.9342831122966213, 0.9014095425909106, 0.915943979534484, 0.9024590593561265, 0.9066985915332046, 0.8835119915979206, 0.8920528329055728, 0.8933458786277674, 0.9113015910141341, 0.8878705266474464, 0.8892284572579479, 0.8702919537784489, 0.9110094706217448, 0.8837954624918581, 0.8552303074577079, 0.874940900857212, 0.8699074963880252, 0.8572147054829974, 0.8656304983692314, 0.8530309608267768, 0.8437057546986878, 0.8447447555665751, 0.8510818407129088, 0.8448775547909676, 0.8364220141757839, 0.8573069760514276, 0.8520048603453405, 0.8399376327754887, 0.8307831630148657, 0.8260497145980369, 0.8529655319133788, 0.8173583730789845, 0.8300947628858435, 0.8423565134747337, 0.8411091367403666, 0.8201405932278427, 0.8131873956466752, 0.8154975699104425, 0.8422437034187122, 0.8235807934486835, 0.8174554629786931, 0.8181751637361735, 0.8020662082970598, 0.8273011285233437, 0.8147323491009137, 0.8187756688540219, 0.811306538320983, 0.8051637929814462, 0.7954260242804316, 0.7918333025985699, 0.8094374720983529, 0.7985276431224425, 0.7943216692703674, 0.807009173410236, 0.7905009630072208, 0.7989222521090326, 0.7971893777980756, 0.8001293660575197, 0.7955221849240116, 0.7931596633738841, 0.7968808787924643, 0.7870699874014041, 0.8020718681599954, 0.7719311058976268, 0.7701639114445402, 0.777069943400133, 0.7618535267184405, 0.758863641333034, 0.7582552602879571, 0.7846035572105389, 0.7596029928925686, 0.7749894490375471, 0.7663724474021193, 0.764549636962153, 0.755514304677342, 0.754073334862561, 0.7661833302816968, 0.7585867748005699, 0.7573651779092299, 0.7591208862591032, 0.7547210809080352, 0.7771458283028834, 0.7549518125354485, 0.7470751409312241, 0.7607924446804832, 0.7486526941828449, 0.7591852353122701, 0.7405567154022877, 0.7477353846147163, 0.7392258916344048, 0.7562569642188288, 0.7457041284661863, 0.7590968341924459, 0.7450569884012673, 0.7462855810275818, 0.7377651559790885, 0.7404261050940166, 0.7403265434973719, 0.7666060100226608, 0.7500326739619403, 0.7331783061719123, 0.7257576256459602, 0.7409240071100133, 0.7317523851649452, 0.7358739912206587, 0.7349397528717536, 0.7460972757739875, 0.7372162668152923, 0.7505371945201592, 0.7304365738050932, 0.736359296030064, 0.7308855327486082, 0.721034342778548, 0.7220235143150688, 0.7486370051180133, 0.7293574291331167, 0.7364211242465876, 0.724781454595294, 0.7175932032916382, 0.7241282486581924, 0.7214889125969574, 0.721462721815546, 0.7181034376299715, 0.7359906595020197, 0.710382344310217, 0.7151840708334634, 0.7162809410623012, 0.7294548529250021, 0.7397616540505989, 0.7062797683644234, 0.7102018708491143, 0.7035594925322302, 0.7240726986459194, 0.7199902493535107, 0.7093565235762802, 0.7160738652747399, 0.7455246909580765, 0.7096156925342162, 0.7189906249367857, 0.7103488332290989, 0.7164562428725585, 0.7012507762314406, 0.7093488135713961, 0.6934081993181893, 0.7144888618975195, 0.7039698297134186]
