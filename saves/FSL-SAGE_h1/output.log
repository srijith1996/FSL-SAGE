[24/11/24 13:41:45, INFO, FSL_SAGE_main.py:<module>():404] Using GPU: True
[24/11/24 13:41:47, INFO, FSL_SAGE_main.py:main():107] Aggregation Factor: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
[24/11/24 13:41:47, INFO, FSL_SAGE_main.py:main():112] Random seed: 200
[24/11/24 13:41:47, INFO, FSL_SAGE_main.py:main():113] Alignment interval (l): 10
[24/11/24 13:41:47, INFO, FSL_SAGE_main.py:main():114] Batch Round (h): 1
[24/11/24 13:42:14, INFO, FSL_SAGE_main.py:main():353]  > R  0, for the weighted aggregated final model, testing loss: 1.8529e+00, testing acc: 34.91% ( 3491/10000), training loss: 1.93, training acc: 29.45%
[24/11/24 13:42:40, INFO, FSL_SAGE_main.py:main():353]  > R  1, for the weighted aggregated final model, testing loss: 1.6687e+00, testing acc: 35.97% ( 3597/10000), training loss: 1.80, training acc: 31.87%
[24/11/24 13:43:06, INFO, FSL_SAGE_main.py:main():353]  > R  2, for the weighted aggregated final model, testing loss: 1.6542e+00, testing acc: 36.58% ( 3658/10000), training loss: 1.79, training acc: 32.78%
[24/11/24 13:43:32, INFO, FSL_SAGE_main.py:main():353]  > R  3, for the weighted aggregated final model, testing loss: 1.5171e+00, testing acc: 44.28% ( 4428/10000), training loss: 1.65, training acc: 39.07%
[24/11/24 13:43:58, INFO, FSL_SAGE_main.py:main():353]  > R  4, for the weighted aggregated final model, testing loss: 1.5230e+00, testing acc: 43.22% ( 4322/10000), training loss: 1.68, training acc: 38.08%
[24/11/24 13:44:24, INFO, FSL_SAGE_main.py:main():353]  > R  5, for the weighted aggregated final model, testing loss: 1.5239e+00, testing acc: 44.38% ( 4438/10000), training loss: 1.64, training acc: 40.38%
[24/11/24 13:44:50, INFO, FSL_SAGE_main.py:main():353]  > R  6, for the weighted aggregated final model, testing loss: 1.5129e+00, testing acc: 43.47% ( 4347/10000), training loss: 1.64, training acc: 39.05%
[24/11/24 13:45:16, INFO, FSL_SAGE_main.py:main():353]  > R  7, for the weighted aggregated final model, testing loss: 1.4867e+00, testing acc: 45.10% ( 4510/10000), training loss: 1.60, training acc: 41.36%
[24/11/24 13:45:42, INFO, FSL_SAGE_main.py:main():353]  > R  8, for the weighted aggregated final model, testing loss: 1.4027e+00, testing acc: 48.20% ( 4820/10000), training loss: 1.54, training acc: 43.59%
[24/11/24 13:46:08, INFO, FSL_SAGE_main.py:main():353]  > R  9, for the weighted aggregated final model, testing loss: 1.4021e+00, testing acc: 48.54% ( 4854/10000), training loss: 1.52, training acc: 44.15%
[24/11/24 13:46:37, INFO, FSL_SAGE_main.py:main():353]  > R 10, for the weighted aggregated final model, testing loss: 1.3362e+00, testing acc: 51.68% ( 5168/10000), training loss: 1.47, training acc: 46.36%
[24/11/24 13:47:03, INFO, FSL_SAGE_main.py:main():353]  > R 11, for the weighted aggregated final model, testing loss: 1.2960e+00, testing acc: 53.62% ( 5362/10000), training loss: 1.41, training acc: 48.62%
[24/11/24 13:47:28, INFO, FSL_SAGE_main.py:main():353]  > R 12, for the weighted aggregated final model, testing loss: 1.2507e+00, testing acc: 54.27% ( 5427/10000), training loss: 1.39, training acc: 49.10%
[24/11/24 13:47:54, INFO, FSL_SAGE_main.py:main():353]  > R 13, for the weighted aggregated final model, testing loss: 1.1919e+00, testing acc: 57.87% ( 5787/10000), training loss: 1.32, training acc: 52.33%
[24/11/24 13:48:20, INFO, FSL_SAGE_main.py:main():353]  > R 14, for the weighted aggregated final model, testing loss: 1.1662e+00, testing acc: 58.62% ( 5862/10000), training loss: 1.28, training acc: 53.90%
[24/11/24 13:48:46, INFO, FSL_SAGE_main.py:main():353]  > R 15, for the weighted aggregated final model, testing loss: 1.1649e+00, testing acc: 58.27% ( 5827/10000), training loss: 1.30, training acc: 52.91%
[24/11/24 13:49:12, INFO, FSL_SAGE_main.py:main():353]  > R 16, for the weighted aggregated final model, testing loss: 1.1399e+00, testing acc: 59.30% ( 5930/10000), training loss: 1.27, training acc: 54.66%
[24/11/24 13:49:38, INFO, FSL_SAGE_main.py:main():353]  > R 17, for the weighted aggregated final model, testing loss: 1.1520e+00, testing acc: 58.77% ( 5877/10000), training loss: 1.28, training acc: 53.56%
[24/11/24 13:50:04, INFO, FSL_SAGE_main.py:main():353]  > R 18, for the weighted aggregated final model, testing loss: 1.1177e+00, testing acc: 59.94% ( 5994/10000), training loss: 1.27, training acc: 54.00%
[24/11/24 13:50:30, INFO, FSL_SAGE_main.py:main():353]  > R 19, for the weighted aggregated final model, testing loss: 1.1452e+00, testing acc: 59.46% ( 5946/10000), training loss: 1.27, training acc: 54.34%
[24/11/24 13:50:59, INFO, FSL_SAGE_main.py:main():353]  > R 20, for the weighted aggregated final model, testing loss: 1.0438e+00, testing acc: 63.03% ( 6303/10000), training loss: 1.18, training acc: 58.13%
[24/11/24 13:51:25, INFO, FSL_SAGE_main.py:main():353]  > R 21, for the weighted aggregated final model, testing loss: 1.0253e+00, testing acc: 64.01% ( 6401/10000), training loss: 1.15, training acc: 58.62%
[24/11/24 13:51:51, INFO, FSL_SAGE_main.py:main():353]  > R 22, for the weighted aggregated final model, testing loss: 1.0070e+00, testing acc: 64.55% ( 6455/10000), training loss: 1.13, training acc: 59.64%
[24/11/24 13:52:17, INFO, FSL_SAGE_main.py:main():353]  > R 23, for the weighted aggregated final model, testing loss: 1.0134e+00, testing acc: 64.25% ( 6425/10000), training loss: 1.13, training acc: 59.57%
[24/11/24 13:52:43, INFO, FSL_SAGE_main.py:main():353]  > R 24, for the weighted aggregated final model, testing loss: 9.7206e-01, testing acc: 66.27% ( 6627/10000), training loss: 1.11, training acc: 60.37%
[24/11/24 13:53:09, INFO, FSL_SAGE_main.py:main():353]  > R 25, for the weighted aggregated final model, testing loss: 1.0112e+00, testing acc: 64.85% ( 6485/10000), training loss: 1.15, training acc: 59.95%
[24/11/24 13:53:35, INFO, FSL_SAGE_main.py:main():353]  > R 26, for the weighted aggregated final model, testing loss: 9.7105e-01, testing acc: 66.12% ( 6612/10000), training loss: 1.08, training acc: 61.55%
[24/11/24 13:54:01, INFO, FSL_SAGE_main.py:main():353]  > R 27, for the weighted aggregated final model, testing loss: 9.3518e-01, testing acc: 67.17% ( 6717/10000), training loss: 1.07, training acc: 62.00%
[24/11/24 13:54:27, INFO, FSL_SAGE_main.py:main():353]  > R 28, for the weighted aggregated final model, testing loss: 9.2832e-01, testing acc: 67.79% ( 6779/10000), training loss: 1.06, training acc: 62.16%
[24/11/24 13:54:52, INFO, FSL_SAGE_main.py:main():353]  > R 29, for the weighted aggregated final model, testing loss: 9.2895e-01, testing acc: 67.22% ( 6722/10000), training loss: 1.04, training acc: 62.89%
[24/11/24 13:55:22, INFO, FSL_SAGE_main.py:main():353]  > R 30, for the weighted aggregated final model, testing loss: 9.4038e-01, testing acc: 66.80% ( 6680/10000), training loss: 1.04, training acc: 63.16%
[24/11/24 13:55:47, INFO, FSL_SAGE_main.py:main():353]  > R 31, for the weighted aggregated final model, testing loss: 9.3118e-01, testing acc: 67.32% ( 6732/10000), training loss: 1.05, training acc: 62.51%
[24/11/24 13:56:13, INFO, FSL_SAGE_main.py:main():353]  > R 32, for the weighted aggregated final model, testing loss: 9.2032e-01, testing acc: 67.73% ( 6773/10000), training loss: 1.03, training acc: 63.09%
[24/11/24 13:56:39, INFO, FSL_SAGE_main.py:main():353]  > R 33, for the weighted aggregated final model, testing loss: 9.0352e-01, testing acc: 68.57% ( 6857/10000), training loss: 1.02, training acc: 63.86%
[24/11/24 13:57:05, INFO, FSL_SAGE_main.py:main():353]  > R 34, for the weighted aggregated final model, testing loss: 8.6726e-01, testing acc: 69.74% ( 6974/10000), training loss: 0.97, training acc: 65.72%
[24/11/24 13:57:31, INFO, FSL_SAGE_main.py:main():353]  > R 35, for the weighted aggregated final model, testing loss: 8.8860e-01, testing acc: 69.33% ( 6933/10000), training loss: 0.99, training acc: 65.06%
[24/11/24 13:57:57, INFO, FSL_SAGE_main.py:main():353]  > R 36, for the weighted aggregated final model, testing loss: 8.5993e-01, testing acc: 69.59% ( 6959/10000), training loss: 0.97, training acc: 65.55%
[24/11/24 13:58:23, INFO, FSL_SAGE_main.py:main():353]  > R 37, for the weighted aggregated final model, testing loss: 8.4863e-01, testing acc: 70.57% ( 7057/10000), training loss: 0.97, training acc: 65.73%
[24/11/24 13:58:49, INFO, FSL_SAGE_main.py:main():353]  > R 38, for the weighted aggregated final model, testing loss: 8.4373e-01, testing acc: 70.66% ( 7066/10000), training loss: 0.95, training acc: 66.49%
[24/11/24 13:59:15, INFO, FSL_SAGE_main.py:main():353]  > R 39, for the weighted aggregated final model, testing loss: 8.5678e-01, testing acc: 70.17% ( 7017/10000), training loss: 0.97, training acc: 65.75%
[24/11/24 13:59:44, INFO, FSL_SAGE_main.py:main():353]  > R 40, for the weighted aggregated final model, testing loss: 8.5578e-01, testing acc: 70.35% ( 7035/10000), training loss: 0.97, training acc: 65.88%
[24/11/24 14:00:10, INFO, FSL_SAGE_main.py:main():353]  > R 41, for the weighted aggregated final model, testing loss: 8.4876e-01, testing acc: 70.07% ( 7007/10000), training loss: 0.95, training acc: 66.20%
[24/11/24 14:00:36, INFO, FSL_SAGE_main.py:main():353]  > R 42, for the weighted aggregated final model, testing loss: 8.2212e-01, testing acc: 71.49% ( 7149/10000), training loss: 0.94, training acc: 66.63%
[24/11/24 14:01:02, INFO, FSL_SAGE_main.py:main():353]  > R 43, for the weighted aggregated final model, testing loss: 8.2300e-01, testing acc: 71.33% ( 7133/10000), training loss: 0.94, training acc: 66.49%
[24/11/24 14:01:28, INFO, FSL_SAGE_main.py:main():353]  > R 44, for the weighted aggregated final model, testing loss: 8.1254e-01, testing acc: 71.59% ( 7159/10000), training loss: 0.92, training acc: 67.32%
[24/11/24 14:01:54, INFO, FSL_SAGE_main.py:main():353]  > R 45, for the weighted aggregated final model, testing loss: 8.3136e-01, testing acc: 70.77% ( 7077/10000), training loss: 0.93, training acc: 66.99%
[24/11/24 14:02:20, INFO, FSL_SAGE_main.py:main():353]  > R 46, for the weighted aggregated final model, testing loss: 8.2035e-01, testing acc: 71.30% ( 7130/10000), training loss: 0.92, training acc: 67.60%
[24/11/24 14:02:45, INFO, FSL_SAGE_main.py:main():353]  > R 47, for the weighted aggregated final model, testing loss: 8.1016e-01, testing acc: 71.88% ( 7188/10000), training loss: 0.92, training acc: 67.23%
[24/11/24 14:03:11, INFO, FSL_SAGE_main.py:main():353]  > R 48, for the weighted aggregated final model, testing loss: 7.9538e-01, testing acc: 72.19% ( 7219/10000), training loss: 0.90, training acc: 68.29%
[24/11/24 14:03:37, INFO, FSL_SAGE_main.py:main():353]  > R 49, for the weighted aggregated final model, testing loss: 7.9403e-01, testing acc: 72.20% ( 7220/10000), training loss: 0.89, training acc: 68.44%
[24/11/24 14:04:06, INFO, FSL_SAGE_main.py:main():353]  > R 50, for the weighted aggregated final model, testing loss: 7.9920e-01, testing acc: 72.05% ( 7205/10000), training loss: 0.91, training acc: 67.86%
[24/11/24 14:04:32, INFO, FSL_SAGE_main.py:main():353]  > R 51, for the weighted aggregated final model, testing loss: 7.9273e-01, testing acc: 72.38% ( 7238/10000), training loss: 0.88, training acc: 68.74%
[24/11/24 14:04:58, INFO, FSL_SAGE_main.py:main():353]  > R 52, for the weighted aggregated final model, testing loss: 7.9478e-01, testing acc: 72.11% ( 7211/10000), training loss: 0.90, training acc: 68.06%
[24/11/24 14:05:24, INFO, FSL_SAGE_main.py:main():353]  > R 53, for the weighted aggregated final model, testing loss: 7.8576e-01, testing acc: 72.76% ( 7276/10000), training loss: 0.88, training acc: 69.10%
[24/11/24 14:05:50, INFO, FSL_SAGE_main.py:main():353]  > R 54, for the weighted aggregated final model, testing loss: 7.9298e-01, testing acc: 72.52% ( 7252/10000), training loss: 0.88, training acc: 68.82%
[24/11/24 14:06:16, INFO, FSL_SAGE_main.py:main():353]  > R 55, for the weighted aggregated final model, testing loss: 7.8111e-01, testing acc: 72.86% ( 7286/10000), training loss: 0.87, training acc: 69.01%
[24/11/24 14:06:42, INFO, FSL_SAGE_main.py:main():353]  > R 56, for the weighted aggregated final model, testing loss: 7.5224e-01, testing acc: 73.69% ( 7369/10000), training loss: 0.85, training acc: 69.85%
[24/11/24 14:07:08, INFO, FSL_SAGE_main.py:main():353]  > R 57, for the weighted aggregated final model, testing loss: 7.7445e-01, testing acc: 73.09% ( 7309/10000), training loss: 0.86, training acc: 69.38%
[24/11/24 14:07:34, INFO, FSL_SAGE_main.py:main():353]  > R 58, for the weighted aggregated final model, testing loss: 7.6459e-01, testing acc: 73.56% ( 7356/10000), training loss: 0.87, training acc: 69.48%
[24/11/24 14:08:00, INFO, FSL_SAGE_main.py:main():353]  > R 59, for the weighted aggregated final model, testing loss: 7.6403e-01, testing acc: 72.96% ( 7296/10000), training loss: 0.85, training acc: 70.09%
[24/11/24 14:08:29, INFO, FSL_SAGE_main.py:main():353]  > R 60, for the weighted aggregated final model, testing loss: 7.7421e-01, testing acc: 73.11% ( 7311/10000), training loss: 0.87, training acc: 69.55%
[24/11/24 14:08:54, INFO, FSL_SAGE_main.py:main():353]  > R 61, for the weighted aggregated final model, testing loss: 7.8106e-01, testing acc: 73.21% ( 7321/10000), training loss: 0.88, training acc: 68.81%
[24/11/24 14:09:20, INFO, FSL_SAGE_main.py:main():353]  > R 62, for the weighted aggregated final model, testing loss: 7.5362e-01, testing acc: 73.47% ( 7347/10000), training loss: 0.85, training acc: 70.04%
[24/11/24 14:09:46, INFO, FSL_SAGE_main.py:main():353]  > R 63, for the weighted aggregated final model, testing loss: 7.4936e-01, testing acc: 74.10% ( 7410/10000), training loss: 0.83, training acc: 70.55%
[24/11/24 14:10:12, INFO, FSL_SAGE_main.py:main():353]  > R 64, for the weighted aggregated final model, testing loss: 7.4884e-01, testing acc: 74.02% ( 7402/10000), training loss: 0.84, training acc: 70.33%
[24/11/24 14:10:38, INFO, FSL_SAGE_main.py:main():353]  > R 65, for the weighted aggregated final model, testing loss: 7.4927e-01, testing acc: 73.71% ( 7371/10000), training loss: 0.82, training acc: 70.92%
[24/11/24 14:11:04, INFO, FSL_SAGE_main.py:main():353]  > R 66, for the weighted aggregated final model, testing loss: 7.3932e-01, testing acc: 74.04% ( 7404/10000), training loss: 0.81, training acc: 71.38%
[24/11/24 14:11:30, INFO, FSL_SAGE_main.py:main():353]  > R 67, for the weighted aggregated final model, testing loss: 7.5105e-01, testing acc: 74.11% ( 7411/10000), training loss: 0.83, training acc: 70.90%
[24/11/24 14:11:56, INFO, FSL_SAGE_main.py:main():353]  > R 68, for the weighted aggregated final model, testing loss: 7.4553e-01, testing acc: 74.18% ( 7418/10000), training loss: 0.82, training acc: 71.51%
[24/11/24 14:12:22, INFO, FSL_SAGE_main.py:main():353]  > R 69, for the weighted aggregated final model, testing loss: 7.3818e-01, testing acc: 74.29% ( 7429/10000), training loss: 0.80, training acc: 71.61%
[24/11/24 14:12:51, INFO, FSL_SAGE_main.py:main():353]  > R 70, for the weighted aggregated final model, testing loss: 7.5086e-01, testing acc: 73.72% ( 7372/10000), training loss: 0.83, training acc: 70.70%
[24/11/24 14:13:17, INFO, FSL_SAGE_main.py:main():353]  > R 71, for the weighted aggregated final model, testing loss: 7.4370e-01, testing acc: 74.10% ( 7410/10000), training loss: 0.82, training acc: 71.14%
[24/11/24 14:13:43, INFO, FSL_SAGE_main.py:main():353]  > R 72, for the weighted aggregated final model, testing loss: 7.3666e-01, testing acc: 74.18% ( 7418/10000), training loss: 0.81, training acc: 71.35%
[24/11/24 14:14:09, INFO, FSL_SAGE_main.py:main():353]  > R 73, for the weighted aggregated final model, testing loss: 7.4215e-01, testing acc: 73.82% ( 7382/10000), training loss: 0.80, training acc: 71.52%
[24/11/24 14:14:35, INFO, FSL_SAGE_main.py:main():353]  > R 74, for the weighted aggregated final model, testing loss: 7.3275e-01, testing acc: 74.64% ( 7464/10000), training loss: 0.79, training acc: 71.75%
[24/11/24 14:15:01, INFO, FSL_SAGE_main.py:main():353]  > R 75, for the weighted aggregated final model, testing loss: 7.4034e-01, testing acc: 74.27% ( 7427/10000), training loss: 0.81, training acc: 71.50%
[24/11/24 14:15:27, INFO, FSL_SAGE_main.py:main():353]  > R 76, for the weighted aggregated final model, testing loss: 7.4666e-01, testing acc: 74.43% ( 7443/10000), training loss: 0.83, training acc: 70.67%
[24/11/24 14:15:53, INFO, FSL_SAGE_main.py:main():353]  > R 77, for the weighted aggregated final model, testing loss: 7.2451e-01, testing acc: 74.98% ( 7498/10000), training loss: 0.79, training acc: 72.09%
[24/11/24 14:16:19, INFO, FSL_SAGE_main.py:main():353]  > R 78, for the weighted aggregated final model, testing loss: 7.3027e-01, testing acc: 74.70% ( 7470/10000), training loss: 0.79, training acc: 72.04%
[24/11/24 14:16:45, INFO, FSL_SAGE_main.py:main():353]  > R 79, for the weighted aggregated final model, testing loss: 7.1082e-01, testing acc: 75.52% ( 7552/10000), training loss: 0.77, training acc: 72.57%
[24/11/24 14:17:14, INFO, FSL_SAGE_main.py:main():353]  > R 80, for the weighted aggregated final model, testing loss: 7.2906e-01, testing acc: 74.44% ( 7444/10000), training loss: 0.78, training acc: 72.23%
[24/11/24 14:17:40, INFO, FSL_SAGE_main.py:main():353]  > R 81, for the weighted aggregated final model, testing loss: 7.3280e-01, testing acc: 74.87% ( 7487/10000), training loss: 0.78, training acc: 72.41%
[24/11/24 14:18:06, INFO, FSL_SAGE_main.py:main():353]  > R 82, for the weighted aggregated final model, testing loss: 7.1686e-01, testing acc: 75.03% ( 7503/10000), training loss: 0.78, training acc: 72.59%
[24/11/24 14:18:32, INFO, FSL_SAGE_main.py:main():353]  > R 83, for the weighted aggregated final model, testing loss: 7.2888e-01, testing acc: 74.99% ( 7499/10000), training loss: 0.80, training acc: 72.03%
[24/11/24 14:18:58, INFO, FSL_SAGE_main.py:main():353]  > R 84, for the weighted aggregated final model, testing loss: 7.2456e-01, testing acc: 74.57% ( 7457/10000), training loss: 0.79, training acc: 72.20%
[24/11/24 14:19:24, INFO, FSL_SAGE_main.py:main():353]  > R 85, for the weighted aggregated final model, testing loss: 7.2197e-01, testing acc: 75.23% ( 7523/10000), training loss: 0.78, training acc: 72.58%
[24/11/24 14:19:50, INFO, FSL_SAGE_main.py:main():353]  > R 86, for the weighted aggregated final model, testing loss: 7.0937e-01, testing acc: 75.43% ( 7543/10000), training loss: 0.77, training acc: 72.85%
[24/11/24 14:20:16, INFO, FSL_SAGE_main.py:main():353]  > R 87, for the weighted aggregated final model, testing loss: 7.3418e-01, testing acc: 74.62% ( 7462/10000), training loss: 0.78, training acc: 72.25%
[24/11/24 14:20:42, INFO, FSL_SAGE_main.py:main():353]  > R 88, for the weighted aggregated final model, testing loss: 7.0967e-01, testing acc: 75.75% ( 7575/10000), training loss: 0.76, training acc: 73.40%
[24/11/24 14:21:08, INFO, FSL_SAGE_main.py:main():353]  > R 89, for the weighted aggregated final model, testing loss: 7.1475e-01, testing acc: 75.21% ( 7521/10000), training loss: 0.78, training acc: 72.58%
[24/11/24 14:21:37, INFO, FSL_SAGE_main.py:main():353]  > R 90, for the weighted aggregated final model, testing loss: 7.2213e-01, testing acc: 75.38% ( 7538/10000), training loss: 0.78, training acc: 72.44%
[24/11/24 14:22:03, INFO, FSL_SAGE_main.py:main():353]  > R 91, for the weighted aggregated final model, testing loss: 7.2153e-01, testing acc: 74.97% ( 7497/10000), training loss: 0.79, training acc: 72.02%
[24/11/24 14:22:29, INFO, FSL_SAGE_main.py:main():353]  > R 92, for the weighted aggregated final model, testing loss: 6.9397e-01, testing acc: 75.99% ( 7599/10000), training loss: 0.76, training acc: 73.42%
[24/11/24 14:22:55, INFO, FSL_SAGE_main.py:main():353]  > R 93, for the weighted aggregated final model, testing loss: 6.8988e-01, testing acc: 75.97% ( 7597/10000), training loss: 0.75, training acc: 73.71%
[24/11/24 14:23:21, INFO, FSL_SAGE_main.py:main():353]  > R 94, for the weighted aggregated final model, testing loss: 7.0225e-01, testing acc: 76.10% ( 7610/10000), training loss: 0.76, training acc: 73.27%
[24/11/24 14:23:47, INFO, FSL_SAGE_main.py:main():353]  > R 95, for the weighted aggregated final model, testing loss: 6.9533e-01, testing acc: 75.77% ( 7577/10000), training loss: 0.76, training acc: 73.34%
[24/11/24 14:24:13, INFO, FSL_SAGE_main.py:main():353]  > R 96, for the weighted aggregated final model, testing loss: 7.0503e-01, testing acc: 76.06% ( 7606/10000), training loss: 0.76, training acc: 73.19%
[24/11/24 14:24:39, INFO, FSL_SAGE_main.py:main():353]  > R 97, for the weighted aggregated final model, testing loss: 6.9696e-01, testing acc: 75.88% ( 7588/10000), training loss: 0.74, training acc: 74.01%
[24/11/24 14:25:05, INFO, FSL_SAGE_main.py:main():353]  > R 98, for the weighted aggregated final model, testing loss: 7.1103e-01, testing acc: 75.33% ( 7533/10000), training loss: 0.76, training acc: 73.05%
[24/11/24 14:25:31, INFO, FSL_SAGE_main.py:main():353]  > R 99, for the weighted aggregated final model, testing loss: 6.9667e-01, testing acc: 75.68% ( 7568/10000), training loss: 0.74, training acc: 73.77%
[24/11/24 14:26:00, INFO, FSL_SAGE_main.py:main():353]  > R 100, for the weighted aggregated final model, testing loss: 6.9676e-01, testing acc: 75.90% ( 7590/10000), training loss: 0.76, training acc: 73.38%
[24/11/24 14:26:26, INFO, FSL_SAGE_main.py:main():353]  > R 101, for the weighted aggregated final model, testing loss: 6.9193e-01, testing acc: 76.18% ( 7618/10000), training loss: 0.75, training acc: 73.64%
[24/11/24 14:26:52, INFO, FSL_SAGE_main.py:main():353]  > R 102, for the weighted aggregated final model, testing loss: 7.1890e-01, testing acc: 74.95% ( 7495/10000), training loss: 0.76, training acc: 73.11%
[24/11/24 14:27:18, INFO, FSL_SAGE_main.py:main():353]  > R 103, for the weighted aggregated final model, testing loss: 6.8391e-01, testing acc: 76.69% ( 7669/10000), training loss: 0.73, training acc: 74.15%
[24/11/24 14:27:44, INFO, FSL_SAGE_main.py:main():353]  > R 104, for the weighted aggregated final model, testing loss: 6.9812e-01, testing acc: 75.87% ( 7587/10000), training loss: 0.75, training acc: 73.48%
[24/11/24 14:28:10, INFO, FSL_SAGE_main.py:main():353]  > R 105, for the weighted aggregated final model, testing loss: 6.8868e-01, testing acc: 75.71% ( 7571/10000), training loss: 0.73, training acc: 74.26%
[24/11/24 14:28:36, INFO, FSL_SAGE_main.py:main():353]  > R 106, for the weighted aggregated final model, testing loss: 6.8074e-01, testing acc: 76.41% ( 7641/10000), training loss: 0.73, training acc: 74.23%
[24/11/24 14:29:02, INFO, FSL_SAGE_main.py:main():353]  > R 107, for the weighted aggregated final model, testing loss: 6.9859e-01, testing acc: 75.80% ( 7580/10000), training loss: 0.76, training acc: 73.20%
[24/11/24 14:29:27, INFO, FSL_SAGE_main.py:main():353]  > R 108, for the weighted aggregated final model, testing loss: 6.8831e-01, testing acc: 76.61% ( 7661/10000), training loss: 0.73, training acc: 74.36%
[24/11/24 14:29:53, INFO, FSL_SAGE_main.py:main():353]  > R 109, for the weighted aggregated final model, testing loss: 6.7270e-01, testing acc: 77.10% ( 7710/10000), training loss: 0.72, training acc: 74.61%
[24/11/24 14:30:22, INFO, FSL_SAGE_main.py:main():353]  > R 110, for the weighted aggregated final model, testing loss: 6.8199e-01, testing acc: 76.62% ( 7662/10000), training loss: 0.73, training acc: 74.26%
[24/11/24 14:30:48, INFO, FSL_SAGE_main.py:main():353]  > R 111, for the weighted aggregated final model, testing loss: 6.8036e-01, testing acc: 76.60% ( 7660/10000), training loss: 0.73, training acc: 74.71%
[24/11/24 14:31:14, INFO, FSL_SAGE_main.py:main():353]  > R 112, for the weighted aggregated final model, testing loss: 6.7911e-01, testing acc: 76.95% ( 7695/10000), training loss: 0.73, training acc: 74.15%
[24/11/24 14:31:40, INFO, FSL_SAGE_main.py:main():353]  > R 113, for the weighted aggregated final model, testing loss: 6.8664e-01, testing acc: 76.35% ( 7635/10000), training loss: 0.73, training acc: 74.36%
[24/11/24 14:32:05, INFO, FSL_SAGE_main.py:main():353]  > R 114, for the weighted aggregated final model, testing loss: 6.8913e-01, testing acc: 76.22% ( 7622/10000), training loss: 0.74, training acc: 73.79%
[24/11/24 14:32:31, INFO, FSL_SAGE_main.py:main():353]  > R 115, for the weighted aggregated final model, testing loss: 6.7394e-01, testing acc: 77.03% ( 7703/10000), training loss: 0.72, training acc: 74.34%
[24/11/24 14:32:57, INFO, FSL_SAGE_main.py:main():353]  > R 116, for the weighted aggregated final model, testing loss: 6.9428e-01, testing acc: 76.22% ( 7622/10000), training loss: 0.73, training acc: 74.49%
[24/11/24 14:33:23, INFO, FSL_SAGE_main.py:main():353]  > R 117, for the weighted aggregated final model, testing loss: 7.0712e-01, testing acc: 75.98% ( 7598/10000), training loss: 0.73, training acc: 73.92%
[24/11/24 14:33:49, INFO, FSL_SAGE_main.py:main():353]  > R 118, for the weighted aggregated final model, testing loss: 6.8408e-01, testing acc: 76.89% ( 7689/10000), training loss: 0.73, training acc: 74.50%
[24/11/24 14:34:14, INFO, FSL_SAGE_main.py:main():353]  > R 119, for the weighted aggregated final model, testing loss: 6.7998e-01, testing acc: 76.37% ( 7637/10000), training loss: 0.71, training acc: 74.94%
[24/11/24 14:34:43, INFO, FSL_SAGE_main.py:main():353]  > R 120, for the weighted aggregated final model, testing loss: 6.8566e-01, testing acc: 76.18% ( 7618/10000), training loss: 0.73, training acc: 74.32%
[24/11/24 14:35:09, INFO, FSL_SAGE_main.py:main():353]  > R 121, for the weighted aggregated final model, testing loss: 6.9715e-01, testing acc: 75.88% ( 7588/10000), training loss: 0.74, training acc: 73.86%
[24/11/24 14:35:35, INFO, FSL_SAGE_main.py:main():353]  > R 122, for the weighted aggregated final model, testing loss: 6.7538e-01, testing acc: 76.71% ( 7671/10000), training loss: 0.72, training acc: 74.48%
[24/11/24 14:36:01, INFO, FSL_SAGE_main.py:main():353]  > R 123, for the weighted aggregated final model, testing loss: 7.0238e-01, testing acc: 75.98% ( 7598/10000), training loss: 0.73, training acc: 74.44%
[24/11/24 14:36:26, INFO, FSL_SAGE_main.py:main():353]  > R 124, for the weighted aggregated final model, testing loss: 6.7316e-01, testing acc: 77.13% ( 7713/10000), training loss: 0.71, training acc: 74.91%
[24/11/24 14:36:52, INFO, FSL_SAGE_main.py:main():353]  > R 125, for the weighted aggregated final model, testing loss: 6.6201e-01, testing acc: 77.23% ( 7723/10000), training loss: 0.71, training acc: 75.19%
[24/11/24 14:37:18, INFO, FSL_SAGE_main.py:main():353]  > R 126, for the weighted aggregated final model, testing loss: 6.7309e-01, testing acc: 77.22% ( 7722/10000), training loss: 0.70, training acc: 75.54%
[24/11/24 14:37:44, INFO, FSL_SAGE_main.py:main():353]  > R 127, for the weighted aggregated final model, testing loss: 6.6763e-01, testing acc: 77.29% ( 7729/10000), training loss: 0.72, training acc: 74.76%
[24/11/24 14:38:09, INFO, FSL_SAGE_main.py:main():353]  > R 128, for the weighted aggregated final model, testing loss: 6.7039e-01, testing acc: 77.32% ( 7732/10000), training loss: 0.71, training acc: 75.06%
[24/11/24 14:38:35, INFO, FSL_SAGE_main.py:main():353]  > R 129, for the weighted aggregated final model, testing loss: 6.6110e-01, testing acc: 77.18% ( 7718/10000), training loss: 0.69, training acc: 75.45%
[24/11/24 14:39:04, INFO, FSL_SAGE_main.py:main():353]  > R 130, for the weighted aggregated final model, testing loss: 6.6190e-01, testing acc: 77.59% ( 7759/10000), training loss: 0.71, training acc: 74.96%
[24/11/24 14:39:30, INFO, FSL_SAGE_main.py:main():353]  > R 131, for the weighted aggregated final model, testing loss: 6.7902e-01, testing acc: 76.90% ( 7690/10000), training loss: 0.74, training acc: 74.22%
[24/11/24 14:39:55, INFO, FSL_SAGE_main.py:main():353]  > R 132, for the weighted aggregated final model, testing loss: 6.7593e-01, testing acc: 76.86% ( 7686/10000), training loss: 0.72, training acc: 74.69%
[24/11/24 14:40:21, INFO, FSL_SAGE_main.py:main():353]  > R 133, for the weighted aggregated final model, testing loss: 6.6542e-01, testing acc: 77.33% ( 7733/10000), training loss: 0.69, training acc: 75.70%
[24/11/24 14:40:47, INFO, FSL_SAGE_main.py:main():353]  > R 134, for the weighted aggregated final model, testing loss: 6.5829e-01, testing acc: 77.03% ( 7703/10000), training loss: 0.70, training acc: 75.27%
[24/11/24 14:41:13, INFO, FSL_SAGE_main.py:main():353]  > R 135, for the weighted aggregated final model, testing loss: 6.7561e-01, testing acc: 76.79% ( 7679/10000), training loss: 0.70, training acc: 75.33%
[24/11/24 14:41:38, INFO, FSL_SAGE_main.py:main():353]  > R 136, for the weighted aggregated final model, testing loss: 6.5374e-01, testing acc: 77.73% ( 7773/10000), training loss: 0.69, training acc: 75.59%
[24/11/24 14:42:04, INFO, FSL_SAGE_main.py:main():353]  > R 137, for the weighted aggregated final model, testing loss: 6.5075e-01, testing acc: 77.67% ( 7767/10000), training loss: 0.69, training acc: 75.69%
[24/11/24 14:42:30, INFO, FSL_SAGE_main.py:main():353]  > R 138, for the weighted aggregated final model, testing loss: 6.5840e-01, testing acc: 77.30% ( 7730/10000), training loss: 0.69, training acc: 75.77%
[24/11/24 14:42:56, INFO, FSL_SAGE_main.py:main():353]  > R 139, for the weighted aggregated final model, testing loss: 6.5357e-01, testing acc: 77.89% ( 7789/10000), training loss: 0.69, training acc: 75.61%
[24/11/24 14:43:25, INFO, FSL_SAGE_main.py:main():353]  > R 140, for the weighted aggregated final model, testing loss: 6.9430e-01, testing acc: 76.27% ( 7627/10000), training loss: 0.72, training acc: 74.49%
[24/11/24 14:43:50, INFO, FSL_SAGE_main.py:main():353]  > R 141, for the weighted aggregated final model, testing loss: 6.7603e-01, testing acc: 76.83% ( 7683/10000), training loss: 0.71, training acc: 75.17%
[24/11/24 14:44:16, INFO, FSL_SAGE_main.py:main():353]  > R 142, for the weighted aggregated final model, testing loss: 6.6664e-01, testing acc: 77.04% ( 7704/10000), training loss: 0.70, training acc: 75.44%
[24/11/24 14:44:42, INFO, FSL_SAGE_main.py:main():353]  > R 143, for the weighted aggregated final model, testing loss: 6.4571e-01, testing acc: 78.06% ( 7806/10000), training loss: 0.68, training acc: 76.27%
[24/11/24 14:45:08, INFO, FSL_SAGE_main.py:main():353]  > R 144, for the weighted aggregated final model, testing loss: 6.7033e-01, testing acc: 77.53% ( 7753/10000), training loss: 0.70, training acc: 75.62%
[24/11/24 14:45:33, INFO, FSL_SAGE_main.py:main():353]  > R 145, for the weighted aggregated final model, testing loss: 6.5193e-01, testing acc: 77.51% ( 7751/10000), training loss: 0.68, training acc: 76.05%
[24/11/24 14:45:59, INFO, FSL_SAGE_main.py:main():353]  > R 146, for the weighted aggregated final model, testing loss: 6.6131e-01, testing acc: 77.46% ( 7746/10000), training loss: 0.68, training acc: 75.91%
[24/11/24 14:46:25, INFO, FSL_SAGE_main.py:main():353]  > R 147, for the weighted aggregated final model, testing loss: 6.4849e-01, testing acc: 77.56% ( 7756/10000), training loss: 0.71, training acc: 75.34%
[24/11/24 14:46:51, INFO, FSL_SAGE_main.py:main():353]  > R 148, for the weighted aggregated final model, testing loss: 6.6013e-01, testing acc: 77.10% ( 7710/10000), training loss: 0.69, training acc: 75.41%
[24/11/24 14:47:17, INFO, FSL_SAGE_main.py:main():353]  > R 149, for the weighted aggregated final model, testing loss: 6.4124e-01, testing acc: 78.07% ( 7807/10000), training loss: 0.67, training acc: 76.30%
[24/11/24 14:47:45, INFO, FSL_SAGE_main.py:main():353]  > R 150, for the weighted aggregated final model, testing loss: 6.5845e-01, testing acc: 77.30% ( 7730/10000), training loss: 0.71, training acc: 74.99%
[24/11/24 14:48:11, INFO, FSL_SAGE_main.py:main():353]  > R 151, for the weighted aggregated final model, testing loss: 6.4044e-01, testing acc: 77.80% ( 7780/10000), training loss: 0.68, training acc: 76.12%
[24/11/24 14:48:37, INFO, FSL_SAGE_main.py:main():353]  > R 152, for the weighted aggregated final model, testing loss: 6.5515e-01, testing acc: 77.79% ( 7779/10000), training loss: 0.69, training acc: 75.76%
[24/11/24 14:49:03, INFO, FSL_SAGE_main.py:main():353]  > R 153, for the weighted aggregated final model, testing loss: 6.5218e-01, testing acc: 77.54% ( 7754/10000), training loss: 0.69, training acc: 75.79%
[24/11/24 14:49:29, INFO, FSL_SAGE_main.py:main():353]  > R 154, for the weighted aggregated final model, testing loss: 6.4610e-01, testing acc: 77.72% ( 7772/10000), training loss: 0.69, training acc: 75.88%
[24/11/24 14:49:54, INFO, FSL_SAGE_main.py:main():353]  > R 155, for the weighted aggregated final model, testing loss: 6.5322e-01, testing acc: 77.59% ( 7759/10000), training loss: 0.68, training acc: 75.79%
[24/11/24 14:50:20, INFO, FSL_SAGE_main.py:main():353]  > R 156, for the weighted aggregated final model, testing loss: 6.5456e-01, testing acc: 77.38% ( 7738/10000), training loss: 0.68, training acc: 75.87%
[24/11/24 14:50:46, INFO, FSL_SAGE_main.py:main():353]  > R 157, for the weighted aggregated final model, testing loss: 6.5513e-01, testing acc: 77.86% ( 7786/10000), training loss: 0.69, training acc: 75.69%
[24/11/24 14:51:12, INFO, FSL_SAGE_main.py:main():353]  > R 158, for the weighted aggregated final model, testing loss: 6.4195e-01, testing acc: 78.25% ( 7825/10000), training loss: 0.68, training acc: 75.91%
[24/11/24 14:51:38, INFO, FSL_SAGE_main.py:main():353]  > R 159, for the weighted aggregated final model, testing loss: 6.4942e-01, testing acc: 77.74% ( 7774/10000), training loss: 0.68, training acc: 76.22%
[24/11/24 14:52:06, INFO, FSL_SAGE_main.py:main():353]  > R 160, for the weighted aggregated final model, testing loss: 6.6798e-01, testing acc: 77.41% ( 7741/10000), training loss: 0.69, training acc: 75.88%
[24/11/24 14:52:32, INFO, FSL_SAGE_main.py:main():353]  > R 161, for the weighted aggregated final model, testing loss: 6.3932e-01, testing acc: 78.21% ( 7821/10000), training loss: 0.68, training acc: 76.08%
[24/11/24 14:52:58, INFO, FSL_SAGE_main.py:main():353]  > R 162, for the weighted aggregated final model, testing loss: 6.5195e-01, testing acc: 77.75% ( 7775/10000), training loss: 0.67, training acc: 76.46%
[24/11/24 14:53:24, INFO, FSL_SAGE_main.py:main():353]  > R 163, for the weighted aggregated final model, testing loss: 6.5586e-01, testing acc: 77.51% ( 7751/10000), training loss: 0.68, training acc: 75.79%
[24/11/24 14:53:49, INFO, FSL_SAGE_main.py:main():353]  > R 164, for the weighted aggregated final model, testing loss: 6.5807e-01, testing acc: 77.45% ( 7745/10000), training loss: 0.70, training acc: 75.61%
[24/11/24 14:54:15, INFO, FSL_SAGE_main.py:main():353]  > R 165, for the weighted aggregated final model, testing loss: 6.4282e-01, testing acc: 77.80% ( 7780/10000), training loss: 0.66, training acc: 76.80%
[24/11/24 14:54:41, INFO, FSL_SAGE_main.py:main():353]  > R 166, for the weighted aggregated final model, testing loss: 6.3390e-01, testing acc: 78.04% ( 7804/10000), training loss: 0.66, training acc: 76.69%
[24/11/24 14:55:07, INFO, FSL_SAGE_main.py:main():353]  > R 167, for the weighted aggregated final model, testing loss: 6.5481e-01, testing acc: 77.47% ( 7747/10000), training loss: 0.68, training acc: 76.16%
[24/11/24 14:55:32, INFO, FSL_SAGE_main.py:main():353]  > R 168, for the weighted aggregated final model, testing loss: 6.4713e-01, testing acc: 77.90% ( 7790/10000), training loss: 0.67, training acc: 76.38%
[24/11/24 14:55:58, INFO, FSL_SAGE_main.py:main():353]  > R 169, for the weighted aggregated final model, testing loss: 6.5866e-01, testing acc: 77.27% ( 7727/10000), training loss: 0.68, training acc: 75.91%
[24/11/24 14:56:27, INFO, FSL_SAGE_main.py:main():353]  > R 170, for the weighted aggregated final model, testing loss: 6.4786e-01, testing acc: 77.96% ( 7796/10000), training loss: 0.67, training acc: 76.40%
[24/11/24 14:56:53, INFO, FSL_SAGE_main.py:main():353]  > R 171, for the weighted aggregated final model, testing loss: 6.3955e-01, testing acc: 77.96% ( 7796/10000), training loss: 0.68, training acc: 76.33%
[24/11/24 14:57:18, INFO, FSL_SAGE_main.py:main():353]  > R 172, for the weighted aggregated final model, testing loss: 6.6553e-01, testing acc: 77.14% ( 7714/10000), training loss: 0.69, training acc: 75.77%
[24/11/24 14:57:44, INFO, FSL_SAGE_main.py:main():353]  > R 173, for the weighted aggregated final model, testing loss: 6.5197e-01, testing acc: 77.46% ( 7746/10000), training loss: 0.68, training acc: 76.05%
[24/11/24 14:58:10, INFO, FSL_SAGE_main.py:main():353]  > R 174, for the weighted aggregated final model, testing loss: 6.6231e-01, testing acc: 77.94% ( 7794/10000), training loss: 0.69, training acc: 75.99%
[24/11/24 14:58:36, INFO, FSL_SAGE_main.py:main():353]  > R 175, for the weighted aggregated final model, testing loss: 6.4460e-01, testing acc: 78.06% ( 7806/10000), training loss: 0.67, training acc: 76.50%
[24/11/24 14:59:02, INFO, FSL_SAGE_main.py:main():353]  > R 176, for the weighted aggregated final model, testing loss: 6.5052e-01, testing acc: 77.64% ( 7764/10000), training loss: 0.68, training acc: 76.20%
[24/11/24 14:59:27, INFO, FSL_SAGE_main.py:main():353]  > R 177, for the weighted aggregated final model, testing loss: 6.5749e-01, testing acc: 77.86% ( 7786/10000), training loss: 0.67, training acc: 76.24%
[24/11/24 14:59:53, INFO, FSL_SAGE_main.py:main():353]  > R 178, for the weighted aggregated final model, testing loss: 6.4263e-01, testing acc: 77.88% ( 7788/10000), training loss: 0.67, training acc: 76.19%
[24/11/24 15:00:19, INFO, FSL_SAGE_main.py:main():353]  > R 179, for the weighted aggregated final model, testing loss: 6.4694e-01, testing acc: 77.94% ( 7794/10000), training loss: 0.67, training acc: 76.60%
[24/11/24 15:00:48, INFO, FSL_SAGE_main.py:main():353]  > R 180, for the weighted aggregated final model, testing loss: 6.4235e-01, testing acc: 77.67% ( 7767/10000), training loss: 0.68, training acc: 76.04%
[24/11/24 15:01:14, INFO, FSL_SAGE_main.py:main():353]  > R 181, for the weighted aggregated final model, testing loss: 6.5725e-01, testing acc: 77.94% ( 7794/10000), training loss: 0.69, training acc: 76.05%
[24/11/24 15:01:40, INFO, FSL_SAGE_main.py:main():353]  > R 182, for the weighted aggregated final model, testing loss: 6.4635e-01, testing acc: 77.99% ( 7799/10000), training loss: 0.66, training acc: 76.61%
[24/11/24 15:02:05, INFO, FSL_SAGE_main.py:main():353]  > R 183, for the weighted aggregated final model, testing loss: 6.6777e-01, testing acc: 77.21% ( 7721/10000), training loss: 0.68, training acc: 76.14%
[24/11/24 15:02:31, INFO, FSL_SAGE_main.py:main():353]  > R 184, for the weighted aggregated final model, testing loss: 6.4646e-01, testing acc: 77.92% ( 7792/10000), training loss: 0.66, training acc: 76.77%
[24/11/24 15:02:57, INFO, FSL_SAGE_main.py:main():353]  > R 185, for the weighted aggregated final model, testing loss: 6.3878e-01, testing acc: 78.01% ( 7801/10000), training loss: 0.65, training acc: 77.12%
[24/11/24 15:03:23, INFO, FSL_SAGE_main.py:main():353]  > R 186, for the weighted aggregated final model, testing loss: 6.3954e-01, testing acc: 77.88% ( 7788/10000), training loss: 0.66, training acc: 76.79%
[24/11/24 15:03:49, INFO, FSL_SAGE_main.py:main():353]  > R 187, for the weighted aggregated final model, testing loss: 6.4754e-01, testing acc: 77.64% ( 7764/10000), training loss: 0.68, training acc: 76.16%
[24/11/24 15:04:14, INFO, FSL_SAGE_main.py:main():353]  > R 188, for the weighted aggregated final model, testing loss: 6.3905e-01, testing acc: 77.98% ( 7798/10000), training loss: 0.66, training acc: 76.89%
[24/11/24 15:04:40, INFO, FSL_SAGE_main.py:main():353]  > R 189, for the weighted aggregated final model, testing loss: 6.2915e-01, testing acc: 78.35% ( 7835/10000), training loss: 0.65, training acc: 77.06%
[24/11/24 15:05:09, INFO, FSL_SAGE_main.py:main():353]  > R 190, for the weighted aggregated final model, testing loss: 6.4923e-01, testing acc: 77.83% ( 7783/10000), training loss: 0.66, training acc: 76.86%
[24/11/24 15:05:35, INFO, FSL_SAGE_main.py:main():353]  > R 191, for the weighted aggregated final model, testing loss: 6.3415e-01, testing acc: 78.03% ( 7803/10000), training loss: 0.67, training acc: 76.39%
[24/11/24 15:06:01, INFO, FSL_SAGE_main.py:main():353]  > R 192, for the weighted aggregated final model, testing loss: 6.3269e-01, testing acc: 78.28% ( 7828/10000), training loss: 0.65, training acc: 77.03%
[24/11/24 15:06:27, INFO, FSL_SAGE_main.py:main():353]  > R 193, for the weighted aggregated final model, testing loss: 6.7004e-01, testing acc: 77.00% ( 7700/10000), training loss: 0.69, training acc: 75.75%
[24/11/24 15:06:52, INFO, FSL_SAGE_main.py:main():353]  > R 194, for the weighted aggregated final model, testing loss: 6.3319e-01, testing acc: 78.50% ( 7850/10000), training loss: 0.66, training acc: 76.89%
[24/11/24 15:07:18, INFO, FSL_SAGE_main.py:main():353]  > R 195, for the weighted aggregated final model, testing loss: 6.4175e-01, testing acc: 78.11% ( 7811/10000), training loss: 0.66, training acc: 76.83%
[24/11/24 15:07:44, INFO, FSL_SAGE_main.py:main():353]  > R 196, for the weighted aggregated final model, testing loss: 6.4752e-01, testing acc: 77.91% ( 7791/10000), training loss: 0.67, training acc: 76.50%
[24/11/24 15:08:10, INFO, FSL_SAGE_main.py:main():353]  > R 197, for the weighted aggregated final model, testing loss: 6.4942e-01, testing acc: 77.67% ( 7767/10000), training loss: 0.67, training acc: 76.53%
[24/11/24 15:08:35, INFO, FSL_SAGE_main.py:main():353]  > R 198, for the weighted aggregated final model, testing loss: 6.4978e-01, testing acc: 77.34% ( 7734/10000), training loss: 0.67, training acc: 76.64%
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():353]  > R 199, for the weighted aggregated final model, testing loss: 6.3937e-01, testing acc: 77.79% ( 7779/10000), training loss: 0.65, training acc: 77.22%
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():355] The total running time for all rounds is 5233.99 seconds
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():365] [NOTICE] Saved results to '../saves/cifar-iid-K3U3E1BR1-200-241124-134145/results.json'.
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():382] Testing accuracy: [0.3491, 0.3597, 0.3658, 0.4428, 0.4322, 0.4438, 0.4347, 0.451, 0.482, 0.4854, 0.5168, 0.5362, 0.5427, 0.5787, 0.5862, 0.5827, 0.593, 0.5877, 0.5994, 0.5946, 0.6303, 0.6401, 0.6455, 0.6425, 0.6627, 0.6485, 0.6612, 0.6717, 0.6779, 0.6722, 0.668, 0.6732, 0.6773, 0.6857, 0.6974, 0.6933, 0.6959, 0.7057, 0.7066, 0.7017, 0.7035, 0.7007, 0.7149, 0.7133, 0.7159, 0.7077, 0.713, 0.7188, 0.7219, 0.722, 0.7205, 0.7238, 0.7211, 0.7276, 0.7252, 0.7286, 0.7369, 0.7309, 0.7356, 0.7296, 0.7311, 0.7321, 0.7347, 0.741, 0.7402, 0.7371, 0.7404, 0.7411, 0.7418, 0.7429, 0.7372, 0.741, 0.7418, 0.7382, 0.7464, 0.7427, 0.7443, 0.7498, 0.747, 0.7552, 0.7444, 0.7487, 0.7503, 0.7499, 0.7457, 0.7523, 0.7543, 0.7462, 0.7575, 0.7521, 0.7538, 0.7497, 0.7599, 0.7597, 0.761, 0.7577, 0.7606, 0.7588, 0.7533, 0.7568, 0.759, 0.7618, 0.7495, 0.7669, 0.7587, 0.7571, 0.7641, 0.758, 0.7661, 0.771, 0.7662, 0.766, 0.7695, 0.7635, 0.7622, 0.7703, 0.7622, 0.7598, 0.7689, 0.7637, 0.7618, 0.7588, 0.7671, 0.7598, 0.7713, 0.7723, 0.7722, 0.7729, 0.7732, 0.7718, 0.7759, 0.769, 0.7686, 0.7733, 0.7703, 0.7679, 0.7773, 0.7767, 0.773, 0.7789, 0.7627, 0.7683, 0.7704, 0.7806, 0.7753, 0.7751, 0.7746, 0.7756, 0.771, 0.7807, 0.773, 0.778, 0.7779, 0.7754, 0.7772, 0.7759, 0.7738, 0.7786, 0.7825, 0.7774, 0.7741, 0.7821, 0.7775, 0.7751, 0.7745, 0.778, 0.7804, 0.7747, 0.779, 0.7727, 0.7796, 0.7796, 0.7714, 0.7746, 0.7794, 0.7806, 0.7764, 0.7786, 0.7788, 0.7794, 0.7767, 0.7794, 0.7799, 0.7721, 0.7792, 0.7801, 0.7788, 0.7764, 0.7798, 0.7835, 0.7783, 0.7803, 0.7828, 0.77, 0.785, 0.7811, 0.7791, 0.7767, 0.7734, 0.7779]
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():383] Testing loss: [1.8528658196895937, 1.6686734791043438, 1.6541717248626902, 1.517083219335049, 1.5229534197457228, 1.5239365312117565, 1.5128568456142764, 1.4867100066776517, 1.402671992024289, 1.4021137786816946, 1.3362283208702184, 1.2960119383244575, 1.2506725742847105, 1.191878465157521, 1.1661659984648982, 1.1649461713018296, 1.1398826279217684, 1.1519904611985894, 1.1177002085915095, 1.1451954343650914, 1.0437956957877437, 1.0252976161015184, 1.0070163622687134, 1.0133873141264613, 0.9720601032051859, 1.0112026416802709, 0.9710474006737335, 0.9351843640774111, 0.9283196624321274, 0.9289487072184116, 0.9403761376308489, 0.9311769212348552, 0.920323237588134, 0.903523632997199, 0.8672580266300636, 0.8885977177680293, 0.8599294371242765, 0.8486272383339798, 0.8437252497371239, 0.8567836352541477, 0.8557765370682825, 0.848761502700516, 0.8221214564540719, 0.822999201997926, 0.8125445291965823, 0.8313564160202123, 0.82035100912746, 0.810158406631856, 0.7953791505173792, 0.7940300378618361, 0.7992007151434694, 0.7927253646186635, 0.7947759432128713, 0.7857602774342404, 0.7929805600190465, 0.7811070295828807, 0.7522434305541122, 0.7744485192661044, 0.764585935616795, 0.764027523843548, 0.7742099995854534, 0.7810559536837325, 0.753619165360173, 0.7493550701986386, 0.7488406300544739, 0.7492731009857564, 0.7393186213094977, 0.7510463610480104, 0.7455313364161721, 0.7381801443009437, 0.7508617058584962, 0.7437000659447682, 0.7366558547261395, 0.742154814774477, 0.7327503829817229, 0.7403444979764238, 0.7466599262213405, 0.7245143040826049, 0.7302703110477592, 0.710821269056465, 0.7290560063682024, 0.7327976638003241, 0.7168603837490082, 0.7288798003257075, 0.7245564755005173, 0.7219747727430319, 0.7093692938738232, 0.734183333719833, 0.7096654135215131, 0.7147482097903385, 0.7221326990218102, 0.7215325206140929, 0.6939662434632266, 0.6898758407635025, 0.7022456356996223, 0.6953279760819447, 0.7050335203545003, 0.6969584833972061, 0.7110314127765124, 0.6966728213467176, 0.6967620193203793, 0.6919348764268658, 0.7188972080055671, 0.6839071893239324, 0.6981176417085189, 0.6886838225624229, 0.6807371260999124, 0.6985929140561744, 0.6883129534087603, 0.6727038995374607, 0.6819927036007748, 0.6803618903401532, 0.6791123925130579, 0.6866430926926529, 0.6891312602954575, 0.6739398512659194, 0.6942752754386468, 0.7071205788775335, 0.6840847666505017, 0.6799841481673566, 0.6856575921366487, 0.6971534188789658, 0.6753757324399827, 0.7023771703243256, 0.6731613455693933, 0.6620060971266106, 0.6730949007257631, 0.6676256977304628, 0.6703909982608843, 0.661095570914353, 0.6618960990181452, 0.6790211351611947, 0.6759289770941191, 0.6654220736479457, 0.6582854823975623, 0.6756127846391895, 0.6537360692326026, 0.6507471792305572, 0.6583995238135133, 0.6535656659663478, 0.6942992881883548, 0.6760254864451252, 0.6666436946090264, 0.6457094097439247, 0.6703269210042833, 0.6519263492354864, 0.6613078102280822, 0.6484924124765999, 0.6601255351229559, 0.6412415994873529, 0.6584545505952232, 0.640439817422553, 0.6551533126378362, 0.6521816687493385, 0.6460991464083707, 0.6532163427600378, 0.6545627347276181, 0.6551281711723231, 0.6419453537916835, 0.6494196932527083, 0.6679788435561748, 0.6393206538278845, 0.6519469550893277, 0.6558616753620438, 0.6580658788922467, 0.6428213492978977, 0.6338996442058419, 0.6548087498809718, 0.6471346115009694, 0.6586638360838347, 0.6478606632238701, 0.6395529097394098, 0.6655279209342184, 0.6519724870029884, 0.6623066237455681, 0.6445985293086571, 0.6505248569235017, 0.6574883653393274, 0.6426307562785812, 0.6469397020490864, 0.6423547022704836, 0.6572453541846215, 0.6463532236557973, 0.6677738443205629, 0.6464560952367662, 0.638778831385359, 0.63954370723495, 0.6475359231610841, 0.6390516369403163, 0.6291511888745465, 0.6492318280890018, 0.6341521755049501, 0.6326928953581219, 0.6700425517709949, 0.6331854519210284, 0.6417514202715475, 0.6475213796277589, 0.6494213840629481, 0.6497754741318619, 0.6393735936925381]
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():384] Training accuracy: [0.2944517780711228, 0.3186527461098444, 0.327753110124405, 0.390655626225049, 0.3807552302092084, 0.40375615024600986, 0.39047561902476097, 0.4136165446617865, 0.4359174366974679, 0.4415176607064283, 0.46361854474178965, 0.48617944717788714, 0.49103964158566343, 0.5232809312372495, 0.5390215608624345, 0.5290611624464978, 0.546621864874595, 0.5355614224568983, 0.5400216008640346, 0.5434417376695068, 0.5812632505300211, 0.5861634465378616, 0.5963638545541822, 0.5956638265530622, 0.6037441497659907, 0.5995039801592064, 0.6155446217848713, 0.6200248009920397, 0.6216448657946317, 0.6289251570062803, 0.6316452658106324, 0.625065002600104, 0.6309252370094803, 0.6385855434217369, 0.65720628825153, 0.6505660226409057, 0.65550622024881, 0.6573262930517221, 0.6649065962638505, 0.6575063002520101, 0.6588463538541541, 0.6620064802592104, 0.6662666506660266, 0.6648865954638186, 0.6731669266770671, 0.6699467978719149, 0.6759870394815792, 0.6723068922756911, 0.6829273170926837, 0.6844273770950838, 0.6786271450858035, 0.687407496299852, 0.6805872234889395, 0.6909676387055482, 0.6882475299011961, 0.6900676027041082, 0.6985279411176447, 0.6938477539101564, 0.6947877915116605, 0.7009280371214849, 0.6954878195127805, 0.68808752350094, 0.7004280171206848, 0.7055082203288131, 0.703328133125325, 0.7091683667346694, 0.7138485539421577, 0.7089883595343813, 0.7150686027441098, 0.7161086443457738, 0.7070082803312132, 0.7114484579383176, 0.7135485419416777, 0.7151886075443018, 0.7174686987479499, 0.7150286011440458, 0.7066682667306692, 0.7209288371534861, 0.7204488179527181, 0.7256890275611024, 0.7223288931557262, 0.7240689627585103, 0.7258890355614225, 0.7203288131525261, 0.7219888795551822, 0.7258090323612945, 0.7285291411656466, 0.7224688987559502, 0.734009360374415, 0.7258490339613585, 0.7243689747589903, 0.7202288091523661, 0.7342093683747349, 0.7370694827793112, 0.7326693067722709, 0.733409336373455, 0.7319292771710868, 0.7401096043841754, 0.7305492219688787, 0.7376895075803032, 0.733829353174127, 0.7363694547781912, 0.7311292451698068, 0.7415296611864475, 0.734849393975759, 0.7426097043881755, 0.7423096923876955, 0.7320492819712788, 0.7436497459898396, 0.7461498459938397, 0.7425897035881436, 0.7471098843953758, 0.7415296611864475, 0.7435897435897436, 0.7379295171806872, 0.7434097363894556, 0.7448697947917917, 0.7391695667826713, 0.7449897995919836, 0.7494499779991199, 0.7432497299891996, 0.7385695427817113, 0.7448297931917277, 0.7444497779911197, 0.7491499659986399, 0.7519100764030561, 0.7553502140085604, 0.7475699027961118, 0.7505900236009441, 0.7544701788071523, 0.74964998599944, 0.7421896875875035, 0.7468698747949918, 0.7569702788111524, 0.7527301092043682, 0.7533101324052962, 0.7558502340093604, 0.7568502740109604, 0.7576703068122725, 0.7560702428097124, 0.7448897955918237, 0.7516700668026721, 0.7543501740069603, 0.7626905076203048, 0.7562302492099684, 0.7605304212168487, 0.7591103644145766, 0.7533701348053922, 0.7540701628065123, 0.7630305212208488, 0.749909996399856, 0.7612304492179687, 0.7575503020120805, 0.7578703148125925, 0.7587703508140325, 0.7578903156126245, 0.7587303492139685, 0.7569302772110884, 0.7590903636145446, 0.7622304892195688, 0.7587903516140646, 0.7607704308172327, 0.7645705828233129, 0.7579103164126565, 0.7561302452098084, 0.7679907196287852, 0.7668906756270251, 0.7616304652186088, 0.7637905516220649, 0.7590903636145446, 0.7639505580223209, 0.7633105324212969, 0.7576503060122405, 0.7604904196167847, 0.7599303972158886, 0.7649705988239529, 0.7619504780191207, 0.7624304972198888, 0.7619104764190567, 0.765950638025521, 0.7604304172166887, 0.7604904196167847, 0.766110644425777, 0.7613904556182247, 0.7676907076283052, 0.7711508460338413, 0.7679307172286891, 0.7615704628185127, 0.7689107564302572, 0.7705908236329453, 0.7686107444297772, 0.7638705548221929, 0.7702708108324333, 0.7575103004120165, 0.7688507540301612, 0.7682707308292331, 0.765030601224049, 0.765290611624465, 0.766370654826193, 0.7721508860354415]
[24/11/24 15:09:01, INFO, FSL_SAGE_main.py:main():385] Training loss: [1.9319292391832852, 1.8047720515394332, 1.790178756980799, 1.6541618196715533, 1.6753827876413747, 1.6430575501827793, 1.6373679731941708, 1.6018955822816028, 1.5380636966258818, 1.5222195010755506, 1.4704484575577366, 1.414063755792516, 1.3889961124376486, 1.3198327358442408, 1.2809768178080785, 1.2996473956957422, 1.2668205032518498, 1.2807079796269347, 1.2726317862517962, 1.2703704621652308, 1.1750826624816915, 1.1545388010015318, 1.128293651054227, 1.134479579579739, 1.1144522231347083, 1.1461486673840433, 1.076805695777631, 1.0736765605196092, 1.0603622116205347, 1.0449064149504703, 1.0384982018980362, 1.050815910025104, 1.0295247321820442, 1.0235429695847684, 0.96929440228387, 0.9887088603949122, 0.9724555673793376, 0.9670002219331173, 0.947536764102431, 0.967447603173535, 0.9702190796230888, 0.9511542673632692, 0.9371084473818616, 0.9421305926398164, 0.9243318164014938, 0.9292431576864714, 0.9175874085826728, 0.9226983515664214, 0.8955006054946181, 0.8900605482606184, 0.9054178044996188, 0.8757216719573994, 0.9046819284368715, 0.8779229512651459, 0.8828712483398787, 0.8743030404619891, 0.8459169214008418, 0.8640100596515277, 0.8652746095002153, 0.8496711621151019, 0.8650005059387847, 0.8759594380703895, 0.846595117912341, 0.8325912051528465, 0.8419583838707921, 0.8242541093559362, 0.8145866899089959, 0.82673210026957, 0.8154153432554871, 0.803877239160562, 0.8286613959999182, 0.8177555521935908, 0.8100807169314862, 0.803891942701267, 0.7946066165546728, 0.8074164354164182, 0.8321748651924328, 0.7926308377705155, 0.790639243659779, 0.7747443875738682, 0.7813956011039307, 0.7838330394742447, 0.7847543102487656, 0.7969695240183338, 0.7858952629657192, 0.7789302277201005, 0.7735564249920784, 0.7846658657525332, 0.7578186651374245, 0.7779522230303622, 0.7825708553081251, 0.7861312300194311, 0.7583154294327015, 0.7510606751945486, 0.758324208908712, 0.758329608968196, 0.7583002261533082, 0.7431849268556551, 0.7595244307857737, 0.7404581136224228, 0.7613744817617285, 0.7459666122310338, 0.7640600143043139, 0.7329872682046041, 0.7526608004066477, 0.7337545003296462, 0.7260273446260215, 0.7603955491808535, 0.7295810107056421, 0.7162714220261149, 0.7300349304087592, 0.7287554300316721, 0.7323632096971264, 0.733656574524086, 0.7407796342712626, 0.7204865281666811, 0.7272703712829803, 0.7344378902560276, 0.7267230247116574, 0.7120031764489094, 0.7304576394970483, 0.7429034165450331, 0.7239448924101036, 0.7286908702389278, 0.70810974470835, 0.7081283537182795, 0.6975820731115705, 0.7154093635749574, 0.7087284049004999, 0.6938190728653478, 0.7123815728204548, 0.7368161022359785, 0.7159505575971142, 0.6936171385471451, 0.7026081148904698, 0.7010257077277768, 0.69338562864687, 0.6853509672725474, 0.6877590319279193, 0.6880438234514863, 0.7226836870189841, 0.7105039642512343, 0.6988387388430782, 0.6818477653971762, 0.6961364734718818, 0.6831031054182514, 0.6847005873236037, 0.7111010125271844, 0.6949135652783566, 0.6716933068428331, 0.7099442165771513, 0.6804745330003685, 0.6863752329774182, 0.6918691288272236, 0.6857319842768079, 0.6846235645484682, 0.6849646322599804, 0.6875573731864075, 0.6838538392657846, 0.6797923136303443, 0.6903479967560173, 0.6816905107510303, 0.67053235631256, 0.682954941542094, 0.6992969571330772, 0.6605248893338609, 0.6622132853090611, 0.676369870816175, 0.6668859745254954, 0.6815369938317752, 0.6700294033716653, 0.6756139668344542, 0.6870817355527222, 0.6760987395245307, 0.6917106083786214, 0.6681693671768858, 0.6766873962854915, 0.6734485209898184, 0.6697554873449505, 0.6679838916272608, 0.6780621212250707, 0.6852998712287302, 0.664474149393368, 0.6822621553908778, 0.6568055546465721, 0.6498872011369118, 0.6586552834086139, 0.6759277397136348, 0.6599516634267705, 0.6500396931747747, 0.6623901752115205, 0.6665186987426748, 0.6522497420244241, 0.6939681405177857, 0.6560711127352775, 0.6581252168759741, 0.66817745695284, 0.6668322885612799, 0.667297698993416, 0.6466339374999054]
